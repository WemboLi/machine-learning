{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [05:22, 529KB/s]                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe491717da0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_norm = x  / 255.0\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    labels = np.zeros((len(x), 10), dtype = np.int)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        labels[i, x[i]] = 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = tf.placeholder(tf.float32, shape = (None, image_shape[0], image_shape[1], image_shape[2]), name = \"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y = tf.placeholder(tf.int64, shape = (None, n_classes), name = 'y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_channels = x_tensor.get_shape()[3].value\n",
    "    \n",
    "    wShape = [conv_ksize[0], conv_ksize[1], num_channels , conv_num_outputs]\n",
    "    \n",
    "    # weight = tf.get_variable('weight', wShape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # bias = tf.get_variable('bias', conv_num_outputs, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal(shape = wShape))\n",
    "    bias = tf.Variable(tf.constant(1.0, shape = [conv_num_outputs]))\n",
    "    \n",
    "    activations = tf.nn.conv2d(x_tensor, weight, [1, conv_strides[0], conv_strides[1], 1], 'SAME') + bias\n",
    "    \n",
    "    activations = tf.nn.sigmoid(activations)\n",
    "\n",
    "    pool = tf.nn.max_pool(activations, [1, pool_ksize[0], pool_ksize[1], 1], [1, pool_strides[0], pool_strides[1], 1], 'SAME')\n",
    "    \n",
    "    return pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    input_shape = x_tensor.get_shape()\n",
    "    # print (input_shape)\n",
    "    num_activations = input_shape[1:4].num_elements()\n",
    "    \n",
    "    flat = tf.reshape(x_tensor, [-1, num_activations])\n",
    "    # print (flat.get_shape())\n",
    "    \n",
    "    return flat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_dim = x_tensor.get_shape()[1].value\n",
    "    \n",
    "    wShape = [input_dim, num_outputs]\n",
    "    \n",
    "    \n",
    "    #weight = tf.get_variable('full_weight', wShape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "    #bias = tf.get_variable('full_bias', [num_outputs], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal(shape = wShape))\n",
    "    bias = tf.Variable(tf.constant(0.0, shape = [num_outputs]))\n",
    "    \n",
    "    fc = tf.matmul(x_tensor, weight) + bias\n",
    "    \n",
    "    fc = tf.nn.sigmoid(fc)\n",
    "    \n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fc = fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    conv_ksize1 = [5, 5]\n",
    "    conv_stride1 = [1, 1]\n",
    "    conv_ksize2 = [5, 5]\n",
    "    conv_stride2 = [1, 1]\n",
    "    \n",
    "    conv_ksize3 = [5, 5]\n",
    "    conv_stride3 = [1, 1]\n",
    "    \n",
    "    conv_num_output1 = 32\n",
    "    conv_num_output2 = 64\n",
    "    conv_num_output3 = 128\n",
    "    \n",
    "    pool_ksize1 = [2, 2]\n",
    "    pool_stride1 = [2, 2]\n",
    "    \n",
    "    pool_ksize2 = [2, 2]\n",
    "    pool_stride2 = [2, 2]\n",
    "    \n",
    "    pool_ksize3= [2, 2]\n",
    "    pool_stride3 = [2, 2]\n",
    "    \n",
    "    #conv1\n",
    "    x1 = conv2d_maxpool(x, conv_num_output1, conv_ksize1, conv_stride1, pool_ksize1, pool_stride1)\n",
    "    x1 = tf.nn.dropout(x1, keep_prob)\n",
    "    #conv2\n",
    "    x2 = conv2d_maxpool(x1, conv_num_output2, conv_ksize2, conv_stride2, pool_ksize2, pool_stride2)\n",
    "    x2 = tf.nn.dropout(x2, keep_prob)\n",
    "    #conv3\n",
    "    x3 = conv2d_maxpool(x2, conv_num_output3, conv_ksize3, conv_stride3, pool_ksize3, pool_stride3)\n",
    "    x3 = tf.nn.dropout(x3, keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flat = flatten(x3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1_size = fc2_size = 128\n",
    "    \n",
    "    fc1 = fully_conn(flat, fc1_size)\n",
    "    fc2 = fully_conn(fc1, fc2_size)\n",
    "    num_outputs = 10\n",
    "      \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    top = output(fc2, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return top\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # pass\n",
    "    session.run([optimizer], feed_dict = {x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tCost, tAccuracy = session.run([cost, accuracy], feed_dict = {x : feature_batch, y : label_batch, keep_prob : 1})\n",
    "    print(\"loss and accuracy: %.4f, %.4f\" % (tCost, tAccuracy))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 3000\n",
    "batch_size = 256\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss and accuracy: 2.3321, 0.1052\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss and accuracy: 2.3378, 0.1052\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss and accuracy: 2.3003, 0.1050\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss and accuracy: 2.3017, 0.1050\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss and accuracy: 2.3018, 0.1050\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss and accuracy: 2.3020, 0.1052\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss and accuracy: 2.3020, 0.1052\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss and accuracy: 2.3020, 0.1050\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1050\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1050\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1050\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1052\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1052\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1048\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1054\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1050\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1070\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1084\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1052\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1052\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1054\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1054\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1052\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1048\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss and accuracy: 2.3020, 0.1050\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss and accuracy: 2.3024, 0.1050\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss and accuracy: 2.3030, 0.1050\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss and accuracy: 2.3033, 0.1050\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss and accuracy: 2.3028, 0.1050\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss and accuracy: 2.3029, 0.1050\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1048\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss and accuracy: 2.3030, 0.1006\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss and accuracy: 2.3029, 0.1050\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss and accuracy: 2.3022, 0.1050\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1050\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1050\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1050\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1050\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1044\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss and accuracy: 2.3020, 0.1050\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss and accuracy: 2.3023, 0.1050\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss and accuracy: 2.3016, 0.1050\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss and accuracy: 2.3028, 0.1050\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss and accuracy: 2.3026, 0.1060\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss and accuracy: 2.2989, 0.1048\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss and accuracy: 2.3010, 0.1050\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss and accuracy: 2.3014, 0.1046\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss and accuracy: 2.3025, 0.1050\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1050\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss and accuracy: 2.3027, 0.1050\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss and accuracy: 2.3021, 0.1050\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss and accuracy: 2.3009, 0.1050\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss and accuracy: 2.3017, 0.0750\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss and accuracy: 2.2996, 0.0750\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss and accuracy: 2.2981, 0.0832\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss and accuracy: 2.2979, 0.0994\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss and accuracy: 2.3017, 0.0998\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss and accuracy: 2.3002, 0.0998\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss and accuracy: 2.3013, 0.1018\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss and accuracy: 2.3022, 0.1246\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss and accuracy: 2.3011, 0.0958\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss and accuracy: 2.3002, 0.1052\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss and accuracy: 2.3001, 0.0930\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss and accuracy: 2.2968, 0.1030\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss and accuracy: 2.2865, 0.1326\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss and accuracy: 2.2796, 0.0998\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss and accuracy: 2.2739, 0.0992\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss and accuracy: 2.2751, 0.0998\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss and accuracy: 2.2680, 0.0922\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss and accuracy: 2.2660, 0.0978\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss and accuracy: 2.2661, 0.1002\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss and accuracy: 2.2889, 0.1084\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss and accuracy: 2.2807, 0.1038\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss and accuracy: 2.2558, 0.0902\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss and accuracy: 2.2792, 0.1044\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss and accuracy: 2.2626, 0.1050\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss and accuracy: 2.2804, 0.1058\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss and accuracy: 2.2627, 0.1050\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss and accuracy: 2.2629, 0.1046\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss and accuracy: 2.2652, 0.1050\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss and accuracy: 2.2494, 0.1048\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss and accuracy: 2.2426, 0.1056\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss and accuracy: 2.2489, 0.1052\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss and accuracy: 2.2443, 0.1062\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss and accuracy: 2.2406, 0.0934\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss and accuracy: 2.2405, 0.0962\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss and accuracy: 2.2438, 0.0854\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss and accuracy: 2.2354, 0.1046\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss and accuracy: 2.2391, 0.1050\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss and accuracy: 2.2311, 0.1046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 1:  loss and accuracy: 2.2425, 0.1050\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss and accuracy: 2.2348, 0.1050\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss and accuracy: 2.2221, 0.1050\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss and accuracy: 2.2371, 0.1050\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss and accuracy: 2.2261, 0.1050\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss and accuracy: 2.2181, 0.1050\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss and accuracy: 2.2272, 0.1050\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss and accuracy: 2.2164, 0.1050\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss and accuracy: 2.2014, 0.1050\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss and accuracy: 2.2020, 0.1050\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss and accuracy: 2.1921, 0.1050\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss and accuracy: 2.1875, 0.1050\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss and accuracy: 2.1968, 0.1050\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss and accuracy: 2.1875, 0.1050\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss and accuracy: 2.1848, 0.1050\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss and accuracy: 2.1872, 0.1050\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss and accuracy: 2.1602, 0.1050\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss and accuracy: 2.1546, 0.1050\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss and accuracy: 2.1467, 0.1050\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss and accuracy: 2.1402, 0.1050\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss and accuracy: 2.1476, 0.1050\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss and accuracy: 2.1366, 0.1050\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss and accuracy: 2.1362, 0.1050\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss and accuracy: 2.1436, 0.1050\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss and accuracy: 2.1347, 0.1050\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss and accuracy: 2.1372, 0.1048\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss and accuracy: 2.1416, 0.1236\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss and accuracy: 2.1182, 0.2166\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss and accuracy: 2.0864, 0.2218\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss and accuracy: 2.0854, 0.2276\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss and accuracy: 2.0792, 0.2488\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss and accuracy: 2.0835, 0.2332\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss and accuracy: 2.0849, 0.2368\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss and accuracy: 2.0805, 0.2400\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss and accuracy: 2.0832, 0.2308\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss and accuracy: 2.0641, 0.2432\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss and accuracy: 2.0772, 0.2308\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss and accuracy: 2.0726, 0.2238\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss and accuracy: 2.0670, 0.2392\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss and accuracy: 2.0683, 0.2360\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss and accuracy: 2.0687, 0.2384\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss and accuracy: 2.0705, 0.2404\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss and accuracy: 2.0653, 0.2434\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss and accuracy: 2.0871, 0.2254\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss and accuracy: 2.0765, 0.2320\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss and accuracy: 2.0764, 0.2486\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss and accuracy: 2.0616, 0.2452\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss and accuracy: 2.0651, 0.2418\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss and accuracy: 2.0757, 0.2382\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss and accuracy: 2.0720, 0.2356\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss and accuracy: 2.0752, 0.2406\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss and accuracy: 2.0508, 0.2478\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss and accuracy: 2.0504, 0.2414\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss and accuracy: 2.0571, 0.2514\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss and accuracy: 2.0731, 0.2566\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss and accuracy: 2.0587, 0.2536\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss and accuracy: 2.0652, 0.2472\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss and accuracy: 2.0656, 0.2362\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss and accuracy: 2.0617, 0.2498\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss and accuracy: 2.0623, 0.2452\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss and accuracy: 2.0738, 0.2356\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss and accuracy: 2.0668, 0.2370\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss and accuracy: 2.0669, 0.2368\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss and accuracy: 2.0773, 0.2250\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss and accuracy: 2.0560, 0.2296\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss and accuracy: 2.0698, 0.2340\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss and accuracy: 2.0790, 0.2250\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss and accuracy: 2.0574, 0.2400\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss and accuracy: 2.0853, 0.2242\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss and accuracy: 2.0584, 0.2432\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss and accuracy: 2.0914, 0.2278\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss and accuracy: 2.0464, 0.2508\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss and accuracy: 2.0400, 0.2516\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss and accuracy: 2.0479, 0.2518\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss and accuracy: 2.0495, 0.2524\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss and accuracy: 2.0538, 0.2524\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss and accuracy: 2.0562, 0.2552\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss and accuracy: 2.0591, 0.2522\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss and accuracy: 2.0407, 0.2556\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss and accuracy: 2.0583, 0.2500\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss and accuracy: 2.0380, 0.2354\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss and accuracy: 2.0418, 0.2434\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss and accuracy: 2.0330, 0.2438\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss and accuracy: 2.0456, 0.2534\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss and accuracy: 2.0411, 0.2494\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss and accuracy: 2.0346, 0.2562\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss and accuracy: 2.0452, 0.2436\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss and accuracy: 2.0467, 0.2450\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss and accuracy: 2.0476, 0.2390\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss and accuracy: 2.0521, 0.2420\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss and accuracy: 2.0711, 0.2282\n",
      "Epoch 221, CIFAR-10 Batch 1:  loss and accuracy: 2.0395, 0.2416\n",
      "Epoch 222, CIFAR-10 Batch 1:  loss and accuracy: 2.0609, 0.2352\n",
      "Epoch 223, CIFAR-10 Batch 1:  loss and accuracy: 2.0419, 0.2378\n",
      "Epoch 224, CIFAR-10 Batch 1:  loss and accuracy: 2.0637, 0.2278\n",
      "Epoch 225, CIFAR-10 Batch 1:  loss and accuracy: 2.0358, 0.2300\n",
      "Epoch 226, CIFAR-10 Batch 1:  loss and accuracy: 2.0389, 0.2414\n",
      "Epoch 227, CIFAR-10 Batch 1:  loss and accuracy: 2.0477, 0.2306\n",
      "Epoch 228, CIFAR-10 Batch 1:  loss and accuracy: 2.0364, 0.2482\n",
      "Epoch 229, CIFAR-10 Batch 1:  loss and accuracy: 2.0436, 0.2594\n",
      "Epoch 230, CIFAR-10 Batch 1:  loss and accuracy: 2.0521, 0.2372\n",
      "Epoch 231, CIFAR-10 Batch 1:  loss and accuracy: 2.0471, 0.2422\n",
      "Epoch 232, CIFAR-10 Batch 1:  loss and accuracy: 2.0455, 0.2354\n",
      "Epoch 233, CIFAR-10 Batch 1:  loss and accuracy: 2.0490, 0.2448\n",
      "Epoch 234, CIFAR-10 Batch 1:  loss and accuracy: 2.0548, 0.2530\n",
      "Epoch 235, CIFAR-10 Batch 1:  loss and accuracy: 2.0432, 0.2570\n",
      "Epoch 236, CIFAR-10 Batch 1:  loss and accuracy: 2.0360, 0.2526\n",
      "Epoch 237, CIFAR-10 Batch 1:  loss and accuracy: 2.0489, 0.2414\n",
      "Epoch 238, CIFAR-10 Batch 1:  loss and accuracy: 2.0462, 0.2448\n",
      "Epoch 239, CIFAR-10 Batch 1:  loss and accuracy: 2.0555, 0.2328\n",
      "Epoch 240, CIFAR-10 Batch 1:  loss and accuracy: 2.0278, 0.2508\n",
      "Epoch 241, CIFAR-10 Batch 1:  loss and accuracy: 2.0512, 0.2396\n",
      "Epoch 242, CIFAR-10 Batch 1:  loss and accuracy: 2.0705, 0.2270\n",
      "Epoch 243, CIFAR-10 Batch 1:  loss and accuracy: 2.0406, 0.2432\n",
      "Epoch 244, CIFAR-10 Batch 1:  loss and accuracy: 2.0445, 0.2432\n",
      "Epoch 245, CIFAR-10 Batch 1:  loss and accuracy: 2.0494, 0.2458\n",
      "Epoch 246, CIFAR-10 Batch 1:  loss and accuracy: 2.0402, 0.2462\n",
      "Epoch 247, CIFAR-10 Batch 1:  loss and accuracy: 2.0507, 0.2398\n",
      "Epoch 248, CIFAR-10 Batch 1:  loss and accuracy: 2.0535, 0.2296\n",
      "Epoch 249, CIFAR-10 Batch 1:  loss and accuracy: 2.0377, 0.2542\n",
      "Epoch 250, CIFAR-10 Batch 1:  loss and accuracy: 2.0186, 0.2608\n",
      "Epoch 251, CIFAR-10 Batch 1:  loss and accuracy: 2.0329, 0.2434\n",
      "Epoch 252, CIFAR-10 Batch 1:  loss and accuracy: 2.0314, 0.2414\n",
      "Epoch 253, CIFAR-10 Batch 1:  loss and accuracy: 2.0346, 0.2456\n",
      "Epoch 254, CIFAR-10 Batch 1:  loss and accuracy: 2.0268, 0.2542\n",
      "Epoch 255, CIFAR-10 Batch 1:  loss and accuracy: 2.0258, 0.2510\n",
      "Epoch 256, CIFAR-10 Batch 1:  loss and accuracy: 2.0328, 0.2454\n",
      "Epoch 257, CIFAR-10 Batch 1:  loss and accuracy: 2.0224, 0.2540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258, CIFAR-10 Batch 1:  loss and accuracy: 2.0291, 0.2716\n",
      "Epoch 259, CIFAR-10 Batch 1:  loss and accuracy: 2.0367, 0.2640\n",
      "Epoch 260, CIFAR-10 Batch 1:  loss and accuracy: 2.0304, 0.2722\n",
      "Epoch 261, CIFAR-10 Batch 1:  loss and accuracy: 2.0414, 0.2558\n",
      "Epoch 262, CIFAR-10 Batch 1:  loss and accuracy: 2.0353, 0.2652\n",
      "Epoch 263, CIFAR-10 Batch 1:  loss and accuracy: 2.0264, 0.2808\n",
      "Epoch 264, CIFAR-10 Batch 1:  loss and accuracy: 2.0325, 0.2748\n",
      "Epoch 265, CIFAR-10 Batch 1:  loss and accuracy: 2.0270, 0.2670\n",
      "Epoch 266, CIFAR-10 Batch 1:  loss and accuracy: 2.0313, 0.2746\n",
      "Epoch 267, CIFAR-10 Batch 1:  loss and accuracy: 2.0077, 0.2984\n",
      "Epoch 268, CIFAR-10 Batch 1:  loss and accuracy: 2.0272, 0.2822\n",
      "Epoch 269, CIFAR-10 Batch 1:  loss and accuracy: 2.0216, 0.2908\n",
      "Epoch 270, CIFAR-10 Batch 1:  loss and accuracy: 2.0206, 0.2884\n",
      "Epoch 271, CIFAR-10 Batch 1:  loss and accuracy: 2.0289, 0.2660\n",
      "Epoch 272, CIFAR-10 Batch 1:  loss and accuracy: 2.0301, 0.2672\n",
      "Epoch 273, CIFAR-10 Batch 1:  loss and accuracy: 2.0405, 0.2616\n",
      "Epoch 274, CIFAR-10 Batch 1:  loss and accuracy: 2.0213, 0.2804\n",
      "Epoch 275, CIFAR-10 Batch 1:  loss and accuracy: 2.0414, 0.2758\n",
      "Epoch 276, CIFAR-10 Batch 1:  loss and accuracy: 2.0616, 0.2540\n",
      "Epoch 277, CIFAR-10 Batch 1:  loss and accuracy: 2.0447, 0.2758\n",
      "Epoch 278, CIFAR-10 Batch 1:  loss and accuracy: 2.0246, 0.2826\n",
      "Epoch 279, CIFAR-10 Batch 1:  loss and accuracy: 2.0297, 0.2770\n",
      "Epoch 280, CIFAR-10 Batch 1:  loss and accuracy: 2.0474, 0.2540\n",
      "Epoch 281, CIFAR-10 Batch 1:  loss and accuracy: 2.0318, 0.2800\n",
      "Epoch 282, CIFAR-10 Batch 1:  loss and accuracy: 2.0334, 0.2818\n",
      "Epoch 283, CIFAR-10 Batch 1:  loss and accuracy: 2.0403, 0.2782\n",
      "Epoch 284, CIFAR-10 Batch 1:  loss and accuracy: 2.0360, 0.2746\n",
      "Epoch 285, CIFAR-10 Batch 1:  loss and accuracy: 2.0223, 0.2866\n",
      "Epoch 286, CIFAR-10 Batch 1:  loss and accuracy: 2.0278, 0.2768\n",
      "Epoch 287, CIFAR-10 Batch 1:  loss and accuracy: 2.0277, 0.2764\n",
      "Epoch 288, CIFAR-10 Batch 1:  loss and accuracy: 2.0570, 0.2548\n",
      "Epoch 289, CIFAR-10 Batch 1:  loss and accuracy: 2.0371, 0.2784\n",
      "Epoch 290, CIFAR-10 Batch 1:  loss and accuracy: 2.0328, 0.2928\n",
      "Epoch 291, CIFAR-10 Batch 1:  loss and accuracy: 2.0150, 0.3042\n",
      "Epoch 292, CIFAR-10 Batch 1:  loss and accuracy: 2.0321, 0.2928\n",
      "Epoch 293, CIFAR-10 Batch 1:  loss and accuracy: 2.0378, 0.2950\n",
      "Epoch 294, CIFAR-10 Batch 1:  loss and accuracy: 2.0183, 0.2968\n",
      "Epoch 295, CIFAR-10 Batch 1:  loss and accuracy: 2.0333, 0.2886\n",
      "Epoch 296, CIFAR-10 Batch 1:  loss and accuracy: 2.0359, 0.2890\n",
      "Epoch 297, CIFAR-10 Batch 1:  loss and accuracy: 2.0289, 0.2926\n",
      "Epoch 298, CIFAR-10 Batch 1:  loss and accuracy: 2.0442, 0.2914\n",
      "Epoch 299, CIFAR-10 Batch 1:  loss and accuracy: 2.0221, 0.3016\n",
      "Epoch 300, CIFAR-10 Batch 1:  loss and accuracy: 2.0413, 0.2808\n",
      "Epoch 301, CIFAR-10 Batch 1:  loss and accuracy: 2.0501, 0.2786\n",
      "Epoch 302, CIFAR-10 Batch 1:  loss and accuracy: 2.0316, 0.2900\n",
      "Epoch 303, CIFAR-10 Batch 1:  loss and accuracy: 2.0124, 0.3030\n",
      "Epoch 304, CIFAR-10 Batch 1:  loss and accuracy: 2.0167, 0.2964\n",
      "Epoch 305, CIFAR-10 Batch 1:  loss and accuracy: 2.0277, 0.2952\n",
      "Epoch 306, CIFAR-10 Batch 1:  loss and accuracy: 2.0162, 0.3000\n",
      "Epoch 307, CIFAR-10 Batch 1:  loss and accuracy: 2.0398, 0.2808\n",
      "Epoch 308, CIFAR-10 Batch 1:  loss and accuracy: 2.0193, 0.2972\n",
      "Epoch 309, CIFAR-10 Batch 1:  loss and accuracy: 2.0208, 0.2892\n",
      "Epoch 310, CIFAR-10 Batch 1:  loss and accuracy: 2.0572, 0.2740\n",
      "Epoch 311, CIFAR-10 Batch 1:  loss and accuracy: 2.0382, 0.2738\n",
      "Epoch 312, CIFAR-10 Batch 1:  loss and accuracy: 2.0260, 0.2840\n",
      "Epoch 313, CIFAR-10 Batch 1:  loss and accuracy: 2.0360, 0.2786\n",
      "Epoch 314, CIFAR-10 Batch 1:  loss and accuracy: 2.0431, 0.2792\n",
      "Epoch 315, CIFAR-10 Batch 1:  loss and accuracy: 2.0429, 0.2770\n",
      "Epoch 316, CIFAR-10 Batch 1:  loss and accuracy: 2.0228, 0.2946\n",
      "Epoch 317, CIFAR-10 Batch 1:  loss and accuracy: 2.0248, 0.2762\n",
      "Epoch 318, CIFAR-10 Batch 1:  loss and accuracy: 2.0356, 0.2850\n",
      "Epoch 319, CIFAR-10 Batch 1:  loss and accuracy: 2.0257, 0.2880\n",
      "Epoch 320, CIFAR-10 Batch 1:  loss and accuracy: 2.0102, 0.2986\n",
      "Epoch 321, CIFAR-10 Batch 1:  loss and accuracy: 2.0125, 0.3004\n",
      "Epoch 322, CIFAR-10 Batch 1:  loss and accuracy: 2.0145, 0.3124\n",
      "Epoch 323, CIFAR-10 Batch 1:  loss and accuracy: 2.0192, 0.3006\n",
      "Epoch 324, CIFAR-10 Batch 1:  loss and accuracy: 2.0372, 0.2868\n",
      "Epoch 325, CIFAR-10 Batch 1:  loss and accuracy: 2.0183, 0.2964\n",
      "Epoch 326, CIFAR-10 Batch 1:  loss and accuracy: 2.0521, 0.2856\n",
      "Epoch 327, CIFAR-10 Batch 1:  loss and accuracy: 2.0435, 0.2892\n",
      "Epoch 328, CIFAR-10 Batch 1:  loss and accuracy: 2.0284, 0.3060\n",
      "Epoch 329, CIFAR-10 Batch 1:  loss and accuracy: 2.0300, 0.2994\n",
      "Epoch 330, CIFAR-10 Batch 1:  loss and accuracy: 2.0185, 0.3152\n",
      "Epoch 331, CIFAR-10 Batch 1:  loss and accuracy: 1.9899, 0.3252\n",
      "Epoch 332, CIFAR-10 Batch 1:  loss and accuracy: 1.9996, 0.3110\n",
      "Epoch 333, CIFAR-10 Batch 1:  loss and accuracy: 2.0161, 0.3052\n",
      "Epoch 334, CIFAR-10 Batch 1:  loss and accuracy: 2.0120, 0.3132\n",
      "Epoch 335, CIFAR-10 Batch 1:  loss and accuracy: 2.0045, 0.3222\n",
      "Epoch 336, CIFAR-10 Batch 1:  loss and accuracy: 2.0063, 0.3180\n",
      "Epoch 337, CIFAR-10 Batch 1:  loss and accuracy: 1.9930, 0.3252\n",
      "Epoch 338, CIFAR-10 Batch 1:  loss and accuracy: 2.0210, 0.2928\n",
      "Epoch 339, CIFAR-10 Batch 1:  loss and accuracy: 2.0568, 0.2634\n",
      "Epoch 340, CIFAR-10 Batch 1:  loss and accuracy: 2.0271, 0.2770\n",
      "Epoch 341, CIFAR-10 Batch 1:  loss and accuracy: 2.0176, 0.2954\n",
      "Epoch 342, CIFAR-10 Batch 1:  loss and accuracy: 2.0110, 0.2898\n",
      "Epoch 343, CIFAR-10 Batch 1:  loss and accuracy: 2.0311, 0.2882\n",
      "Epoch 344, CIFAR-10 Batch 1:  loss and accuracy: 2.0361, 0.2780\n",
      "Epoch 345, CIFAR-10 Batch 1:  loss and accuracy: 2.0212, 0.2976\n",
      "Epoch 346, CIFAR-10 Batch 1:  loss and accuracy: 2.0223, 0.2978\n",
      "Epoch 347, CIFAR-10 Batch 1:  loss and accuracy: 2.0167, 0.2978\n",
      "Epoch 348, CIFAR-10 Batch 1:  loss and accuracy: 2.0212, 0.2934\n",
      "Epoch 349, CIFAR-10 Batch 1:  loss and accuracy: 2.0310, 0.2902\n",
      "Epoch 350, CIFAR-10 Batch 1:  loss and accuracy: 2.0239, 0.2912\n",
      "Epoch 351, CIFAR-10 Batch 1:  loss and accuracy: 2.0144, 0.2978\n",
      "Epoch 352, CIFAR-10 Batch 1:  loss and accuracy: 2.0036, 0.3068\n",
      "Epoch 353, CIFAR-10 Batch 1:  loss and accuracy: 2.0238, 0.2916\n",
      "Epoch 354, CIFAR-10 Batch 1:  loss and accuracy: 2.0431, 0.2816\n",
      "Epoch 355, CIFAR-10 Batch 1:  loss and accuracy: 1.9959, 0.3118\n",
      "Epoch 356, CIFAR-10 Batch 1:  loss and accuracy: 2.0218, 0.2944\n",
      "Epoch 357, CIFAR-10 Batch 1:  loss and accuracy: 2.0267, 0.2912\n",
      "Epoch 358, CIFAR-10 Batch 1:  loss and accuracy: 1.9957, 0.3064\n",
      "Epoch 359, CIFAR-10 Batch 1:  loss and accuracy: 2.0465, 0.2822\n",
      "Epoch 360, CIFAR-10 Batch 1:  loss and accuracy: 2.0276, 0.2950\n",
      "Epoch 361, CIFAR-10 Batch 1:  loss and accuracy: 2.0232, 0.2924\n",
      "Epoch 362, CIFAR-10 Batch 1:  loss and accuracy: 2.0020, 0.3162\n",
      "Epoch 363, CIFAR-10 Batch 1:  loss and accuracy: 2.0066, 0.3090\n",
      "Epoch 364, CIFAR-10 Batch 1:  loss and accuracy: 2.0432, 0.2870\n",
      "Epoch 365, CIFAR-10 Batch 1:  loss and accuracy: 2.0328, 0.2940\n",
      "Epoch 366, CIFAR-10 Batch 1:  loss and accuracy: 1.9960, 0.3046\n",
      "Epoch 367, CIFAR-10 Batch 1:  loss and accuracy: 2.0161, 0.2918\n",
      "Epoch 368, CIFAR-10 Batch 1:  loss and accuracy: 2.0054, 0.3062\n",
      "Epoch 369, CIFAR-10 Batch 1:  loss and accuracy: 2.0588, 0.2822\n",
      "Epoch 370, CIFAR-10 Batch 1:  loss and accuracy: 1.9974, 0.3156\n",
      "Epoch 371, CIFAR-10 Batch 1:  loss and accuracy: 2.0178, 0.2920\n",
      "Epoch 372, CIFAR-10 Batch 1:  loss and accuracy: 2.0132, 0.3008\n",
      "Epoch 373, CIFAR-10 Batch 1:  loss and accuracy: 2.0225, 0.2900\n",
      "Epoch 374, CIFAR-10 Batch 1:  loss and accuracy: 2.0007, 0.3064\n",
      "Epoch 375, CIFAR-10 Batch 1:  loss and accuracy: 2.0536, 0.2728\n",
      "Epoch 376, CIFAR-10 Batch 1:  loss and accuracy: 2.0162, 0.2932\n",
      "Epoch 377, CIFAR-10 Batch 1:  loss and accuracy: 2.0101, 0.2924\n",
      "Epoch 378, CIFAR-10 Batch 1:  loss and accuracy: 2.0555, 0.2762\n",
      "Epoch 379, CIFAR-10 Batch 1:  loss and accuracy: 2.0217, 0.3078\n",
      "Epoch 380, CIFAR-10 Batch 1:  loss and accuracy: 2.0386, 0.2772\n",
      "Epoch 381, CIFAR-10 Batch 1:  loss and accuracy: 2.0128, 0.2972\n",
      "Epoch 382, CIFAR-10 Batch 1:  loss and accuracy: 2.0187, 0.2970\n",
      "Epoch 383, CIFAR-10 Batch 1:  loss and accuracy: 2.0013, 0.3064\n",
      "Epoch 384, CIFAR-10 Batch 1:  loss and accuracy: 2.0637, 0.2736\n",
      "Epoch 385, CIFAR-10 Batch 1:  loss and accuracy: 2.0500, 0.2796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386, CIFAR-10 Batch 1:  loss and accuracy: 2.0277, 0.2862\n",
      "Epoch 387, CIFAR-10 Batch 1:  loss and accuracy: 2.0437, 0.2690\n",
      "Epoch 388, CIFAR-10 Batch 1:  loss and accuracy: 2.0428, 0.2876\n",
      "Epoch 389, CIFAR-10 Batch 1:  loss and accuracy: 2.0243, 0.3024\n",
      "Epoch 390, CIFAR-10 Batch 1:  loss and accuracy: 2.0335, 0.2894\n",
      "Epoch 391, CIFAR-10 Batch 1:  loss and accuracy: 2.0227, 0.3020\n",
      "Epoch 392, CIFAR-10 Batch 1:  loss and accuracy: 2.0056, 0.3254\n",
      "Epoch 393, CIFAR-10 Batch 1:  loss and accuracy: 2.0657, 0.2724\n",
      "Epoch 394, CIFAR-10 Batch 1:  loss and accuracy: 2.0327, 0.2844\n",
      "Epoch 395, CIFAR-10 Batch 1:  loss and accuracy: 2.0198, 0.2866\n",
      "Epoch 396, CIFAR-10 Batch 1:  loss and accuracy: 2.0598, 0.2774\n",
      "Epoch 397, CIFAR-10 Batch 1:  loss and accuracy: 2.0189, 0.2930\n",
      "Epoch 398, CIFAR-10 Batch 1:  loss and accuracy: 2.0161, 0.2954\n",
      "Epoch 399, CIFAR-10 Batch 1:  loss and accuracy: 2.0279, 0.2868\n",
      "Epoch 400, CIFAR-10 Batch 1:  loss and accuracy: 2.0194, 0.2942\n",
      "Epoch 401, CIFAR-10 Batch 1:  loss and accuracy: 2.0121, 0.2974\n",
      "Epoch 402, CIFAR-10 Batch 1:  loss and accuracy: 2.0171, 0.2994\n",
      "Epoch 403, CIFAR-10 Batch 1:  loss and accuracy: 2.0130, 0.2976\n",
      "Epoch 404, CIFAR-10 Batch 1:  loss and accuracy: 2.0199, 0.3002\n",
      "Epoch 405, CIFAR-10 Batch 1:  loss and accuracy: 2.0751, 0.2676\n",
      "Epoch 406, CIFAR-10 Batch 1:  loss and accuracy: 2.0100, 0.3048\n",
      "Epoch 407, CIFAR-10 Batch 1:  loss and accuracy: 2.0304, 0.2928\n",
      "Epoch 408, CIFAR-10 Batch 1:  loss and accuracy: 2.0051, 0.2992\n",
      "Epoch 409, CIFAR-10 Batch 1:  loss and accuracy: 2.0003, 0.3056\n",
      "Epoch 410, CIFAR-10 Batch 1:  loss and accuracy: 2.0083, 0.2928\n",
      "Epoch 411, CIFAR-10 Batch 1:  loss and accuracy: 2.0354, 0.2854\n",
      "Epoch 412, CIFAR-10 Batch 1:  loss and accuracy: 2.0310, 0.2888\n",
      "Epoch 413, CIFAR-10 Batch 1:  loss and accuracy: 2.0142, 0.3044\n",
      "Epoch 414, CIFAR-10 Batch 1:  loss and accuracy: 2.0076, 0.3016\n",
      "Epoch 415, CIFAR-10 Batch 1:  loss and accuracy: 2.0204, 0.2974\n",
      "Epoch 416, CIFAR-10 Batch 1:  loss and accuracy: 2.0022, 0.3250\n",
      "Epoch 417, CIFAR-10 Batch 1:  loss and accuracy: 2.0333, 0.3050\n",
      "Epoch 418, CIFAR-10 Batch 1:  loss and accuracy: 2.0440, 0.2950\n",
      "Epoch 419, CIFAR-10 Batch 1:  loss and accuracy: 2.0509, 0.2822\n",
      "Epoch 420, CIFAR-10 Batch 1:  loss and accuracy: 2.0098, 0.3056\n",
      "Epoch 421, CIFAR-10 Batch 1:  loss and accuracy: 2.0323, 0.3030\n",
      "Epoch 422, CIFAR-10 Batch 1:  loss and accuracy: 2.0370, 0.2924\n",
      "Epoch 423, CIFAR-10 Batch 1:  loss and accuracy: 1.9985, 0.3182\n",
      "Epoch 424, CIFAR-10 Batch 1:  loss and accuracy: 2.0288, 0.3098\n",
      "Epoch 425, CIFAR-10 Batch 1:  loss and accuracy: 1.9907, 0.3310\n",
      "Epoch 426, CIFAR-10 Batch 1:  loss and accuracy: 2.0491, 0.2888\n",
      "Epoch 427, CIFAR-10 Batch 1:  loss and accuracy: 2.0290, 0.2976\n",
      "Epoch 428, CIFAR-10 Batch 1:  loss and accuracy: 2.0303, 0.2934\n",
      "Epoch 429, CIFAR-10 Batch 1:  loss and accuracy: 2.0073, 0.3034\n",
      "Epoch 430, CIFAR-10 Batch 1:  loss and accuracy: 2.0081, 0.3116\n",
      "Epoch 431, CIFAR-10 Batch 1:  loss and accuracy: 1.9901, 0.3126\n",
      "Epoch 432, CIFAR-10 Batch 1:  loss and accuracy: 1.9943, 0.3144\n",
      "Epoch 433, CIFAR-10 Batch 1:  loss and accuracy: 2.0092, 0.3028\n",
      "Epoch 434, CIFAR-10 Batch 1:  loss and accuracy: 2.0515, 0.2786\n",
      "Epoch 435, CIFAR-10 Batch 1:  loss and accuracy: 2.0067, 0.3082\n",
      "Epoch 436, CIFAR-10 Batch 1:  loss and accuracy: 2.0208, 0.3140\n",
      "Epoch 437, CIFAR-10 Batch 1:  loss and accuracy: 2.0367, 0.2956\n",
      "Epoch 438, CIFAR-10 Batch 1:  loss and accuracy: 2.0123, 0.3094\n",
      "Epoch 439, CIFAR-10 Batch 1:  loss and accuracy: 1.9880, 0.3200\n",
      "Epoch 440, CIFAR-10 Batch 1:  loss and accuracy: 2.0253, 0.3042\n",
      "Epoch 441, CIFAR-10 Batch 1:  loss and accuracy: 2.0326, 0.2964\n",
      "Epoch 442, CIFAR-10 Batch 1:  loss and accuracy: 2.0015, 0.3130\n",
      "Epoch 443, CIFAR-10 Batch 1:  loss and accuracy: 2.0332, 0.2918\n",
      "Epoch 444, CIFAR-10 Batch 1:  loss and accuracy: 2.0228, 0.3016\n",
      "Epoch 445, CIFAR-10 Batch 1:  loss and accuracy: 2.0324, 0.3008\n",
      "Epoch 446, CIFAR-10 Batch 1:  loss and accuracy: 1.9831, 0.3284\n",
      "Epoch 447, CIFAR-10 Batch 1:  loss and accuracy: 2.0066, 0.3166\n",
      "Epoch 448, CIFAR-10 Batch 1:  loss and accuracy: 2.0274, 0.3018\n",
      "Epoch 449, CIFAR-10 Batch 1:  loss and accuracy: 1.9909, 0.3192\n",
      "Epoch 450, CIFAR-10 Batch 1:  loss and accuracy: 1.9814, 0.3332\n",
      "Epoch 451, CIFAR-10 Batch 1:  loss and accuracy: 2.0154, 0.3072\n",
      "Epoch 452, CIFAR-10 Batch 1:  loss and accuracy: 2.0428, 0.2968\n",
      "Epoch 453, CIFAR-10 Batch 1:  loss and accuracy: 2.0064, 0.3170\n",
      "Epoch 454, CIFAR-10 Batch 1:  loss and accuracy: 1.9885, 0.3228\n",
      "Epoch 455, CIFAR-10 Batch 1:  loss and accuracy: 1.9997, 0.3186\n",
      "Epoch 456, CIFAR-10 Batch 1:  loss and accuracy: 1.9831, 0.3188\n",
      "Epoch 457, CIFAR-10 Batch 1:  loss and accuracy: 2.0215, 0.3062\n",
      "Epoch 458, CIFAR-10 Batch 1:  loss and accuracy: 2.0287, 0.3066\n",
      "Epoch 459, CIFAR-10 Batch 1:  loss and accuracy: 1.9870, 0.3176\n",
      "Epoch 460, CIFAR-10 Batch 1:  loss and accuracy: 1.9933, 0.3218\n",
      "Epoch 461, CIFAR-10 Batch 1:  loss and accuracy: 1.9867, 0.3188\n",
      "Epoch 462, CIFAR-10 Batch 1:  loss and accuracy: 1.9897, 0.3220\n",
      "Epoch 463, CIFAR-10 Batch 1:  loss and accuracy: 1.9854, 0.3162\n",
      "Epoch 464, CIFAR-10 Batch 1:  loss and accuracy: 1.9768, 0.3330\n",
      "Epoch 465, CIFAR-10 Batch 1:  loss and accuracy: 2.0182, 0.3024\n",
      "Epoch 466, CIFAR-10 Batch 1:  loss and accuracy: 1.9926, 0.3244\n",
      "Epoch 467, CIFAR-10 Batch 1:  loss and accuracy: 2.0249, 0.3010\n",
      "Epoch 468, CIFAR-10 Batch 1:  loss and accuracy: 1.9845, 0.3436\n",
      "Epoch 469, CIFAR-10 Batch 1:  loss and accuracy: 1.9921, 0.3326\n",
      "Epoch 470, CIFAR-10 Batch 1:  loss and accuracy: 2.0004, 0.3272\n",
      "Epoch 471, CIFAR-10 Batch 1:  loss and accuracy: 2.0164, 0.3030\n",
      "Epoch 472, CIFAR-10 Batch 1:  loss and accuracy: 1.9814, 0.3214\n",
      "Epoch 473, CIFAR-10 Batch 1:  loss and accuracy: 2.0105, 0.3192\n",
      "Epoch 474, CIFAR-10 Batch 1:  loss and accuracy: 2.0098, 0.3216\n",
      "Epoch 475, CIFAR-10 Batch 1:  loss and accuracy: 2.0044, 0.3072\n",
      "Epoch 476, CIFAR-10 Batch 1:  loss and accuracy: 2.0077, 0.3134\n",
      "Epoch 477, CIFAR-10 Batch 1:  loss and accuracy: 2.0233, 0.3032\n",
      "Epoch 478, CIFAR-10 Batch 1:  loss and accuracy: 2.0265, 0.3004\n",
      "Epoch 479, CIFAR-10 Batch 1:  loss and accuracy: 2.0027, 0.3156\n",
      "Epoch 480, CIFAR-10 Batch 1:  loss and accuracy: 2.0091, 0.3152\n",
      "Epoch 481, CIFAR-10 Batch 1:  loss and accuracy: 2.0098, 0.3048\n",
      "Epoch 482, CIFAR-10 Batch 1:  loss and accuracy: 1.9930, 0.3004\n",
      "Epoch 483, CIFAR-10 Batch 1:  loss and accuracy: 1.9837, 0.3092\n",
      "Epoch 484, CIFAR-10 Batch 1:  loss and accuracy: 2.0212, 0.2944\n",
      "Epoch 485, CIFAR-10 Batch 1:  loss and accuracy: 2.0047, 0.2940\n",
      "Epoch 486, CIFAR-10 Batch 1:  loss and accuracy: 2.0098, 0.2908\n",
      "Epoch 487, CIFAR-10 Batch 1:  loss and accuracy: 1.9995, 0.3162\n",
      "Epoch 488, CIFAR-10 Batch 1:  loss and accuracy: 1.9933, 0.3118\n",
      "Epoch 489, CIFAR-10 Batch 1:  loss and accuracy: 1.9946, 0.3214\n",
      "Epoch 490, CIFAR-10 Batch 1:  loss and accuracy: 2.0252, 0.2952\n",
      "Epoch 491, CIFAR-10 Batch 1:  loss and accuracy: 2.0369, 0.2886\n",
      "Epoch 492, CIFAR-10 Batch 1:  loss and accuracy: 2.0202, 0.3038\n",
      "Epoch 493, CIFAR-10 Batch 1:  loss and accuracy: 2.0319, 0.2938\n",
      "Epoch 494, CIFAR-10 Batch 1:  loss and accuracy: 2.0107, 0.3144\n",
      "Epoch 495, CIFAR-10 Batch 1:  loss and accuracy: 2.0268, 0.2836\n",
      "Epoch 496, CIFAR-10 Batch 1:  loss and accuracy: 2.0187, 0.3076\n",
      "Epoch 497, CIFAR-10 Batch 1:  loss and accuracy: 1.9789, 0.3322\n",
      "Epoch 498, CIFAR-10 Batch 1:  loss and accuracy: 2.0182, 0.3072\n",
      "Epoch 499, CIFAR-10 Batch 1:  loss and accuracy: 2.0185, 0.3080\n",
      "Epoch 500, CIFAR-10 Batch 1:  loss and accuracy: 1.9748, 0.3348\n",
      "Epoch 501, CIFAR-10 Batch 1:  loss and accuracy: 1.9942, 0.3072\n",
      "Epoch 502, CIFAR-10 Batch 1:  loss and accuracy: 1.9857, 0.3182\n",
      "Epoch 503, CIFAR-10 Batch 1:  loss and accuracy: 1.9855, 0.3222\n",
      "Epoch 504, CIFAR-10 Batch 1:  loss and accuracy: 2.0003, 0.3132\n",
      "Epoch 505, CIFAR-10 Batch 1:  loss and accuracy: 2.0092, 0.3074\n",
      "Epoch 506, CIFAR-10 Batch 1:  loss and accuracy: 1.9993, 0.3178\n",
      "Epoch 507, CIFAR-10 Batch 1:  loss and accuracy: 2.0175, 0.3106\n",
      "Epoch 508, CIFAR-10 Batch 1:  loss and accuracy: 1.9897, 0.3310\n",
      "Epoch 509, CIFAR-10 Batch 1:  loss and accuracy: 1.9627, 0.3410\n",
      "Epoch 510, CIFAR-10 Batch 1:  loss and accuracy: 1.9949, 0.3096\n",
      "Epoch 511, CIFAR-10 Batch 1:  loss and accuracy: 2.0006, 0.2988\n",
      "Epoch 512, CIFAR-10 Batch 1:  loss and accuracy: 1.9666, 0.3332\n",
      "Epoch 513, CIFAR-10 Batch 1:  loss and accuracy: 2.0028, 0.3106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514, CIFAR-10 Batch 1:  loss and accuracy: 2.0168, 0.2966\n",
      "Epoch 515, CIFAR-10 Batch 1:  loss and accuracy: 1.9648, 0.3360\n",
      "Epoch 516, CIFAR-10 Batch 1:  loss and accuracy: 2.0059, 0.2978\n",
      "Epoch 517, CIFAR-10 Batch 1:  loss and accuracy: 1.9945, 0.3076\n",
      "Epoch 518, CIFAR-10 Batch 1:  loss and accuracy: 1.9803, 0.3236\n",
      "Epoch 519, CIFAR-10 Batch 1:  loss and accuracy: 1.9857, 0.3230\n",
      "Epoch 520, CIFAR-10 Batch 1:  loss and accuracy: 1.9875, 0.3222\n",
      "Epoch 521, CIFAR-10 Batch 1:  loss and accuracy: 2.0263, 0.2864\n",
      "Epoch 522, CIFAR-10 Batch 1:  loss and accuracy: 1.9925, 0.3144\n",
      "Epoch 523, CIFAR-10 Batch 1:  loss and accuracy: 2.0322, 0.2956\n",
      "Epoch 524, CIFAR-10 Batch 1:  loss and accuracy: 2.0407, 0.2868\n",
      "Epoch 525, CIFAR-10 Batch 1:  loss and accuracy: 1.9987, 0.3024\n",
      "Epoch 526, CIFAR-10 Batch 1:  loss and accuracy: 2.0245, 0.2824\n",
      "Epoch 527, CIFAR-10 Batch 1:  loss and accuracy: 2.0269, 0.2806\n",
      "Epoch 528, CIFAR-10 Batch 1:  loss and accuracy: 2.0202, 0.3016\n",
      "Epoch 529, CIFAR-10 Batch 1:  loss and accuracy: 2.0185, 0.3140\n",
      "Epoch 530, CIFAR-10 Batch 1:  loss and accuracy: 2.0158, 0.3050\n",
      "Epoch 531, CIFAR-10 Batch 1:  loss and accuracy: 2.0103, 0.3120\n",
      "Epoch 532, CIFAR-10 Batch 1:  loss and accuracy: 2.0243, 0.3008\n",
      "Epoch 533, CIFAR-10 Batch 1:  loss and accuracy: 2.0139, 0.2984\n",
      "Epoch 534, CIFAR-10 Batch 1:  loss and accuracy: 2.0553, 0.2708\n",
      "Epoch 535, CIFAR-10 Batch 1:  loss and accuracy: 2.0049, 0.3076\n",
      "Epoch 536, CIFAR-10 Batch 1:  loss and accuracy: 2.0085, 0.3050\n",
      "Epoch 537, CIFAR-10 Batch 1:  loss and accuracy: 2.0073, 0.3046\n",
      "Epoch 538, CIFAR-10 Batch 1:  loss and accuracy: 1.9944, 0.3178\n",
      "Epoch 539, CIFAR-10 Batch 1:  loss and accuracy: 1.9909, 0.3272\n",
      "Epoch 540, CIFAR-10 Batch 1:  loss and accuracy: 2.0181, 0.2918\n",
      "Epoch 541, CIFAR-10 Batch 1:  loss and accuracy: 1.9814, 0.3206\n",
      "Epoch 542, CIFAR-10 Batch 1:  loss and accuracy: 1.9956, 0.3040\n",
      "Epoch 543, CIFAR-10 Batch 1:  loss and accuracy: 1.9972, 0.3050\n",
      "Epoch 544, CIFAR-10 Batch 1:  loss and accuracy: 1.9981, 0.3070\n",
      "Epoch 545, CIFAR-10 Batch 1:  loss and accuracy: 2.0078, 0.2902\n",
      "Epoch 546, CIFAR-10 Batch 1:  loss and accuracy: 2.0194, 0.2824\n",
      "Epoch 547, CIFAR-10 Batch 1:  loss and accuracy: 2.0102, 0.2986\n",
      "Epoch 548, CIFAR-10 Batch 1:  loss and accuracy: 1.9837, 0.3132\n",
      "Epoch 549, CIFAR-10 Batch 1:  loss and accuracy: 1.9744, 0.3112\n",
      "Epoch 550, CIFAR-10 Batch 1:  loss and accuracy: 1.9818, 0.3050\n",
      "Epoch 551, CIFAR-10 Batch 1:  loss and accuracy: 1.9994, 0.2898\n",
      "Epoch 552, CIFAR-10 Batch 1:  loss and accuracy: 1.9872, 0.3050\n",
      "Epoch 553, CIFAR-10 Batch 1:  loss and accuracy: 1.9972, 0.2968\n",
      "Epoch 554, CIFAR-10 Batch 1:  loss and accuracy: 2.0026, 0.2954\n",
      "Epoch 555, CIFAR-10 Batch 1:  loss and accuracy: 2.0255, 0.2902\n",
      "Epoch 556, CIFAR-10 Batch 1:  loss and accuracy: 2.0215, 0.2790\n",
      "Epoch 557, CIFAR-10 Batch 1:  loss and accuracy: 1.9936, 0.3024\n",
      "Epoch 558, CIFAR-10 Batch 1:  loss and accuracy: 2.0164, 0.2904\n",
      "Epoch 559, CIFAR-10 Batch 1:  loss and accuracy: 1.9672, 0.3252\n",
      "Epoch 560, CIFAR-10 Batch 1:  loss and accuracy: 1.9547, 0.3262\n",
      "Epoch 561, CIFAR-10 Batch 1:  loss and accuracy: 2.0100, 0.2912\n",
      "Epoch 562, CIFAR-10 Batch 1:  loss and accuracy: 2.0031, 0.2968\n",
      "Epoch 563, CIFAR-10 Batch 1:  loss and accuracy: 1.9987, 0.3052\n",
      "Epoch 564, CIFAR-10 Batch 1:  loss and accuracy: 2.0039, 0.3046\n",
      "Epoch 565, CIFAR-10 Batch 1:  loss and accuracy: 1.9973, 0.3098\n",
      "Epoch 566, CIFAR-10 Batch 1:  loss and accuracy: 1.9809, 0.3244\n",
      "Epoch 567, CIFAR-10 Batch 1:  loss and accuracy: 2.0017, 0.2982\n",
      "Epoch 568, CIFAR-10 Batch 1:  loss and accuracy: 1.9582, 0.3314\n",
      "Epoch 569, CIFAR-10 Batch 1:  loss and accuracy: 1.9918, 0.3146\n",
      "Epoch 570, CIFAR-10 Batch 1:  loss and accuracy: 1.9942, 0.3166\n",
      "Epoch 571, CIFAR-10 Batch 1:  loss and accuracy: 2.0047, 0.2930\n",
      "Epoch 572, CIFAR-10 Batch 1:  loss and accuracy: 1.9925, 0.3124\n",
      "Epoch 573, CIFAR-10 Batch 1:  loss and accuracy: 2.0014, 0.3074\n",
      "Epoch 574, CIFAR-10 Batch 1:  loss and accuracy: 1.9901, 0.3216\n",
      "Epoch 575, CIFAR-10 Batch 1:  loss and accuracy: 1.9859, 0.3324\n",
      "Epoch 576, CIFAR-10 Batch 1:  loss and accuracy: 1.9942, 0.3104\n",
      "Epoch 577, CIFAR-10 Batch 1:  loss and accuracy: 1.9954, 0.3080\n",
      "Epoch 578, CIFAR-10 Batch 1:  loss and accuracy: 1.9979, 0.3094\n",
      "Epoch 579, CIFAR-10 Batch 1:  loss and accuracy: 1.9711, 0.3276\n",
      "Epoch 580, CIFAR-10 Batch 1:  loss and accuracy: 1.9748, 0.3216\n",
      "Epoch 581, CIFAR-10 Batch 1:  loss and accuracy: 1.9807, 0.3170\n",
      "Epoch 582, CIFAR-10 Batch 1:  loss and accuracy: 1.9840, 0.3088\n",
      "Epoch 583, CIFAR-10 Batch 1:  loss and accuracy: 1.9817, 0.3128\n",
      "Epoch 584, CIFAR-10 Batch 1:  loss and accuracy: 1.9933, 0.3070\n",
      "Epoch 585, CIFAR-10 Batch 1:  loss and accuracy: 1.9948, 0.3008\n",
      "Epoch 586, CIFAR-10 Batch 1:  loss and accuracy: 1.9835, 0.3146\n",
      "Epoch 587, CIFAR-10 Batch 1:  loss and accuracy: 1.9774, 0.3146\n",
      "Epoch 588, CIFAR-10 Batch 1:  loss and accuracy: 1.9950, 0.3000\n",
      "Epoch 589, CIFAR-10 Batch 1:  loss and accuracy: 1.9745, 0.3280\n",
      "Epoch 590, CIFAR-10 Batch 1:  loss and accuracy: 1.9662, 0.3260\n",
      "Epoch 591, CIFAR-10 Batch 1:  loss and accuracy: 2.0348, 0.2770\n",
      "Epoch 592, CIFAR-10 Batch 1:  loss and accuracy: 1.9932, 0.3128\n",
      "Epoch 593, CIFAR-10 Batch 1:  loss and accuracy: 1.9612, 0.3346\n",
      "Epoch 594, CIFAR-10 Batch 1:  loss and accuracy: 1.9504, 0.3342\n",
      "Epoch 595, CIFAR-10 Batch 1:  loss and accuracy: 1.9712, 0.3176\n",
      "Epoch 596, CIFAR-10 Batch 1:  loss and accuracy: 1.9967, 0.3010\n",
      "Epoch 597, CIFAR-10 Batch 1:  loss and accuracy: 1.9717, 0.3336\n",
      "Epoch 598, CIFAR-10 Batch 1:  loss and accuracy: 2.0056, 0.2902\n",
      "Epoch 599, CIFAR-10 Batch 1:  loss and accuracy: 1.9682, 0.3118\n",
      "Epoch 600, CIFAR-10 Batch 1:  loss and accuracy: 1.9618, 0.3358\n",
      "Epoch 601, CIFAR-10 Batch 1:  loss and accuracy: 1.9775, 0.3120\n",
      "Epoch 602, CIFAR-10 Batch 1:  loss and accuracy: 1.9791, 0.3112\n",
      "Epoch 603, CIFAR-10 Batch 1:  loss and accuracy: 1.9771, 0.3214\n",
      "Epoch 604, CIFAR-10 Batch 1:  loss and accuracy: 1.9687, 0.3274\n",
      "Epoch 605, CIFAR-10 Batch 1:  loss and accuracy: 1.9730, 0.3244\n",
      "Epoch 606, CIFAR-10 Batch 1:  loss and accuracy: 1.9915, 0.3194\n",
      "Epoch 607, CIFAR-10 Batch 1:  loss and accuracy: 2.0180, 0.2998\n",
      "Epoch 608, CIFAR-10 Batch 1:  loss and accuracy: 1.9744, 0.3306\n",
      "Epoch 609, CIFAR-10 Batch 1:  loss and accuracy: 1.9889, 0.3110\n",
      "Epoch 610, CIFAR-10 Batch 1:  loss and accuracy: 2.0135, 0.2918\n",
      "Epoch 611, CIFAR-10 Batch 1:  loss and accuracy: 1.9669, 0.3236\n",
      "Epoch 612, CIFAR-10 Batch 1:  loss and accuracy: 1.9580, 0.3306\n",
      "Epoch 613, CIFAR-10 Batch 1:  loss and accuracy: 1.9801, 0.3230\n",
      "Epoch 614, CIFAR-10 Batch 1:  loss and accuracy: 1.9499, 0.3456\n",
      "Epoch 615, CIFAR-10 Batch 1:  loss and accuracy: 1.9979, 0.3018\n",
      "Epoch 616, CIFAR-10 Batch 1:  loss and accuracy: 1.9875, 0.3038\n",
      "Epoch 617, CIFAR-10 Batch 1:  loss and accuracy: 1.9940, 0.2994\n",
      "Epoch 618, CIFAR-10 Batch 1:  loss and accuracy: 1.9777, 0.3136\n",
      "Epoch 619, CIFAR-10 Batch 1:  loss and accuracy: 1.9859, 0.3198\n",
      "Epoch 620, CIFAR-10 Batch 1:  loss and accuracy: 1.9658, 0.3272\n",
      "Epoch 621, CIFAR-10 Batch 1:  loss and accuracy: 1.9665, 0.3214\n",
      "Epoch 622, CIFAR-10 Batch 1:  loss and accuracy: 1.9876, 0.3222\n",
      "Epoch 623, CIFAR-10 Batch 1:  loss and accuracy: 1.9814, 0.3152\n",
      "Epoch 624, CIFAR-10 Batch 1:  loss and accuracy: 1.9945, 0.3084\n",
      "Epoch 625, CIFAR-10 Batch 1:  loss and accuracy: 1.9450, 0.3382\n",
      "Epoch 626, CIFAR-10 Batch 1:  loss and accuracy: 1.9529, 0.3312\n",
      "Epoch 627, CIFAR-10 Batch 1:  loss and accuracy: 1.9595, 0.3248\n",
      "Epoch 628, CIFAR-10 Batch 1:  loss and accuracy: 1.9549, 0.3308\n",
      "Epoch 629, CIFAR-10 Batch 1:  loss and accuracy: 1.9652, 0.3254\n",
      "Epoch 630, CIFAR-10 Batch 1:  loss and accuracy: 1.9762, 0.3112\n",
      "Epoch 631, CIFAR-10 Batch 1:  loss and accuracy: 1.9552, 0.3398\n",
      "Epoch 632, CIFAR-10 Batch 1:  loss and accuracy: 1.9726, 0.3254\n",
      "Epoch 633, CIFAR-10 Batch 1:  loss and accuracy: 1.9600, 0.3358\n",
      "Epoch 634, CIFAR-10 Batch 1:  loss and accuracy: 1.9683, 0.3206\n",
      "Epoch 635, CIFAR-10 Batch 1:  loss and accuracy: 1.9792, 0.3218\n",
      "Epoch 636, CIFAR-10 Batch 1:  loss and accuracy: 2.0124, 0.2940\n",
      "Epoch 637, CIFAR-10 Batch 1:  loss and accuracy: 1.9718, 0.3138\n",
      "Epoch 638, CIFAR-10 Batch 1:  loss and accuracy: 1.9772, 0.3152\n",
      "Epoch 639, CIFAR-10 Batch 1:  loss and accuracy: 1.9832, 0.3088\n",
      "Epoch 640, CIFAR-10 Batch 1:  loss and accuracy: 1.9661, 0.3332\n",
      "Epoch 641, CIFAR-10 Batch 1:  loss and accuracy: 1.9592, 0.3338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642, CIFAR-10 Batch 1:  loss and accuracy: 1.9667, 0.3254\n",
      "Epoch 643, CIFAR-10 Batch 1:  loss and accuracy: 1.9672, 0.3326\n",
      "Epoch 644, CIFAR-10 Batch 1:  loss and accuracy: 1.9656, 0.3388\n",
      "Epoch 645, CIFAR-10 Batch 1:  loss and accuracy: 1.9947, 0.3166\n",
      "Epoch 646, CIFAR-10 Batch 1:  loss and accuracy: 2.0071, 0.3044\n",
      "Epoch 647, CIFAR-10 Batch 1:  loss and accuracy: 2.0023, 0.3030\n",
      "Epoch 648, CIFAR-10 Batch 1:  loss and accuracy: 1.9996, 0.2924\n",
      "Epoch 649, CIFAR-10 Batch 1:  loss and accuracy: 1.9817, 0.3228\n",
      "Epoch 650, CIFAR-10 Batch 1:  loss and accuracy: 1.9977, 0.3014\n",
      "Epoch 651, CIFAR-10 Batch 1:  loss and accuracy: 2.0183, 0.2960\n",
      "Epoch 652, CIFAR-10 Batch 1:  loss and accuracy: 2.0093, 0.3006\n",
      "Epoch 653, CIFAR-10 Batch 1:  loss and accuracy: 1.9948, 0.3144\n",
      "Epoch 654, CIFAR-10 Batch 1:  loss and accuracy: 1.9922, 0.3152\n",
      "Epoch 655, CIFAR-10 Batch 1:  loss and accuracy: 1.9750, 0.3296\n",
      "Epoch 656, CIFAR-10 Batch 1:  loss and accuracy: 2.0081, 0.3188\n",
      "Epoch 657, CIFAR-10 Batch 1:  loss and accuracy: 2.0015, 0.3312\n",
      "Epoch 658, CIFAR-10 Batch 1:  loss and accuracy: 1.9861, 0.3328\n",
      "Epoch 659, CIFAR-10 Batch 1:  loss and accuracy: 1.9943, 0.3218\n",
      "Epoch 660, CIFAR-10 Batch 1:  loss and accuracy: 1.9792, 0.3306\n",
      "Epoch 661, CIFAR-10 Batch 1:  loss and accuracy: 1.9853, 0.3174\n",
      "Epoch 662, CIFAR-10 Batch 1:  loss and accuracy: 1.9710, 0.3408\n",
      "Epoch 663, CIFAR-10 Batch 1:  loss and accuracy: 1.9908, 0.3170\n",
      "Epoch 664, CIFAR-10 Batch 1:  loss and accuracy: 1.9846, 0.3176\n",
      "Epoch 665, CIFAR-10 Batch 1:  loss and accuracy: 1.9995, 0.3070\n",
      "Epoch 666, CIFAR-10 Batch 1:  loss and accuracy: 1.9961, 0.3120\n",
      "Epoch 667, CIFAR-10 Batch 1:  loss and accuracy: 2.0022, 0.3136\n",
      "Epoch 668, CIFAR-10 Batch 1:  loss and accuracy: 2.0251, 0.2954\n",
      "Epoch 669, CIFAR-10 Batch 1:  loss and accuracy: 1.9969, 0.3182\n",
      "Epoch 670, CIFAR-10 Batch 1:  loss and accuracy: 2.0118, 0.3096\n",
      "Epoch 671, CIFAR-10 Batch 1:  loss and accuracy: 1.9919, 0.3252\n",
      "Epoch 672, CIFAR-10 Batch 1:  loss and accuracy: 1.9686, 0.3354\n",
      "Epoch 673, CIFAR-10 Batch 1:  loss and accuracy: 1.9775, 0.3280\n",
      "Epoch 674, CIFAR-10 Batch 1:  loss and accuracy: 1.9654, 0.3400\n",
      "Epoch 675, CIFAR-10 Batch 1:  loss and accuracy: 1.9848, 0.3324\n",
      "Epoch 676, CIFAR-10 Batch 1:  loss and accuracy: 1.9774, 0.3390\n",
      "Epoch 677, CIFAR-10 Batch 1:  loss and accuracy: 2.0002, 0.3200\n",
      "Epoch 678, CIFAR-10 Batch 1:  loss and accuracy: 1.9735, 0.3342\n",
      "Epoch 679, CIFAR-10 Batch 1:  loss and accuracy: 1.9939, 0.3190\n",
      "Epoch 680, CIFAR-10 Batch 1:  loss and accuracy: 1.9875, 0.3218\n",
      "Epoch 681, CIFAR-10 Batch 1:  loss and accuracy: 1.9539, 0.3426\n",
      "Epoch 682, CIFAR-10 Batch 1:  loss and accuracy: 1.9941, 0.3102\n",
      "Epoch 683, CIFAR-10 Batch 1:  loss and accuracy: 1.9706, 0.3366\n",
      "Epoch 684, CIFAR-10 Batch 1:  loss and accuracy: 1.9617, 0.3380\n",
      "Epoch 685, CIFAR-10 Batch 1:  loss and accuracy: 1.9812, 0.3372\n",
      "Epoch 686, CIFAR-10 Batch 1:  loss and accuracy: 1.9863, 0.3190\n",
      "Epoch 687, CIFAR-10 Batch 1:  loss and accuracy: 1.9609, 0.3408\n",
      "Epoch 688, CIFAR-10 Batch 1:  loss and accuracy: 1.9883, 0.3244\n",
      "Epoch 689, CIFAR-10 Batch 1:  loss and accuracy: 1.9573, 0.3514\n",
      "Epoch 690, CIFAR-10 Batch 1:  loss and accuracy: 1.9919, 0.3210\n",
      "Epoch 691, CIFAR-10 Batch 1:  loss and accuracy: 1.9855, 0.3308\n",
      "Epoch 692, CIFAR-10 Batch 1:  loss and accuracy: 1.9873, 0.3334\n",
      "Epoch 693, CIFAR-10 Batch 1:  loss and accuracy: 2.0020, 0.3214\n",
      "Epoch 694, CIFAR-10 Batch 1:  loss and accuracy: 1.9830, 0.3412\n",
      "Epoch 695, CIFAR-10 Batch 1:  loss and accuracy: 1.9650, 0.3482\n",
      "Epoch 696, CIFAR-10 Batch 1:  loss and accuracy: 1.9723, 0.3436\n",
      "Epoch 697, CIFAR-10 Batch 1:  loss and accuracy: 2.0001, 0.3234\n",
      "Epoch 698, CIFAR-10 Batch 1:  loss and accuracy: 1.9738, 0.3450\n",
      "Epoch 699, CIFAR-10 Batch 1:  loss and accuracy: 1.9848, 0.3256\n",
      "Epoch 700, CIFAR-10 Batch 1:  loss and accuracy: 1.9763, 0.3278\n",
      "Epoch 701, CIFAR-10 Batch 1:  loss and accuracy: 1.9617, 0.3510\n",
      "Epoch 702, CIFAR-10 Batch 1:  loss and accuracy: 1.9571, 0.3578\n",
      "Epoch 703, CIFAR-10 Batch 1:  loss and accuracy: 1.9873, 0.3504\n",
      "Epoch 704, CIFAR-10 Batch 1:  loss and accuracy: 1.9564, 0.3560\n",
      "Epoch 705, CIFAR-10 Batch 1:  loss and accuracy: 1.9676, 0.3440\n",
      "Epoch 706, CIFAR-10 Batch 1:  loss and accuracy: 2.0108, 0.3128\n",
      "Epoch 707, CIFAR-10 Batch 1:  loss and accuracy: 1.9709, 0.3276\n",
      "Epoch 708, CIFAR-10 Batch 1:  loss and accuracy: 1.9736, 0.3282\n",
      "Epoch 709, CIFAR-10 Batch 1:  loss and accuracy: 1.9605, 0.3456\n",
      "Epoch 710, CIFAR-10 Batch 1:  loss and accuracy: 1.9708, 0.3374\n",
      "Epoch 711, CIFAR-10 Batch 1:  loss and accuracy: 2.0002, 0.3214\n",
      "Epoch 712, CIFAR-10 Batch 1:  loss and accuracy: 1.9748, 0.3290\n",
      "Epoch 713, CIFAR-10 Batch 1:  loss and accuracy: 1.9696, 0.3414\n",
      "Epoch 714, CIFAR-10 Batch 1:  loss and accuracy: 1.9614, 0.3536\n",
      "Epoch 715, CIFAR-10 Batch 1:  loss and accuracy: 2.0097, 0.3146\n",
      "Epoch 716, CIFAR-10 Batch 1:  loss and accuracy: 1.9493, 0.3596\n",
      "Epoch 717, CIFAR-10 Batch 1:  loss and accuracy: 1.9515, 0.3582\n",
      "Epoch 718, CIFAR-10 Batch 1:  loss and accuracy: 1.9662, 0.3436\n",
      "Epoch 719, CIFAR-10 Batch 1:  loss and accuracy: 1.9597, 0.3414\n",
      "Epoch 720, CIFAR-10 Batch 1:  loss and accuracy: 1.9552, 0.3582\n",
      "Epoch 721, CIFAR-10 Batch 1:  loss and accuracy: 1.9611, 0.3472\n",
      "Epoch 722, CIFAR-10 Batch 1:  loss and accuracy: 1.9608, 0.3468\n",
      "Epoch 723, CIFAR-10 Batch 1:  loss and accuracy: 1.9502, 0.3698\n",
      "Epoch 724, CIFAR-10 Batch 1:  loss and accuracy: 1.9677, 0.3538\n",
      "Epoch 725, CIFAR-10 Batch 1:  loss and accuracy: 1.9885, 0.3344\n",
      "Epoch 726, CIFAR-10 Batch 1:  loss and accuracy: 1.9956, 0.3246\n",
      "Epoch 727, CIFAR-10 Batch 1:  loss and accuracy: 1.9777, 0.3278\n",
      "Epoch 728, CIFAR-10 Batch 1:  loss and accuracy: 1.9563, 0.3508\n",
      "Epoch 729, CIFAR-10 Batch 1:  loss and accuracy: 1.9526, 0.3466\n",
      "Epoch 730, CIFAR-10 Batch 1:  loss and accuracy: 1.9532, 0.3490\n",
      "Epoch 731, CIFAR-10 Batch 1:  loss and accuracy: 1.9773, 0.3252\n",
      "Epoch 732, CIFAR-10 Batch 1:  loss and accuracy: 1.9910, 0.3270\n",
      "Epoch 733, CIFAR-10 Batch 1:  loss and accuracy: 1.9858, 0.3272\n",
      "Epoch 734, CIFAR-10 Batch 1:  loss and accuracy: 1.9699, 0.3340\n",
      "Epoch 735, CIFAR-10 Batch 1:  loss and accuracy: 1.9581, 0.3432\n",
      "Epoch 736, CIFAR-10 Batch 1:  loss and accuracy: 1.9496, 0.3550\n",
      "Epoch 737, CIFAR-10 Batch 1:  loss and accuracy: 1.9460, 0.3588\n",
      "Epoch 738, CIFAR-10 Batch 1:  loss and accuracy: 1.9716, 0.3282\n",
      "Epoch 739, CIFAR-10 Batch 1:  loss and accuracy: 2.0017, 0.3164\n",
      "Epoch 740, CIFAR-10 Batch 1:  loss and accuracy: 1.9743, 0.3278\n",
      "Epoch 741, CIFAR-10 Batch 1:  loss and accuracy: 1.9523, 0.3482\n",
      "Epoch 742, CIFAR-10 Batch 1:  loss and accuracy: 1.9600, 0.3294\n",
      "Epoch 743, CIFAR-10 Batch 1:  loss and accuracy: 1.9765, 0.3404\n",
      "Epoch 744, CIFAR-10 Batch 1:  loss and accuracy: 1.9385, 0.3730\n",
      "Epoch 745, CIFAR-10 Batch 1:  loss and accuracy: 1.9599, 0.3412\n",
      "Epoch 746, CIFAR-10 Batch 1:  loss and accuracy: 1.9493, 0.3530\n",
      "Epoch 747, CIFAR-10 Batch 1:  loss and accuracy: 1.9478, 0.3562\n",
      "Epoch 748, CIFAR-10 Batch 1:  loss and accuracy: 1.9747, 0.3352\n",
      "Epoch 749, CIFAR-10 Batch 1:  loss and accuracy: 1.9840, 0.3206\n",
      "Epoch 750, CIFAR-10 Batch 1:  loss and accuracy: 1.9801, 0.3266\n",
      "Epoch 751, CIFAR-10 Batch 1:  loss and accuracy: 1.9982, 0.3160\n",
      "Epoch 752, CIFAR-10 Batch 1:  loss and accuracy: 1.9546, 0.3434\n",
      "Epoch 753, CIFAR-10 Batch 1:  loss and accuracy: 1.9800, 0.3264\n",
      "Epoch 754, CIFAR-10 Batch 1:  loss and accuracy: 1.9973, 0.3064\n",
      "Epoch 755, CIFAR-10 Batch 1:  loss and accuracy: 1.9672, 0.3276\n",
      "Epoch 756, CIFAR-10 Batch 1:  loss and accuracy: 1.9633, 0.3204\n",
      "Epoch 757, CIFAR-10 Batch 1:  loss and accuracy: 1.9668, 0.3194\n",
      "Epoch 758, CIFAR-10 Batch 1:  loss and accuracy: 1.9879, 0.3114\n",
      "Epoch 759, CIFAR-10 Batch 1:  loss and accuracy: 1.9566, 0.3292\n",
      "Epoch 760, CIFAR-10 Batch 1:  loss and accuracy: 1.9604, 0.3426\n",
      "Epoch 761, CIFAR-10 Batch 1:  loss and accuracy: 1.9608, 0.3450\n",
      "Epoch 762, CIFAR-10 Batch 1:  loss and accuracy: 1.9450, 0.3596\n",
      "Epoch 763, CIFAR-10 Batch 1:  loss and accuracy: 1.9422, 0.3594\n",
      "Epoch 764, CIFAR-10 Batch 1:  loss and accuracy: 1.9816, 0.3270\n",
      "Epoch 765, CIFAR-10 Batch 1:  loss and accuracy: 1.9725, 0.3370\n",
      "Epoch 766, CIFAR-10 Batch 1:  loss and accuracy: 1.9703, 0.3322\n",
      "Epoch 767, CIFAR-10 Batch 1:  loss and accuracy: 1.9581, 0.3532\n",
      "Epoch 768, CIFAR-10 Batch 1:  loss and accuracy: 1.9500, 0.3550\n",
      "Epoch 769, CIFAR-10 Batch 1:  loss and accuracy: 1.9390, 0.3682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770, CIFAR-10 Batch 1:  loss and accuracy: 1.9499, 0.3596\n",
      "Epoch 771, CIFAR-10 Batch 1:  loss and accuracy: 1.9717, 0.3432\n",
      "Epoch 772, CIFAR-10 Batch 1:  loss and accuracy: 1.9705, 0.3366\n",
      "Epoch 773, CIFAR-10 Batch 1:  loss and accuracy: 1.9579, 0.3446\n",
      "Epoch 774, CIFAR-10 Batch 1:  loss and accuracy: 1.9477, 0.3626\n",
      "Epoch 775, CIFAR-10 Batch 1:  loss and accuracy: 1.9532, 0.3486\n",
      "Epoch 776, CIFAR-10 Batch 1:  loss and accuracy: 1.9632, 0.3450\n",
      "Epoch 777, CIFAR-10 Batch 1:  loss and accuracy: 1.9480, 0.3610\n",
      "Epoch 778, CIFAR-10 Batch 1:  loss and accuracy: 1.9556, 0.3440\n",
      "Epoch 779, CIFAR-10 Batch 1:  loss and accuracy: 1.9385, 0.3610\n",
      "Epoch 780, CIFAR-10 Batch 1:  loss and accuracy: 1.9434, 0.3538\n",
      "Epoch 781, CIFAR-10 Batch 1:  loss and accuracy: 1.9524, 0.3410\n",
      "Epoch 782, CIFAR-10 Batch 1:  loss and accuracy: 1.9420, 0.3634\n",
      "Epoch 783, CIFAR-10 Batch 1:  loss and accuracy: 1.9550, 0.3526\n",
      "Epoch 784, CIFAR-10 Batch 1:  loss and accuracy: 1.9483, 0.3618\n",
      "Epoch 785, CIFAR-10 Batch 1:  loss and accuracy: 1.9690, 0.3474\n",
      "Epoch 786, CIFAR-10 Batch 1:  loss and accuracy: 1.9726, 0.3404\n",
      "Epoch 787, CIFAR-10 Batch 1:  loss and accuracy: 1.9525, 0.3578\n",
      "Epoch 788, CIFAR-10 Batch 1:  loss and accuracy: 1.9808, 0.3388\n",
      "Epoch 789, CIFAR-10 Batch 1:  loss and accuracy: 1.9522, 0.3528\n",
      "Epoch 790, CIFAR-10 Batch 1:  loss and accuracy: 1.9518, 0.3540\n",
      "Epoch 791, CIFAR-10 Batch 1:  loss and accuracy: 1.9309, 0.3714\n",
      "Epoch 792, CIFAR-10 Batch 1:  loss and accuracy: 1.9532, 0.3574\n",
      "Epoch 793, CIFAR-10 Batch 1:  loss and accuracy: 1.9498, 0.3620\n",
      "Epoch 794, CIFAR-10 Batch 1:  loss and accuracy: 1.9474, 0.3598\n",
      "Epoch 795, CIFAR-10 Batch 1:  loss and accuracy: 1.9424, 0.3718\n",
      "Epoch 796, CIFAR-10 Batch 1:  loss and accuracy: 1.9293, 0.3784\n",
      "Epoch 797, CIFAR-10 Batch 1:  loss and accuracy: 1.9542, 0.3552\n",
      "Epoch 798, CIFAR-10 Batch 1:  loss and accuracy: 1.9632, 0.3462\n",
      "Epoch 799, CIFAR-10 Batch 1:  loss and accuracy: 1.9755, 0.3398\n",
      "Epoch 800, CIFAR-10 Batch 1:  loss and accuracy: 1.9639, 0.3522\n",
      "Epoch 801, CIFAR-10 Batch 1:  loss and accuracy: 1.9630, 0.3508\n",
      "Epoch 802, CIFAR-10 Batch 1:  loss and accuracy: 1.9458, 0.3600\n",
      "Epoch 803, CIFAR-10 Batch 1:  loss and accuracy: 1.9453, 0.3654\n",
      "Epoch 804, CIFAR-10 Batch 1:  loss and accuracy: 1.9560, 0.3526\n",
      "Epoch 805, CIFAR-10 Batch 1:  loss and accuracy: 1.9524, 0.3566\n",
      "Epoch 806, CIFAR-10 Batch 1:  loss and accuracy: 1.9569, 0.3546\n",
      "Epoch 807, CIFAR-10 Batch 1:  loss and accuracy: 1.9471, 0.3662\n",
      "Epoch 808, CIFAR-10 Batch 1:  loss and accuracy: 1.9512, 0.3590\n",
      "Epoch 809, CIFAR-10 Batch 1:  loss and accuracy: 1.9488, 0.3528\n",
      "Epoch 810, CIFAR-10 Batch 1:  loss and accuracy: 1.9579, 0.3420\n",
      "Epoch 811, CIFAR-10 Batch 1:  loss and accuracy: 1.9400, 0.3636\n",
      "Epoch 812, CIFAR-10 Batch 1:  loss and accuracy: 1.9392, 0.3598\n",
      "Epoch 813, CIFAR-10 Batch 1:  loss and accuracy: 1.9471, 0.3636\n",
      "Epoch 814, CIFAR-10 Batch 1:  loss and accuracy: 1.9456, 0.3612\n",
      "Epoch 815, CIFAR-10 Batch 1:  loss and accuracy: 1.9622, 0.3562\n",
      "Epoch 816, CIFAR-10 Batch 1:  loss and accuracy: 1.9629, 0.3408\n",
      "Epoch 817, CIFAR-10 Batch 1:  loss and accuracy: 1.9518, 0.3604\n",
      "Epoch 818, CIFAR-10 Batch 1:  loss and accuracy: 1.9491, 0.3566\n",
      "Epoch 819, CIFAR-10 Batch 1:  loss and accuracy: 1.9413, 0.3696\n",
      "Epoch 820, CIFAR-10 Batch 1:  loss and accuracy: 1.9380, 0.3728\n",
      "Epoch 821, CIFAR-10 Batch 1:  loss and accuracy: 1.9444, 0.3656\n",
      "Epoch 822, CIFAR-10 Batch 1:  loss and accuracy: 1.9252, 0.3862\n",
      "Epoch 823, CIFAR-10 Batch 1:  loss and accuracy: 1.9345, 0.3762\n",
      "Epoch 824, CIFAR-10 Batch 1:  loss and accuracy: 1.9482, 0.3646\n",
      "Epoch 825, CIFAR-10 Batch 1:  loss and accuracy: 1.9507, 0.3594\n",
      "Epoch 826, CIFAR-10 Batch 1:  loss and accuracy: 1.9518, 0.3538\n",
      "Epoch 827, CIFAR-10 Batch 1:  loss and accuracy: 1.9562, 0.3558\n",
      "Epoch 828, CIFAR-10 Batch 1:  loss and accuracy: 1.9596, 0.3562\n",
      "Epoch 829, CIFAR-10 Batch 1:  loss and accuracy: 1.9694, 0.3474\n",
      "Epoch 830, CIFAR-10 Batch 1:  loss and accuracy: 1.9548, 0.3598\n",
      "Epoch 831, CIFAR-10 Batch 1:  loss and accuracy: 1.9743, 0.3476\n",
      "Epoch 832, CIFAR-10 Batch 1:  loss and accuracy: 1.9540, 0.3584\n",
      "Epoch 833, CIFAR-10 Batch 1:  loss and accuracy: 1.9430, 0.3726\n",
      "Epoch 834, CIFAR-10 Batch 1:  loss and accuracy: 1.9316, 0.3748\n",
      "Epoch 835, CIFAR-10 Batch 1:  loss and accuracy: 1.9260, 0.3774\n",
      "Epoch 836, CIFAR-10 Batch 1:  loss and accuracy: 1.9718, 0.3386\n",
      "Epoch 837, CIFAR-10 Batch 1:  loss and accuracy: 1.9569, 0.3524\n",
      "Epoch 838, CIFAR-10 Batch 1:  loss and accuracy: 1.9460, 0.3638\n",
      "Epoch 839, CIFAR-10 Batch 1:  loss and accuracy: 1.9811, 0.3488\n",
      "Epoch 840, CIFAR-10 Batch 1:  loss and accuracy: 1.9370, 0.3732\n",
      "Epoch 841, CIFAR-10 Batch 1:  loss and accuracy: 1.9704, 0.3464\n",
      "Epoch 842, CIFAR-10 Batch 1:  loss and accuracy: 1.9630, 0.3588\n",
      "Epoch 843, CIFAR-10 Batch 1:  loss and accuracy: 1.9540, 0.3588\n",
      "Epoch 844, CIFAR-10 Batch 1:  loss and accuracy: 1.9512, 0.3630\n",
      "Epoch 845, CIFAR-10 Batch 1:  loss and accuracy: 1.9416, 0.3724\n",
      "Epoch 846, CIFAR-10 Batch 1:  loss and accuracy: 1.9304, 0.3782\n",
      "Epoch 847, CIFAR-10 Batch 1:  loss and accuracy: 1.9553, 0.3584\n",
      "Epoch 848, CIFAR-10 Batch 1:  loss and accuracy: 1.9535, 0.3602\n",
      "Epoch 849, CIFAR-10 Batch 1:  loss and accuracy: 1.9622, 0.3574\n",
      "Epoch 850, CIFAR-10 Batch 1:  loss and accuracy: 1.9610, 0.3584\n",
      "Epoch 851, CIFAR-10 Batch 1:  loss and accuracy: 1.9400, 0.3692\n",
      "Epoch 852, CIFAR-10 Batch 1:  loss and accuracy: 1.9420, 0.3744\n",
      "Epoch 853, CIFAR-10 Batch 1:  loss and accuracy: 1.9591, 0.3580\n",
      "Epoch 854, CIFAR-10 Batch 1:  loss and accuracy: 1.9572, 0.3468\n",
      "Epoch 855, CIFAR-10 Batch 1:  loss and accuracy: 1.9615, 0.3376\n",
      "Epoch 856, CIFAR-10 Batch 1:  loss and accuracy: 1.9507, 0.3534\n",
      "Epoch 857, CIFAR-10 Batch 1:  loss and accuracy: 1.9449, 0.3692\n",
      "Epoch 858, CIFAR-10 Batch 1:  loss and accuracy: 1.9527, 0.3528\n",
      "Epoch 859, CIFAR-10 Batch 1:  loss and accuracy: 1.9431, 0.3562\n",
      "Epoch 860, CIFAR-10 Batch 1:  loss and accuracy: 1.9253, 0.3838\n",
      "Epoch 861, CIFAR-10 Batch 1:  loss and accuracy: 1.9396, 0.3690\n",
      "Epoch 862, CIFAR-10 Batch 1:  loss and accuracy: 1.9296, 0.3836\n",
      "Epoch 863, CIFAR-10 Batch 1:  loss and accuracy: 1.9419, 0.3650\n",
      "Epoch 864, CIFAR-10 Batch 1:  loss and accuracy: 1.9386, 0.3670\n",
      "Epoch 865, CIFAR-10 Batch 1:  loss and accuracy: 1.9507, 0.3698\n",
      "Epoch 866, CIFAR-10 Batch 1:  loss and accuracy: 1.9608, 0.3644\n",
      "Epoch 867, CIFAR-10 Batch 1:  loss and accuracy: 1.9492, 0.3708\n",
      "Epoch 868, CIFAR-10 Batch 1:  loss and accuracy: 1.9581, 0.3712\n",
      "Epoch 869, CIFAR-10 Batch 1:  loss and accuracy: 1.9402, 0.3774\n",
      "Epoch 870, CIFAR-10 Batch 1:  loss and accuracy: 1.9631, 0.3456\n",
      "Epoch 871, CIFAR-10 Batch 1:  loss and accuracy: 1.9706, 0.3540\n",
      "Epoch 872, CIFAR-10 Batch 1:  loss and accuracy: 1.9961, 0.3164\n",
      "Epoch 873, CIFAR-10 Batch 1:  loss and accuracy: 1.9660, 0.3524\n",
      "Epoch 874, CIFAR-10 Batch 1:  loss and accuracy: 1.9428, 0.3618\n",
      "Epoch 875, CIFAR-10 Batch 1:  loss and accuracy: 1.9572, 0.3560\n",
      "Epoch 876, CIFAR-10 Batch 1:  loss and accuracy: 1.9872, 0.3280\n",
      "Epoch 877, CIFAR-10 Batch 1:  loss and accuracy: 1.9552, 0.3638\n",
      "Epoch 878, CIFAR-10 Batch 1:  loss and accuracy: 1.9632, 0.3578\n",
      "Epoch 879, CIFAR-10 Batch 1:  loss and accuracy: 1.9335, 0.3816\n",
      "Epoch 880, CIFAR-10 Batch 1:  loss and accuracy: 1.9510, 0.3592\n",
      "Epoch 881, CIFAR-10 Batch 1:  loss and accuracy: 1.9409, 0.3662\n",
      "Epoch 882, CIFAR-10 Batch 1:  loss and accuracy: 1.9860, 0.3420\n",
      "Epoch 883, CIFAR-10 Batch 1:  loss and accuracy: 1.9567, 0.3658\n",
      "Epoch 884, CIFAR-10 Batch 1:  loss and accuracy: 1.9511, 0.3588\n",
      "Epoch 885, CIFAR-10 Batch 1:  loss and accuracy: 1.9489, 0.3748\n",
      "Epoch 886, CIFAR-10 Batch 1:  loss and accuracy: 1.9608, 0.3574\n",
      "Epoch 887, CIFAR-10 Batch 1:  loss and accuracy: 1.9274, 0.3944\n",
      "Epoch 888, CIFAR-10 Batch 1:  loss and accuracy: 1.9312, 0.3778\n",
      "Epoch 889, CIFAR-10 Batch 1:  loss and accuracy: 1.9380, 0.3798\n",
      "Epoch 890, CIFAR-10 Batch 1:  loss and accuracy: 1.9640, 0.3536\n",
      "Epoch 891, CIFAR-10 Batch 1:  loss and accuracy: 1.9499, 0.3606\n",
      "Epoch 892, CIFAR-10 Batch 1:  loss and accuracy: 1.9683, 0.3504\n",
      "Epoch 893, CIFAR-10 Batch 1:  loss and accuracy: 1.9596, 0.3532\n",
      "Epoch 894, CIFAR-10 Batch 1:  loss and accuracy: 1.9365, 0.3806\n",
      "Epoch 895, CIFAR-10 Batch 1:  loss and accuracy: 1.9549, 0.3660\n",
      "Epoch 896, CIFAR-10 Batch 1:  loss and accuracy: 1.9203, 0.3926\n",
      "Epoch 897, CIFAR-10 Batch 1:  loss and accuracy: 1.9626, 0.3536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 898, CIFAR-10 Batch 1:  loss and accuracy: 1.9504, 0.3700\n",
      "Epoch 899, CIFAR-10 Batch 1:  loss and accuracy: 1.9399, 0.3628\n",
      "Epoch 900, CIFAR-10 Batch 1:  loss and accuracy: 1.9303, 0.3820\n",
      "Epoch 901, CIFAR-10 Batch 1:  loss and accuracy: 1.9315, 0.3782\n",
      "Epoch 902, CIFAR-10 Batch 1:  loss and accuracy: 1.9446, 0.3684\n",
      "Epoch 903, CIFAR-10 Batch 1:  loss and accuracy: 1.9184, 0.3976\n",
      "Epoch 904, CIFAR-10 Batch 1:  loss and accuracy: 1.9303, 0.3858\n",
      "Epoch 905, CIFAR-10 Batch 1:  loss and accuracy: 1.9458, 0.3798\n",
      "Epoch 906, CIFAR-10 Batch 1:  loss and accuracy: 1.9300, 0.3944\n",
      "Epoch 907, CIFAR-10 Batch 1:  loss and accuracy: 1.9312, 0.3904\n",
      "Epoch 908, CIFAR-10 Batch 1:  loss and accuracy: 1.9533, 0.3674\n",
      "Epoch 909, CIFAR-10 Batch 1:  loss and accuracy: 1.9220, 0.4020\n",
      "Epoch 910, CIFAR-10 Batch 1:  loss and accuracy: 1.9407, 0.3830\n",
      "Epoch 911, CIFAR-10 Batch 1:  loss and accuracy: 1.9249, 0.3876\n",
      "Epoch 912, CIFAR-10 Batch 1:  loss and accuracy: 1.9383, 0.3796\n",
      "Epoch 913, CIFAR-10 Batch 1:  loss and accuracy: 1.9568, 0.3588\n",
      "Epoch 914, CIFAR-10 Batch 1:  loss and accuracy: 1.9290, 0.3908\n",
      "Epoch 915, CIFAR-10 Batch 1:  loss and accuracy: 1.9488, 0.3776\n",
      "Epoch 916, CIFAR-10 Batch 1:  loss and accuracy: 1.9289, 0.3958\n",
      "Epoch 917, CIFAR-10 Batch 1:  loss and accuracy: 1.9279, 0.3880\n",
      "Epoch 918, CIFAR-10 Batch 1:  loss and accuracy: 1.9170, 0.4020\n",
      "Epoch 919, CIFAR-10 Batch 1:  loss and accuracy: 1.9472, 0.3718\n",
      "Epoch 920, CIFAR-10 Batch 1:  loss and accuracy: 1.9466, 0.3804\n",
      "Epoch 921, CIFAR-10 Batch 1:  loss and accuracy: 1.9381, 0.3858\n",
      "Epoch 922, CIFAR-10 Batch 1:  loss and accuracy: 1.9428, 0.3754\n",
      "Epoch 923, CIFAR-10 Batch 1:  loss and accuracy: 1.9405, 0.3818\n",
      "Epoch 924, CIFAR-10 Batch 1:  loss and accuracy: 1.9681, 0.3666\n",
      "Epoch 925, CIFAR-10 Batch 1:  loss and accuracy: 1.9396, 0.3872\n",
      "Epoch 926, CIFAR-10 Batch 1:  loss and accuracy: 1.9279, 0.3902\n",
      "Epoch 927, CIFAR-10 Batch 1:  loss and accuracy: 1.9344, 0.3908\n",
      "Epoch 928, CIFAR-10 Batch 1:  loss and accuracy: 1.9529, 0.3748\n",
      "Epoch 929, CIFAR-10 Batch 1:  loss and accuracy: 1.9555, 0.3678\n",
      "Epoch 930, CIFAR-10 Batch 1:  loss and accuracy: 1.9183, 0.3958\n",
      "Epoch 931, CIFAR-10 Batch 1:  loss and accuracy: 1.9407, 0.3684\n",
      "Epoch 932, CIFAR-10 Batch 1:  loss and accuracy: 1.9410, 0.3694\n",
      "Epoch 933, CIFAR-10 Batch 1:  loss and accuracy: 1.9484, 0.3582\n",
      "Epoch 934, CIFAR-10 Batch 1:  loss and accuracy: 1.9181, 0.3812\n",
      "Epoch 935, CIFAR-10 Batch 1:  loss and accuracy: 1.9580, 0.3628\n",
      "Epoch 936, CIFAR-10 Batch 1:  loss and accuracy: 1.9301, 0.3774\n",
      "Epoch 937, CIFAR-10 Batch 1:  loss and accuracy: 1.9280, 0.3940\n",
      "Epoch 938, CIFAR-10 Batch 1:  loss and accuracy: 1.9507, 0.3666\n",
      "Epoch 939, CIFAR-10 Batch 1:  loss and accuracy: 1.9320, 0.3932\n",
      "Epoch 940, CIFAR-10 Batch 1:  loss and accuracy: 1.9482, 0.3724\n",
      "Epoch 941, CIFAR-10 Batch 1:  loss and accuracy: 1.9374, 0.3838\n",
      "Epoch 942, CIFAR-10 Batch 1:  loss and accuracy: 1.9499, 0.3688\n",
      "Epoch 943, CIFAR-10 Batch 1:  loss and accuracy: 1.9260, 0.3850\n",
      "Epoch 944, CIFAR-10 Batch 1:  loss and accuracy: 1.9473, 0.3678\n",
      "Epoch 945, CIFAR-10 Batch 1:  loss and accuracy: 1.9384, 0.3790\n",
      "Epoch 946, CIFAR-10 Batch 1:  loss and accuracy: 1.9192, 0.3944\n",
      "Epoch 947, CIFAR-10 Batch 1:  loss and accuracy: 1.9113, 0.3996\n",
      "Epoch 948, CIFAR-10 Batch 1:  loss and accuracy: 1.9263, 0.3778\n",
      "Epoch 949, CIFAR-10 Batch 1:  loss and accuracy: 1.9177, 0.3948\n",
      "Epoch 950, CIFAR-10 Batch 1:  loss and accuracy: 1.9518, 0.3712\n",
      "Epoch 951, CIFAR-10 Batch 1:  loss and accuracy: 1.9205, 0.3928\n",
      "Epoch 952, CIFAR-10 Batch 1:  loss and accuracy: 1.9597, 0.3540\n",
      "Epoch 953, CIFAR-10 Batch 1:  loss and accuracy: 1.9395, 0.3710\n",
      "Epoch 954, CIFAR-10 Batch 1:  loss and accuracy: 1.9434, 0.3768\n",
      "Epoch 955, CIFAR-10 Batch 1:  loss and accuracy: 1.9391, 0.3758\n",
      "Epoch 956, CIFAR-10 Batch 1:  loss and accuracy: 1.9374, 0.3762\n",
      "Epoch 957, CIFAR-10 Batch 1:  loss and accuracy: 1.9320, 0.3878\n",
      "Epoch 958, CIFAR-10 Batch 1:  loss and accuracy: 1.9401, 0.3752\n",
      "Epoch 959, CIFAR-10 Batch 1:  loss and accuracy: 1.9516, 0.3660\n",
      "Epoch 960, CIFAR-10 Batch 1:  loss and accuracy: 1.9222, 0.3920\n",
      "Epoch 961, CIFAR-10 Batch 1:  loss and accuracy: 1.9376, 0.3734\n",
      "Epoch 962, CIFAR-10 Batch 1:  loss and accuracy: 1.9496, 0.3724\n",
      "Epoch 963, CIFAR-10 Batch 1:  loss and accuracy: 1.9299, 0.3774\n",
      "Epoch 964, CIFAR-10 Batch 1:  loss and accuracy: 1.9298, 0.3834\n",
      "Epoch 965, CIFAR-10 Batch 1:  loss and accuracy: 1.9439, 0.3702\n",
      "Epoch 966, CIFAR-10 Batch 1:  loss and accuracy: 1.9517, 0.3576\n",
      "Epoch 967, CIFAR-10 Batch 1:  loss and accuracy: 1.9363, 0.3688\n",
      "Epoch 968, CIFAR-10 Batch 1:  loss and accuracy: 1.9187, 0.3836\n",
      "Epoch 969, CIFAR-10 Batch 1:  loss and accuracy: 1.9228, 0.3806\n",
      "Epoch 970, CIFAR-10 Batch 1:  loss and accuracy: 1.9276, 0.3816\n",
      "Epoch 971, CIFAR-10 Batch 1:  loss and accuracy: 1.9186, 0.3906\n",
      "Epoch 972, CIFAR-10 Batch 1:  loss and accuracy: 1.9180, 0.3928\n",
      "Epoch 973, CIFAR-10 Batch 1:  loss and accuracy: 1.9199, 0.3936\n",
      "Epoch 974, CIFAR-10 Batch 1:  loss and accuracy: 1.9308, 0.3840\n",
      "Epoch 975, CIFAR-10 Batch 1:  loss and accuracy: 1.9267, 0.3872\n",
      "Epoch 976, CIFAR-10 Batch 1:  loss and accuracy: 1.9132, 0.4006\n",
      "Epoch 977, CIFAR-10 Batch 1:  loss and accuracy: 1.9255, 0.3918\n",
      "Epoch 978, CIFAR-10 Batch 1:  loss and accuracy: 1.9226, 0.3894\n",
      "Epoch 979, CIFAR-10 Batch 1:  loss and accuracy: 1.9271, 0.3762\n",
      "Epoch 980, CIFAR-10 Batch 1:  loss and accuracy: 1.9138, 0.3962\n",
      "Epoch 981, CIFAR-10 Batch 1:  loss and accuracy: 1.9098, 0.4024\n",
      "Epoch 982, CIFAR-10 Batch 1:  loss and accuracy: 1.9286, 0.3898\n",
      "Epoch 983, CIFAR-10 Batch 1:  loss and accuracy: 1.9206, 0.4004\n",
      "Epoch 984, CIFAR-10 Batch 1:  loss and accuracy: 1.9301, 0.3850\n",
      "Epoch 985, CIFAR-10 Batch 1:  loss and accuracy: 1.9190, 0.3954\n",
      "Epoch 986, CIFAR-10 Batch 1:  loss and accuracy: 1.9267, 0.3830\n",
      "Epoch 987, CIFAR-10 Batch 1:  loss and accuracy: 1.9271, 0.3940\n",
      "Epoch 988, CIFAR-10 Batch 1:  loss and accuracy: 1.9541, 0.3608\n",
      "Epoch 989, CIFAR-10 Batch 1:  loss and accuracy: 1.9285, 0.3862\n",
      "Epoch 990, CIFAR-10 Batch 1:  loss and accuracy: 1.9426, 0.3852\n",
      "Epoch 991, CIFAR-10 Batch 1:  loss and accuracy: 1.9331, 0.3962\n",
      "Epoch 992, CIFAR-10 Batch 1:  loss and accuracy: 1.9269, 0.3952\n",
      "Epoch 993, CIFAR-10 Batch 1:  loss and accuracy: 1.9143, 0.4034\n",
      "Epoch 994, CIFAR-10 Batch 1:  loss and accuracy: 1.9150, 0.3986\n",
      "Epoch 995, CIFAR-10 Batch 1:  loss and accuracy: 1.9661, 0.3570\n",
      "Epoch 996, CIFAR-10 Batch 1:  loss and accuracy: 1.9321, 0.3878\n",
      "Epoch 997, CIFAR-10 Batch 1:  loss and accuracy: 1.9592, 0.3696\n",
      "Epoch 998, CIFAR-10 Batch 1:  loss and accuracy: 1.9777, 0.3596\n",
      "Epoch 999, CIFAR-10 Batch 1:  loss and accuracy: 1.9443, 0.3776\n",
      "Epoch 1000, CIFAR-10 Batch 1:  loss and accuracy: 1.9526, 0.3630\n",
      "Epoch 1001, CIFAR-10 Batch 1:  loss and accuracy: 1.9566, 0.3652\n",
      "Epoch 1002, CIFAR-10 Batch 1:  loss and accuracy: 1.9552, 0.3656\n",
      "Epoch 1003, CIFAR-10 Batch 1:  loss and accuracy: 1.9414, 0.3820\n",
      "Epoch 1004, CIFAR-10 Batch 1:  loss and accuracy: 1.9454, 0.3862\n",
      "Epoch 1005, CIFAR-10 Batch 1:  loss and accuracy: 1.9532, 0.3742\n",
      "Epoch 1006, CIFAR-10 Batch 1:  loss and accuracy: 1.9550, 0.3708\n",
      "Epoch 1007, CIFAR-10 Batch 1:  loss and accuracy: 1.9378, 0.3714\n",
      "Epoch 1008, CIFAR-10 Batch 1:  loss and accuracy: 1.9273, 0.3930\n",
      "Epoch 1009, CIFAR-10 Batch 1:  loss and accuracy: 1.9337, 0.3826\n",
      "Epoch 1010, CIFAR-10 Batch 1:  loss and accuracy: 1.9240, 0.3986\n",
      "Epoch 1011, CIFAR-10 Batch 1:  loss and accuracy: 1.9632, 0.3650\n",
      "Epoch 1012, CIFAR-10 Batch 1:  loss and accuracy: 1.9195, 0.3854\n",
      "Epoch 1013, CIFAR-10 Batch 1:  loss and accuracy: 1.9290, 0.3852\n",
      "Epoch 1014, CIFAR-10 Batch 1:  loss and accuracy: 1.9192, 0.3916\n",
      "Epoch 1015, CIFAR-10 Batch 1:  loss and accuracy: 1.9189, 0.3858\n",
      "Epoch 1016, CIFAR-10 Batch 1:  loss and accuracy: 1.9211, 0.3844\n",
      "Epoch 1017, CIFAR-10 Batch 1:  loss and accuracy: 1.9534, 0.3628\n",
      "Epoch 1018, CIFAR-10 Batch 1:  loss and accuracy: 1.9504, 0.3642\n",
      "Epoch 1019, CIFAR-10 Batch 1:  loss and accuracy: 1.9191, 0.3928\n",
      "Epoch 1020, CIFAR-10 Batch 1:  loss and accuracy: 1.9342, 0.3846\n",
      "Epoch 1021, CIFAR-10 Batch 1:  loss and accuracy: 1.9119, 0.4022\n",
      "Epoch 1022, CIFAR-10 Batch 1:  loss and accuracy: 1.9223, 0.3904\n",
      "Epoch 1023, CIFAR-10 Batch 1:  loss and accuracy: 1.9314, 0.3908\n",
      "Epoch 1024, CIFAR-10 Batch 1:  loss and accuracy: 1.9349, 0.3786\n",
      "Epoch 1025, CIFAR-10 Batch 1:  loss and accuracy: 1.9476, 0.3720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1026, CIFAR-10 Batch 1:  loss and accuracy: 1.9392, 0.3848\n",
      "Epoch 1027, CIFAR-10 Batch 1:  loss and accuracy: 1.9484, 0.3646\n",
      "Epoch 1028, CIFAR-10 Batch 1:  loss and accuracy: 1.9238, 0.3906\n",
      "Epoch 1029, CIFAR-10 Batch 1:  loss and accuracy: 1.9443, 0.3782\n",
      "Epoch 1030, CIFAR-10 Batch 1:  loss and accuracy: 1.9233, 0.3996\n",
      "Epoch 1031, CIFAR-10 Batch 1:  loss and accuracy: 1.9286, 0.3880\n",
      "Epoch 1032, CIFAR-10 Batch 1:  loss and accuracy: 1.9138, 0.3950\n",
      "Epoch 1033, CIFAR-10 Batch 1:  loss and accuracy: 1.9193, 0.3928\n",
      "Epoch 1034, CIFAR-10 Batch 1:  loss and accuracy: 1.9298, 0.3856\n",
      "Epoch 1035, CIFAR-10 Batch 1:  loss and accuracy: 1.9108, 0.3990\n",
      "Epoch 1036, CIFAR-10 Batch 1:  loss and accuracy: 1.9030, 0.3968\n",
      "Epoch 1037, CIFAR-10 Batch 1:  loss and accuracy: 1.9081, 0.4000\n",
      "Epoch 1038, CIFAR-10 Batch 1:  loss and accuracy: 1.9182, 0.3904\n",
      "Epoch 1039, CIFAR-10 Batch 1:  loss and accuracy: 1.9219, 0.3888\n",
      "Epoch 1040, CIFAR-10 Batch 1:  loss and accuracy: 1.9268, 0.3868\n",
      "Epoch 1041, CIFAR-10 Batch 1:  loss and accuracy: 1.9592, 0.3644\n",
      "Epoch 1042, CIFAR-10 Batch 1:  loss and accuracy: 1.9308, 0.3756\n",
      "Epoch 1043, CIFAR-10 Batch 1:  loss and accuracy: 1.9348, 0.3822\n",
      "Epoch 1044, CIFAR-10 Batch 1:  loss and accuracy: 1.9308, 0.3828\n",
      "Epoch 1045, CIFAR-10 Batch 1:  loss and accuracy: 1.9503, 0.3692\n",
      "Epoch 1046, CIFAR-10 Batch 1:  loss and accuracy: 1.9199, 0.3874\n",
      "Epoch 1047, CIFAR-10 Batch 1:  loss and accuracy: 1.9204, 0.3842\n",
      "Epoch 1048, CIFAR-10 Batch 1:  loss and accuracy: 1.9281, 0.3860\n",
      "Epoch 1049, CIFAR-10 Batch 1:  loss and accuracy: 1.9317, 0.3896\n",
      "Epoch 1050, CIFAR-10 Batch 1:  loss and accuracy: 1.9073, 0.3978\n",
      "Epoch 1051, CIFAR-10 Batch 1:  loss and accuracy: 1.9378, 0.3884\n",
      "Epoch 1052, CIFAR-10 Batch 1:  loss and accuracy: 1.9050, 0.4008\n",
      "Epoch 1053, CIFAR-10 Batch 1:  loss and accuracy: 1.9059, 0.4000\n",
      "Epoch 1054, CIFAR-10 Batch 1:  loss and accuracy: 1.9099, 0.3938\n",
      "Epoch 1055, CIFAR-10 Batch 1:  loss and accuracy: 1.9376, 0.3718\n",
      "Epoch 1056, CIFAR-10 Batch 1:  loss and accuracy: 1.9539, 0.3620\n",
      "Epoch 1057, CIFAR-10 Batch 1:  loss and accuracy: 1.9151, 0.3956\n",
      "Epoch 1058, CIFAR-10 Batch 1:  loss and accuracy: 1.9286, 0.3876\n",
      "Epoch 1059, CIFAR-10 Batch 1:  loss and accuracy: 1.9198, 0.3812\n",
      "Epoch 1060, CIFAR-10 Batch 1:  loss and accuracy: 1.9479, 0.3654\n",
      "Epoch 1061, CIFAR-10 Batch 1:  loss and accuracy: 1.9465, 0.3662\n",
      "Epoch 1062, CIFAR-10 Batch 1:  loss and accuracy: 1.9219, 0.3942\n",
      "Epoch 1063, CIFAR-10 Batch 1:  loss and accuracy: 1.9109, 0.4064\n",
      "Epoch 1064, CIFAR-10 Batch 1:  loss and accuracy: 1.9083, 0.4080\n",
      "Epoch 1065, CIFAR-10 Batch 1:  loss and accuracy: 1.9124, 0.4050\n",
      "Epoch 1066, CIFAR-10 Batch 1:  loss and accuracy: 1.9206, 0.4084\n",
      "Epoch 1067, CIFAR-10 Batch 1:  loss and accuracy: 1.9312, 0.3950\n",
      "Epoch 1068, CIFAR-10 Batch 1:  loss and accuracy: 1.9158, 0.4086\n",
      "Epoch 1069, CIFAR-10 Batch 1:  loss and accuracy: 1.9088, 0.4146\n",
      "Epoch 1070, CIFAR-10 Batch 1:  loss and accuracy: 1.9252, 0.3914\n",
      "Epoch 1071, CIFAR-10 Batch 1:  loss and accuracy: 1.9221, 0.4112\n",
      "Epoch 1072, CIFAR-10 Batch 1:  loss and accuracy: 1.9465, 0.3830\n",
      "Epoch 1073, CIFAR-10 Batch 1:  loss and accuracy: 1.9050, 0.4106\n",
      "Epoch 1074, CIFAR-10 Batch 1:  loss and accuracy: 1.9252, 0.3818\n",
      "Epoch 1075, CIFAR-10 Batch 1:  loss and accuracy: 1.9230, 0.3976\n",
      "Epoch 1076, CIFAR-10 Batch 1:  loss and accuracy: 1.9113, 0.4098\n",
      "Epoch 1077, CIFAR-10 Batch 1:  loss and accuracy: 1.9204, 0.3960\n",
      "Epoch 1078, CIFAR-10 Batch 1:  loss and accuracy: 1.9270, 0.4018\n",
      "Epoch 1079, CIFAR-10 Batch 1:  loss and accuracy: 1.9136, 0.4058\n",
      "Epoch 1080, CIFAR-10 Batch 1:  loss and accuracy: 1.9254, 0.3786\n",
      "Epoch 1081, CIFAR-10 Batch 1:  loss and accuracy: 1.9333, 0.3876\n",
      "Epoch 1082, CIFAR-10 Batch 1:  loss and accuracy: 1.9291, 0.3920\n",
      "Epoch 1083, CIFAR-10 Batch 1:  loss and accuracy: 1.9242, 0.3890\n",
      "Epoch 1084, CIFAR-10 Batch 1:  loss and accuracy: 1.9080, 0.4010\n",
      "Epoch 1085, CIFAR-10 Batch 1:  loss and accuracy: 1.9036, 0.4006\n",
      "Epoch 1086, CIFAR-10 Batch 1:  loss and accuracy: 1.9120, 0.4002\n",
      "Epoch 1087, CIFAR-10 Batch 1:  loss and accuracy: 1.9135, 0.3952\n",
      "Epoch 1088, CIFAR-10 Batch 1:  loss and accuracy: 1.9163, 0.3954\n",
      "Epoch 1089, CIFAR-10 Batch 1:  loss and accuracy: 1.9260, 0.3874\n",
      "Epoch 1090, CIFAR-10 Batch 1:  loss and accuracy: 1.9081, 0.4036\n",
      "Epoch 1091, CIFAR-10 Batch 1:  loss and accuracy: 1.9193, 0.4012\n",
      "Epoch 1092, CIFAR-10 Batch 1:  loss and accuracy: 1.9104, 0.4118\n",
      "Epoch 1093, CIFAR-10 Batch 1:  loss and accuracy: 1.9131, 0.4120\n",
      "Epoch 1094, CIFAR-10 Batch 1:  loss and accuracy: 1.9231, 0.3978\n",
      "Epoch 1095, CIFAR-10 Batch 1:  loss and accuracy: 1.9145, 0.4038\n",
      "Epoch 1096, CIFAR-10 Batch 1:  loss and accuracy: 1.9373, 0.3858\n",
      "Epoch 1097, CIFAR-10 Batch 1:  loss and accuracy: 1.9141, 0.4028\n",
      "Epoch 1098, CIFAR-10 Batch 1:  loss and accuracy: 1.9340, 0.4052\n",
      "Epoch 1099, CIFAR-10 Batch 1:  loss and accuracy: 1.9144, 0.4060\n",
      "Epoch 1100, CIFAR-10 Batch 1:  loss and accuracy: 1.9275, 0.4030\n",
      "Epoch 1101, CIFAR-10 Batch 1:  loss and accuracy: 1.9356, 0.3898\n",
      "Epoch 1102, CIFAR-10 Batch 1:  loss and accuracy: 1.9187, 0.3982\n",
      "Epoch 1103, CIFAR-10 Batch 1:  loss and accuracy: 1.9232, 0.4002\n",
      "Epoch 1104, CIFAR-10 Batch 1:  loss and accuracy: 1.9360, 0.3822\n",
      "Epoch 1105, CIFAR-10 Batch 1:  loss and accuracy: 1.9045, 0.4130\n",
      "Epoch 1106, CIFAR-10 Batch 1:  loss and accuracy: 1.9246, 0.3976\n",
      "Epoch 1107, CIFAR-10 Batch 1:  loss and accuracy: 1.8980, 0.4222\n",
      "Epoch 1108, CIFAR-10 Batch 1:  loss and accuracy: 1.9124, 0.4102\n",
      "Epoch 1109, CIFAR-10 Batch 1:  loss and accuracy: 1.8971, 0.4298\n",
      "Epoch 1110, CIFAR-10 Batch 1:  loss and accuracy: 1.9243, 0.3946\n",
      "Epoch 1111, CIFAR-10 Batch 1:  loss and accuracy: 1.9224, 0.3998\n",
      "Epoch 1112, CIFAR-10 Batch 1:  loss and accuracy: 1.9253, 0.3936\n",
      "Epoch 1113, CIFAR-10 Batch 1:  loss and accuracy: 1.9092, 0.3916\n",
      "Epoch 1114, CIFAR-10 Batch 1:  loss and accuracy: 1.9051, 0.4008\n",
      "Epoch 1115, CIFAR-10 Batch 1:  loss and accuracy: 1.9043, 0.4016\n",
      "Epoch 1116, CIFAR-10 Batch 1:  loss and accuracy: 1.9384, 0.3764\n",
      "Epoch 1117, CIFAR-10 Batch 1:  loss and accuracy: 1.9088, 0.4082\n",
      "Epoch 1118, CIFAR-10 Batch 1:  loss and accuracy: 1.9208, 0.3914\n",
      "Epoch 1119, CIFAR-10 Batch 1:  loss and accuracy: 1.9098, 0.4056\n",
      "Epoch 1120, CIFAR-10 Batch 1:  loss and accuracy: 1.9276, 0.4000\n",
      "Epoch 1121, CIFAR-10 Batch 1:  loss and accuracy: 1.9034, 0.4132\n",
      "Epoch 1122, CIFAR-10 Batch 1:  loss and accuracy: 1.9039, 0.4142\n",
      "Epoch 1123, CIFAR-10 Batch 1:  loss and accuracy: 1.9162, 0.4000\n",
      "Epoch 1124, CIFAR-10 Batch 1:  loss and accuracy: 1.9149, 0.3980\n",
      "Epoch 1125, CIFAR-10 Batch 1:  loss and accuracy: 1.9189, 0.4012\n",
      "Epoch 1126, CIFAR-10 Batch 1:  loss and accuracy: 1.9466, 0.3730\n",
      "Epoch 1127, CIFAR-10 Batch 1:  loss and accuracy: 1.9185, 0.4026\n",
      "Epoch 1128, CIFAR-10 Batch 1:  loss and accuracy: 1.9205, 0.4068\n",
      "Epoch 1129, CIFAR-10 Batch 1:  loss and accuracy: 1.9154, 0.4054\n",
      "Epoch 1130, CIFAR-10 Batch 1:  loss and accuracy: 1.9211, 0.4002\n",
      "Epoch 1131, CIFAR-10 Batch 1:  loss and accuracy: 1.9330, 0.3970\n",
      "Epoch 1132, CIFAR-10 Batch 1:  loss and accuracy: 1.9236, 0.4026\n",
      "Epoch 1133, CIFAR-10 Batch 1:  loss and accuracy: 1.9011, 0.4218\n",
      "Epoch 1134, CIFAR-10 Batch 1:  loss and accuracy: 1.9037, 0.4206\n",
      "Epoch 1135, CIFAR-10 Batch 1:  loss and accuracy: 1.9109, 0.4144\n",
      "Epoch 1136, CIFAR-10 Batch 1:  loss and accuracy: 1.9066, 0.4100\n",
      "Epoch 1137, CIFAR-10 Batch 1:  loss and accuracy: 1.9158, 0.4082\n",
      "Epoch 1138, CIFAR-10 Batch 1:  loss and accuracy: 1.9101, 0.3992\n",
      "Epoch 1139, CIFAR-10 Batch 1:  loss and accuracy: 1.9394, 0.3764\n",
      "Epoch 1140, CIFAR-10 Batch 1:  loss and accuracy: 1.9100, 0.4036\n",
      "Epoch 1141, CIFAR-10 Batch 1:  loss and accuracy: 1.9003, 0.4036\n",
      "Epoch 1142, CIFAR-10 Batch 1:  loss and accuracy: 1.9147, 0.3984\n",
      "Epoch 1143, CIFAR-10 Batch 1:  loss and accuracy: 1.9102, 0.4000\n",
      "Epoch 1144, CIFAR-10 Batch 1:  loss and accuracy: 1.9094, 0.3960\n",
      "Epoch 1145, CIFAR-10 Batch 1:  loss and accuracy: 1.9145, 0.3996\n",
      "Epoch 1146, CIFAR-10 Batch 1:  loss and accuracy: 1.9087, 0.4102\n",
      "Epoch 1147, CIFAR-10 Batch 1:  loss and accuracy: 1.9009, 0.4074\n",
      "Epoch 1148, CIFAR-10 Batch 1:  loss and accuracy: 1.9076, 0.4158\n",
      "Epoch 1149, CIFAR-10 Batch 1:  loss and accuracy: 1.9120, 0.4034\n",
      "Epoch 1150, CIFAR-10 Batch 1:  loss and accuracy: 1.9260, 0.3936\n",
      "Epoch 1151, CIFAR-10 Batch 1:  loss and accuracy: 1.9269, 0.3886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1152, CIFAR-10 Batch 1:  loss and accuracy: 1.9086, 0.4072\n",
      "Epoch 1153, CIFAR-10 Batch 1:  loss and accuracy: 1.9075, 0.4008\n",
      "Epoch 1154, CIFAR-10 Batch 1:  loss and accuracy: 1.9139, 0.4028\n",
      "Epoch 1155, CIFAR-10 Batch 1:  loss and accuracy: 1.8959, 0.4148\n",
      "Epoch 1156, CIFAR-10 Batch 1:  loss and accuracy: 1.9025, 0.4034\n",
      "Epoch 1157, CIFAR-10 Batch 1:  loss and accuracy: 1.8963, 0.4138\n",
      "Epoch 1158, CIFAR-10 Batch 1:  loss and accuracy: 1.9159, 0.3992\n",
      "Epoch 1159, CIFAR-10 Batch 1:  loss and accuracy: 1.9139, 0.3960\n",
      "Epoch 1160, CIFAR-10 Batch 1:  loss and accuracy: 1.9116, 0.4052\n",
      "Epoch 1161, CIFAR-10 Batch 1:  loss and accuracy: 1.9190, 0.3958\n",
      "Epoch 1162, CIFAR-10 Batch 1:  loss and accuracy: 1.9134, 0.4070\n",
      "Epoch 1163, CIFAR-10 Batch 1:  loss and accuracy: 1.9173, 0.3970\n",
      "Epoch 1164, CIFAR-10 Batch 1:  loss and accuracy: 1.9058, 0.4102\n",
      "Epoch 1165, CIFAR-10 Batch 1:  loss and accuracy: 1.8920, 0.4260\n",
      "Epoch 1166, CIFAR-10 Batch 1:  loss and accuracy: 1.8929, 0.4252\n",
      "Epoch 1167, CIFAR-10 Batch 1:  loss and accuracy: 1.9018, 0.4118\n",
      "Epoch 1168, CIFAR-10 Batch 1:  loss and accuracy: 1.9162, 0.4110\n",
      "Epoch 1169, CIFAR-10 Batch 1:  loss and accuracy: 1.9022, 0.4116\n",
      "Epoch 1170, CIFAR-10 Batch 1:  loss and accuracy: 1.9044, 0.4066\n",
      "Epoch 1171, CIFAR-10 Batch 1:  loss and accuracy: 1.9142, 0.3994\n",
      "Epoch 1172, CIFAR-10 Batch 1:  loss and accuracy: 1.8990, 0.4134\n",
      "Epoch 1173, CIFAR-10 Batch 1:  loss and accuracy: 1.9005, 0.4202\n",
      "Epoch 1174, CIFAR-10 Batch 1:  loss and accuracy: 1.8931, 0.4266\n",
      "Epoch 1175, CIFAR-10 Batch 1:  loss and accuracy: 1.9090, 0.4118\n",
      "Epoch 1176, CIFAR-10 Batch 1:  loss and accuracy: 1.9247, 0.3958\n",
      "Epoch 1177, CIFAR-10 Batch 1:  loss and accuracy: 1.9359, 0.3812\n",
      "Epoch 1178, CIFAR-10 Batch 1:  loss and accuracy: 1.9252, 0.3960\n",
      "Epoch 1179, CIFAR-10 Batch 1:  loss and accuracy: 1.9101, 0.4060\n",
      "Epoch 1180, CIFAR-10 Batch 1:  loss and accuracy: 1.9238, 0.3956\n",
      "Epoch 1181, CIFAR-10 Batch 1:  loss and accuracy: 1.9334, 0.3818\n",
      "Epoch 1182, CIFAR-10 Batch 1:  loss and accuracy: 1.9178, 0.4042\n",
      "Epoch 1183, CIFAR-10 Batch 1:  loss and accuracy: 1.9238, 0.3986\n",
      "Epoch 1184, CIFAR-10 Batch 1:  loss and accuracy: 1.9255, 0.4002\n",
      "Epoch 1185, CIFAR-10 Batch 1:  loss and accuracy: 1.9227, 0.3998\n",
      "Epoch 1186, CIFAR-10 Batch 1:  loss and accuracy: 1.9252, 0.4048\n",
      "Epoch 1187, CIFAR-10 Batch 1:  loss and accuracy: 1.9183, 0.4078\n",
      "Epoch 1188, CIFAR-10 Batch 1:  loss and accuracy: 1.9241, 0.4016\n",
      "Epoch 1189, CIFAR-10 Batch 1:  loss and accuracy: 1.9224, 0.3986\n",
      "Epoch 1190, CIFAR-10 Batch 1:  loss and accuracy: 1.9229, 0.3938\n",
      "Epoch 1191, CIFAR-10 Batch 1:  loss and accuracy: 1.9362, 0.3950\n",
      "Epoch 1192, CIFAR-10 Batch 1:  loss and accuracy: 1.9195, 0.4110\n",
      "Epoch 1193, CIFAR-10 Batch 1:  loss and accuracy: 1.9407, 0.3914\n",
      "Epoch 1194, CIFAR-10 Batch 1:  loss and accuracy: 1.9386, 0.3868\n",
      "Epoch 1195, CIFAR-10 Batch 1:  loss and accuracy: 1.9020, 0.4208\n",
      "Epoch 1196, CIFAR-10 Batch 1:  loss and accuracy: 1.9085, 0.4066\n",
      "Epoch 1197, CIFAR-10 Batch 1:  loss and accuracy: 1.9171, 0.4088\n",
      "Epoch 1198, CIFAR-10 Batch 1:  loss and accuracy: 1.8838, 0.4302\n",
      "Epoch 1199, CIFAR-10 Batch 1:  loss and accuracy: 1.9060, 0.4226\n",
      "Epoch 1200, CIFAR-10 Batch 1:  loss and accuracy: 1.9073, 0.4118\n",
      "Epoch 1201, CIFAR-10 Batch 1:  loss and accuracy: 1.9134, 0.4122\n",
      "Epoch 1202, CIFAR-10 Batch 1:  loss and accuracy: 1.8996, 0.4218\n",
      "Epoch 1203, CIFAR-10 Batch 1:  loss and accuracy: 1.9321, 0.3978\n",
      "Epoch 1204, CIFAR-10 Batch 1:  loss and accuracy: 1.8905, 0.4206\n",
      "Epoch 1205, CIFAR-10 Batch 1:  loss and accuracy: 1.9075, 0.4092\n",
      "Epoch 1206, CIFAR-10 Batch 1:  loss and accuracy: 1.9104, 0.4068\n",
      "Epoch 1207, CIFAR-10 Batch 1:  loss and accuracy: 1.9154, 0.4000\n",
      "Epoch 1208, CIFAR-10 Batch 1:  loss and accuracy: 1.9118, 0.4058\n",
      "Epoch 1209, CIFAR-10 Batch 1:  loss and accuracy: 1.9103, 0.4092\n",
      "Epoch 1210, CIFAR-10 Batch 1:  loss and accuracy: 1.9312, 0.4020\n",
      "Epoch 1211, CIFAR-10 Batch 1:  loss and accuracy: 1.9058, 0.4136\n",
      "Epoch 1212, CIFAR-10 Batch 1:  loss and accuracy: 1.9123, 0.4108\n",
      "Epoch 1213, CIFAR-10 Batch 1:  loss and accuracy: 1.9191, 0.4058\n",
      "Epoch 1214, CIFAR-10 Batch 1:  loss and accuracy: 1.9184, 0.4024\n",
      "Epoch 1215, CIFAR-10 Batch 1:  loss and accuracy: 1.9370, 0.3926\n",
      "Epoch 1216, CIFAR-10 Batch 1:  loss and accuracy: 1.9140, 0.4100\n",
      "Epoch 1217, CIFAR-10 Batch 1:  loss and accuracy: 1.9099, 0.4136\n",
      "Epoch 1218, CIFAR-10 Batch 1:  loss and accuracy: 1.9052, 0.4218\n",
      "Epoch 1219, CIFAR-10 Batch 1:  loss and accuracy: 1.9207, 0.4150\n",
      "Epoch 1220, CIFAR-10 Batch 1:  loss and accuracy: 1.8963, 0.4256\n",
      "Epoch 1221, CIFAR-10 Batch 1:  loss and accuracy: 1.8972, 0.4216\n",
      "Epoch 1222, CIFAR-10 Batch 1:  loss and accuracy: 1.9351, 0.3912\n",
      "Epoch 1223, CIFAR-10 Batch 1:  loss and accuracy: 1.9254, 0.4002\n",
      "Epoch 1224, CIFAR-10 Batch 1:  loss and accuracy: 1.9308, 0.3908\n",
      "Epoch 1225, CIFAR-10 Batch 1:  loss and accuracy: 1.9413, 0.3818\n",
      "Epoch 1226, CIFAR-10 Batch 1:  loss and accuracy: 1.9009, 0.4238\n",
      "Epoch 1227, CIFAR-10 Batch 1:  loss and accuracy: 1.8946, 0.4318\n",
      "Epoch 1228, CIFAR-10 Batch 1:  loss and accuracy: 1.9005, 0.4240\n",
      "Epoch 1229, CIFAR-10 Batch 1:  loss and accuracy: 1.9040, 0.4274\n",
      "Epoch 1230, CIFAR-10 Batch 1:  loss and accuracy: 1.8986, 0.4300\n",
      "Epoch 1231, CIFAR-10 Batch 1:  loss and accuracy: 1.9057, 0.4202\n",
      "Epoch 1232, CIFAR-10 Batch 1:  loss and accuracy: 1.8991, 0.4232\n",
      "Epoch 1233, CIFAR-10 Batch 1:  loss and accuracy: 1.9217, 0.4096\n",
      "Epoch 1234, CIFAR-10 Batch 1:  loss and accuracy: 1.9120, 0.4156\n",
      "Epoch 1235, CIFAR-10 Batch 1:  loss and accuracy: 1.9066, 0.4148\n",
      "Epoch 1236, CIFAR-10 Batch 1:  loss and accuracy: 1.9024, 0.4036\n",
      "Epoch 1237, CIFAR-10 Batch 1:  loss and accuracy: 1.9098, 0.4126\n",
      "Epoch 1238, CIFAR-10 Batch 1:  loss and accuracy: 1.9054, 0.4150\n",
      "Epoch 1239, CIFAR-10 Batch 1:  loss and accuracy: 1.9113, 0.4088\n",
      "Epoch 1240, CIFAR-10 Batch 1:  loss and accuracy: 1.9133, 0.4106\n",
      "Epoch 1241, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4328\n",
      "Epoch 1242, CIFAR-10 Batch 1:  loss and accuracy: 1.8898, 0.4206\n",
      "Epoch 1243, CIFAR-10 Batch 1:  loss and accuracy: 1.9139, 0.4050\n",
      "Epoch 1244, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4086\n",
      "Epoch 1245, CIFAR-10 Batch 1:  loss and accuracy: 1.9271, 0.3880\n",
      "Epoch 1246, CIFAR-10 Batch 1:  loss and accuracy: 1.9021, 0.4126\n",
      "Epoch 1247, CIFAR-10 Batch 1:  loss and accuracy: 1.9055, 0.4092\n",
      "Epoch 1248, CIFAR-10 Batch 1:  loss and accuracy: 1.8824, 0.4162\n",
      "Epoch 1249, CIFAR-10 Batch 1:  loss and accuracy: 1.8838, 0.4132\n",
      "Epoch 1250, CIFAR-10 Batch 1:  loss and accuracy: 1.9003, 0.4116\n",
      "Epoch 1251, CIFAR-10 Batch 1:  loss and accuracy: 1.8947, 0.4180\n",
      "Epoch 1252, CIFAR-10 Batch 1:  loss and accuracy: 1.8923, 0.4126\n",
      "Epoch 1253, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4182\n",
      "Epoch 1254, CIFAR-10 Batch 1:  loss and accuracy: 1.9030, 0.4018\n",
      "Epoch 1255, CIFAR-10 Batch 1:  loss and accuracy: 1.8932, 0.4274\n",
      "Epoch 1256, CIFAR-10 Batch 1:  loss and accuracy: 1.9071, 0.4140\n",
      "Epoch 1257, CIFAR-10 Batch 1:  loss and accuracy: 1.9047, 0.4158\n",
      "Epoch 1258, CIFAR-10 Batch 1:  loss and accuracy: 1.9086, 0.4040\n",
      "Epoch 1259, CIFAR-10 Batch 1:  loss and accuracy: 1.8950, 0.4102\n",
      "Epoch 1260, CIFAR-10 Batch 1:  loss and accuracy: 1.8890, 0.4256\n",
      "Epoch 1261, CIFAR-10 Batch 1:  loss and accuracy: 1.8992, 0.4214\n",
      "Epoch 1262, CIFAR-10 Batch 1:  loss and accuracy: 1.8892, 0.4230\n",
      "Epoch 1263, CIFAR-10 Batch 1:  loss and accuracy: 1.9050, 0.4100\n",
      "Epoch 1264, CIFAR-10 Batch 1:  loss and accuracy: 1.9033, 0.4096\n",
      "Epoch 1265, CIFAR-10 Batch 1:  loss and accuracy: 1.8872, 0.4194\n",
      "Epoch 1266, CIFAR-10 Batch 1:  loss and accuracy: 1.9001, 0.4102\n",
      "Epoch 1267, CIFAR-10 Batch 1:  loss and accuracy: 1.9046, 0.4146\n",
      "Epoch 1268, CIFAR-10 Batch 1:  loss and accuracy: 1.9019, 0.4170\n",
      "Epoch 1269, CIFAR-10 Batch 1:  loss and accuracy: 1.8845, 0.4294\n",
      "Epoch 1270, CIFAR-10 Batch 1:  loss and accuracy: 1.8891, 0.4244\n",
      "Epoch 1271, CIFAR-10 Batch 1:  loss and accuracy: 1.8911, 0.4252\n",
      "Epoch 1272, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4066\n",
      "Epoch 1273, CIFAR-10 Batch 1:  loss and accuracy: 1.8881, 0.4220\n",
      "Epoch 1274, CIFAR-10 Batch 1:  loss and accuracy: 1.8871, 0.4204\n",
      "Epoch 1275, CIFAR-10 Batch 1:  loss and accuracy: 1.8916, 0.4072\n",
      "Epoch 1276, CIFAR-10 Batch 1:  loss and accuracy: 1.8927, 0.4150\n",
      "Epoch 1277, CIFAR-10 Batch 1:  loss and accuracy: 1.8924, 0.4126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1278, CIFAR-10 Batch 1:  loss and accuracy: 1.8865, 0.4222\n",
      "Epoch 1279, CIFAR-10 Batch 1:  loss and accuracy: 1.8927, 0.4100\n",
      "Epoch 1280, CIFAR-10 Batch 1:  loss and accuracy: 1.8898, 0.4200\n",
      "Epoch 1281, CIFAR-10 Batch 1:  loss and accuracy: 1.8987, 0.4192\n",
      "Epoch 1282, CIFAR-10 Batch 1:  loss and accuracy: 1.9081, 0.4078\n",
      "Epoch 1283, CIFAR-10 Batch 1:  loss and accuracy: 1.9079, 0.4132\n",
      "Epoch 1284, CIFAR-10 Batch 1:  loss and accuracy: 1.9007, 0.4168\n",
      "Epoch 1285, CIFAR-10 Batch 1:  loss and accuracy: 1.8946, 0.4178\n",
      "Epoch 1286, CIFAR-10 Batch 1:  loss and accuracy: 1.8920, 0.4242\n",
      "Epoch 1287, CIFAR-10 Batch 1:  loss and accuracy: 1.8934, 0.4214\n",
      "Epoch 1288, CIFAR-10 Batch 1:  loss and accuracy: 1.8989, 0.4168\n",
      "Epoch 1289, CIFAR-10 Batch 1:  loss and accuracy: 1.9048, 0.4182\n",
      "Epoch 1290, CIFAR-10 Batch 1:  loss and accuracy: 1.9153, 0.4174\n",
      "Epoch 1291, CIFAR-10 Batch 1:  loss and accuracy: 1.9110, 0.4150\n",
      "Epoch 1292, CIFAR-10 Batch 1:  loss and accuracy: 1.9104, 0.4048\n",
      "Epoch 1293, CIFAR-10 Batch 1:  loss and accuracy: 1.8973, 0.4196\n",
      "Epoch 1294, CIFAR-10 Batch 1:  loss and accuracy: 1.9025, 0.4236\n",
      "Epoch 1295, CIFAR-10 Batch 1:  loss and accuracy: 1.8929, 0.4284\n",
      "Epoch 1296, CIFAR-10 Batch 1:  loss and accuracy: 1.9067, 0.4282\n",
      "Epoch 1297, CIFAR-10 Batch 1:  loss and accuracy: 1.9093, 0.4240\n",
      "Epoch 1298, CIFAR-10 Batch 1:  loss and accuracy: 1.9024, 0.4204\n",
      "Epoch 1299, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4300\n",
      "Epoch 1300, CIFAR-10 Batch 1:  loss and accuracy: 1.8993, 0.4208\n",
      "Epoch 1301, CIFAR-10 Batch 1:  loss and accuracy: 1.9153, 0.4140\n",
      "Epoch 1302, CIFAR-10 Batch 1:  loss and accuracy: 1.8996, 0.4080\n",
      "Epoch 1303, CIFAR-10 Batch 1:  loss and accuracy: 1.9023, 0.4140\n",
      "Epoch 1304, CIFAR-10 Batch 1:  loss and accuracy: 1.9060, 0.4134\n",
      "Epoch 1305, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4178\n",
      "Epoch 1306, CIFAR-10 Batch 1:  loss and accuracy: 1.8938, 0.4240\n",
      "Epoch 1307, CIFAR-10 Batch 1:  loss and accuracy: 1.8940, 0.4202\n",
      "Epoch 1308, CIFAR-10 Batch 1:  loss and accuracy: 1.9127, 0.4100\n",
      "Epoch 1309, CIFAR-10 Batch 1:  loss and accuracy: 1.8808, 0.4186\n",
      "Epoch 1310, CIFAR-10 Batch 1:  loss and accuracy: 1.8945, 0.4188\n",
      "Epoch 1311, CIFAR-10 Batch 1:  loss and accuracy: 1.8816, 0.4260\n",
      "Epoch 1312, CIFAR-10 Batch 1:  loss and accuracy: 1.8888, 0.4252\n",
      "Epoch 1313, CIFAR-10 Batch 1:  loss and accuracy: 1.8877, 0.4258\n",
      "Epoch 1314, CIFAR-10 Batch 1:  loss and accuracy: 1.8904, 0.4250\n",
      "Epoch 1315, CIFAR-10 Batch 1:  loss and accuracy: 1.8960, 0.4168\n",
      "Epoch 1316, CIFAR-10 Batch 1:  loss and accuracy: 1.9179, 0.4044\n",
      "Epoch 1317, CIFAR-10 Batch 1:  loss and accuracy: 1.8823, 0.4288\n",
      "Epoch 1318, CIFAR-10 Batch 1:  loss and accuracy: 1.8778, 0.4270\n",
      "Epoch 1319, CIFAR-10 Batch 1:  loss and accuracy: 1.8844, 0.4318\n",
      "Epoch 1320, CIFAR-10 Batch 1:  loss and accuracy: 1.8833, 0.4334\n",
      "Epoch 1321, CIFAR-10 Batch 1:  loss and accuracy: 1.8814, 0.4290\n",
      "Epoch 1322, CIFAR-10 Batch 1:  loss and accuracy: 1.9039, 0.4088\n",
      "Epoch 1323, CIFAR-10 Batch 1:  loss and accuracy: 1.8856, 0.4252\n",
      "Epoch 1324, CIFAR-10 Batch 1:  loss and accuracy: 1.8906, 0.4256\n",
      "Epoch 1325, CIFAR-10 Batch 1:  loss and accuracy: 1.8865, 0.4216\n",
      "Epoch 1326, CIFAR-10 Batch 1:  loss and accuracy: 1.8937, 0.4260\n",
      "Epoch 1327, CIFAR-10 Batch 1:  loss and accuracy: 1.9188, 0.4014\n",
      "Epoch 1328, CIFAR-10 Batch 1:  loss and accuracy: 1.9041, 0.4146\n",
      "Epoch 1329, CIFAR-10 Batch 1:  loss and accuracy: 1.8838, 0.4368\n",
      "Epoch 1330, CIFAR-10 Batch 1:  loss and accuracy: 1.8898, 0.4252\n",
      "Epoch 1331, CIFAR-10 Batch 1:  loss and accuracy: 1.9018, 0.4198\n",
      "Epoch 1332, CIFAR-10 Batch 1:  loss and accuracy: 1.8980, 0.4252\n",
      "Epoch 1333, CIFAR-10 Batch 1:  loss and accuracy: 1.8863, 0.4362\n",
      "Epoch 1334, CIFAR-10 Batch 1:  loss and accuracy: 1.9054, 0.4190\n",
      "Epoch 1335, CIFAR-10 Batch 1:  loss and accuracy: 1.8756, 0.4350\n",
      "Epoch 1336, CIFAR-10 Batch 1:  loss and accuracy: 1.8855, 0.4334\n",
      "Epoch 1337, CIFAR-10 Batch 1:  loss and accuracy: 1.8964, 0.4176\n",
      "Epoch 1338, CIFAR-10 Batch 1:  loss and accuracy: 1.8899, 0.4252\n",
      "Epoch 1339, CIFAR-10 Batch 1:  loss and accuracy: 1.8951, 0.4268\n",
      "Epoch 1340, CIFAR-10 Batch 1:  loss and accuracy: 1.9123, 0.4150\n",
      "Epoch 1341, CIFAR-10 Batch 1:  loss and accuracy: 1.8816, 0.4262\n",
      "Epoch 1342, CIFAR-10 Batch 1:  loss and accuracy: 1.8935, 0.4208\n",
      "Epoch 1343, CIFAR-10 Batch 1:  loss and accuracy: 1.8867, 0.4302\n",
      "Epoch 1344, CIFAR-10 Batch 1:  loss and accuracy: 1.9064, 0.4200\n",
      "Epoch 1345, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4302\n",
      "Epoch 1346, CIFAR-10 Batch 1:  loss and accuracy: 1.8812, 0.4300\n",
      "Epoch 1347, CIFAR-10 Batch 1:  loss and accuracy: 1.9107, 0.4166\n",
      "Epoch 1348, CIFAR-10 Batch 1:  loss and accuracy: 1.9049, 0.4208\n",
      "Epoch 1349, CIFAR-10 Batch 1:  loss and accuracy: 1.8887, 0.4306\n",
      "Epoch 1350, CIFAR-10 Batch 1:  loss and accuracy: 1.8849, 0.4274\n",
      "Epoch 1351, CIFAR-10 Batch 1:  loss and accuracy: 1.8797, 0.4322\n",
      "Epoch 1352, CIFAR-10 Batch 1:  loss and accuracy: 1.8890, 0.4312\n",
      "Epoch 1353, CIFAR-10 Batch 1:  loss and accuracy: 1.8884, 0.4268\n",
      "Epoch 1354, CIFAR-10 Batch 1:  loss and accuracy: 1.8923, 0.4304\n",
      "Epoch 1355, CIFAR-10 Batch 1:  loss and accuracy: 1.8972, 0.4286\n",
      "Epoch 1356, CIFAR-10 Batch 1:  loss and accuracy: 1.8876, 0.4284\n",
      "Epoch 1357, CIFAR-10 Batch 1:  loss and accuracy: 1.8897, 0.4276\n",
      "Epoch 1358, CIFAR-10 Batch 1:  loss and accuracy: 1.8778, 0.4408\n",
      "Epoch 1359, CIFAR-10 Batch 1:  loss and accuracy: 1.8834, 0.4386\n",
      "Epoch 1360, CIFAR-10 Batch 1:  loss and accuracy: 1.8775, 0.4350\n",
      "Epoch 1361, CIFAR-10 Batch 1:  loss and accuracy: 1.8861, 0.4304\n",
      "Epoch 1362, CIFAR-10 Batch 1:  loss and accuracy: 1.8887, 0.4270\n",
      "Epoch 1363, CIFAR-10 Batch 1:  loss and accuracy: 1.8963, 0.4058\n",
      "Epoch 1364, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4330\n",
      "Epoch 1365, CIFAR-10 Batch 1:  loss and accuracy: 1.8909, 0.4290\n",
      "Epoch 1366, CIFAR-10 Batch 1:  loss and accuracy: 1.8926, 0.4246\n",
      "Epoch 1367, CIFAR-10 Batch 1:  loss and accuracy: 1.9090, 0.4144\n",
      "Epoch 1368, CIFAR-10 Batch 1:  loss and accuracy: 1.8879, 0.4288\n",
      "Epoch 1369, CIFAR-10 Batch 1:  loss and accuracy: 1.8920, 0.4202\n",
      "Epoch 1370, CIFAR-10 Batch 1:  loss and accuracy: 1.9008, 0.4154\n",
      "Epoch 1371, CIFAR-10 Batch 1:  loss and accuracy: 1.8857, 0.4230\n",
      "Epoch 1372, CIFAR-10 Batch 1:  loss and accuracy: 1.9100, 0.4124\n",
      "Epoch 1373, CIFAR-10 Batch 1:  loss and accuracy: 1.9020, 0.4140\n",
      "Epoch 1374, CIFAR-10 Batch 1:  loss and accuracy: 1.8883, 0.4326\n",
      "Epoch 1375, CIFAR-10 Batch 1:  loss and accuracy: 1.9060, 0.4214\n",
      "Epoch 1376, CIFAR-10 Batch 1:  loss and accuracy: 1.8770, 0.4374\n",
      "Epoch 1377, CIFAR-10 Batch 1:  loss and accuracy: 1.8871, 0.4240\n",
      "Epoch 1378, CIFAR-10 Batch 1:  loss and accuracy: 1.8963, 0.4230\n",
      "Epoch 1379, CIFAR-10 Batch 1:  loss and accuracy: 1.9094, 0.4084\n",
      "Epoch 1380, CIFAR-10 Batch 1:  loss and accuracy: 1.8850, 0.4350\n",
      "Epoch 1381, CIFAR-10 Batch 1:  loss and accuracy: 1.9021, 0.4270\n",
      "Epoch 1382, CIFAR-10 Batch 1:  loss and accuracy: 1.9145, 0.4206\n",
      "Epoch 1383, CIFAR-10 Batch 1:  loss and accuracy: 1.9028, 0.4250\n",
      "Epoch 1384, CIFAR-10 Batch 1:  loss and accuracy: 1.8958, 0.4318\n",
      "Epoch 1385, CIFAR-10 Batch 1:  loss and accuracy: 1.8984, 0.4290\n",
      "Epoch 1386, CIFAR-10 Batch 1:  loss and accuracy: 1.8915, 0.4350\n",
      "Epoch 1387, CIFAR-10 Batch 1:  loss and accuracy: 1.8838, 0.4380\n",
      "Epoch 1388, CIFAR-10 Batch 1:  loss and accuracy: 1.8867, 0.4388\n",
      "Epoch 1389, CIFAR-10 Batch 1:  loss and accuracy: 1.8987, 0.4298\n",
      "Epoch 1390, CIFAR-10 Batch 1:  loss and accuracy: 1.8807, 0.4410\n",
      "Epoch 1391, CIFAR-10 Batch 1:  loss and accuracy: 1.8827, 0.4388\n",
      "Epoch 1392, CIFAR-10 Batch 1:  loss and accuracy: 1.9090, 0.4148\n",
      "Epoch 1393, CIFAR-10 Batch 1:  loss and accuracy: 1.8858, 0.4350\n",
      "Epoch 1394, CIFAR-10 Batch 1:  loss and accuracy: 1.8867, 0.4334\n",
      "Epoch 1395, CIFAR-10 Batch 1:  loss and accuracy: 1.8818, 0.4364\n",
      "Epoch 1396, CIFAR-10 Batch 1:  loss and accuracy: 1.8890, 0.4330\n",
      "Epoch 1397, CIFAR-10 Batch 1:  loss and accuracy: 1.8923, 0.4290\n",
      "Epoch 1398, CIFAR-10 Batch 1:  loss and accuracy: 1.9173, 0.4092\n",
      "Epoch 1399, CIFAR-10 Batch 1:  loss and accuracy: 1.8868, 0.4336\n",
      "Epoch 1400, CIFAR-10 Batch 1:  loss and accuracy: 1.9146, 0.4138\n",
      "Epoch 1401, CIFAR-10 Batch 1:  loss and accuracy: 1.8974, 0.4276\n",
      "Epoch 1402, CIFAR-10 Batch 1:  loss and accuracy: 1.9025, 0.4152\n",
      "Epoch 1403, CIFAR-10 Batch 1:  loss and accuracy: 1.8815, 0.4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1404, CIFAR-10 Batch 1:  loss and accuracy: 1.8911, 0.4272\n",
      "Epoch 1405, CIFAR-10 Batch 1:  loss and accuracy: 1.9167, 0.4092\n",
      "Epoch 1406, CIFAR-10 Batch 1:  loss and accuracy: 1.8998, 0.4188\n",
      "Epoch 1407, CIFAR-10 Batch 1:  loss and accuracy: 1.8705, 0.4390\n",
      "Epoch 1408, CIFAR-10 Batch 1:  loss and accuracy: 1.8831, 0.4252\n",
      "Epoch 1409, CIFAR-10 Batch 1:  loss and accuracy: 1.8943, 0.4142\n",
      "Epoch 1410, CIFAR-10 Batch 1:  loss and accuracy: 1.9113, 0.4018\n",
      "Epoch 1411, CIFAR-10 Batch 1:  loss and accuracy: 1.8964, 0.4286\n",
      "Epoch 1412, CIFAR-10 Batch 1:  loss and accuracy: 1.8885, 0.4286\n",
      "Epoch 1413, CIFAR-10 Batch 1:  loss and accuracy: 1.8878, 0.4376\n",
      "Epoch 1414, CIFAR-10 Batch 1:  loss and accuracy: 1.9074, 0.4234\n",
      "Epoch 1415, CIFAR-10 Batch 1:  loss and accuracy: 1.8733, 0.4462\n",
      "Epoch 1416, CIFAR-10 Batch 1:  loss and accuracy: 1.8980, 0.4320\n",
      "Epoch 1417, CIFAR-10 Batch 1:  loss and accuracy: 1.8972, 0.4272\n",
      "Epoch 1418, CIFAR-10 Batch 1:  loss and accuracy: 1.8869, 0.4272\n",
      "Epoch 1419, CIFAR-10 Batch 1:  loss and accuracy: 1.8956, 0.4270\n",
      "Epoch 1420, CIFAR-10 Batch 1:  loss and accuracy: 1.9053, 0.4246\n",
      "Epoch 1421, CIFAR-10 Batch 1:  loss and accuracy: 1.8910, 0.4336\n",
      "Epoch 1422, CIFAR-10 Batch 1:  loss and accuracy: 1.8926, 0.4298\n",
      "Epoch 1423, CIFAR-10 Batch 1:  loss and accuracy: 1.8751, 0.4470\n",
      "Epoch 1424, CIFAR-10 Batch 1:  loss and accuracy: 1.8905, 0.4196\n",
      "Epoch 1425, CIFAR-10 Batch 1:  loss and accuracy: 1.8994, 0.4164\n",
      "Epoch 1426, CIFAR-10 Batch 1:  loss and accuracy: 1.9158, 0.4092\n",
      "Epoch 1427, CIFAR-10 Batch 1:  loss and accuracy: 1.9082, 0.4212\n",
      "Epoch 1428, CIFAR-10 Batch 1:  loss and accuracy: 1.8885, 0.4328\n",
      "Epoch 1429, CIFAR-10 Batch 1:  loss and accuracy: 1.9141, 0.4220\n",
      "Epoch 1430, CIFAR-10 Batch 1:  loss and accuracy: 1.9039, 0.4242\n",
      "Epoch 1431, CIFAR-10 Batch 1:  loss and accuracy: 1.8941, 0.4308\n",
      "Epoch 1432, CIFAR-10 Batch 1:  loss and accuracy: 1.9046, 0.4184\n",
      "Epoch 1433, CIFAR-10 Batch 1:  loss and accuracy: 1.8920, 0.4284\n",
      "Epoch 1434, CIFAR-10 Batch 1:  loss and accuracy: 1.8860, 0.4344\n",
      "Epoch 1435, CIFAR-10 Batch 1:  loss and accuracy: 1.8899, 0.4384\n",
      "Epoch 1436, CIFAR-10 Batch 1:  loss and accuracy: 1.9075, 0.4194\n",
      "Epoch 1437, CIFAR-10 Batch 1:  loss and accuracy: 1.8975, 0.4256\n",
      "Epoch 1438, CIFAR-10 Batch 1:  loss and accuracy: 1.9001, 0.4164\n",
      "Epoch 1439, CIFAR-10 Batch 1:  loss and accuracy: 1.8917, 0.4290\n",
      "Epoch 1440, CIFAR-10 Batch 1:  loss and accuracy: 1.8787, 0.4388\n",
      "Epoch 1441, CIFAR-10 Batch 1:  loss and accuracy: 1.8848, 0.4404\n",
      "Epoch 1442, CIFAR-10 Batch 1:  loss and accuracy: 1.9072, 0.4200\n",
      "Epoch 1443, CIFAR-10 Batch 1:  loss and accuracy: 1.8812, 0.4390\n",
      "Epoch 1444, CIFAR-10 Batch 1:  loss and accuracy: 1.8697, 0.4474\n",
      "Epoch 1445, CIFAR-10 Batch 1:  loss and accuracy: 1.8842, 0.4436\n",
      "Epoch 1446, CIFAR-10 Batch 1:  loss and accuracy: 1.8842, 0.4366\n",
      "Epoch 1447, CIFAR-10 Batch 1:  loss and accuracy: 1.8910, 0.4322\n",
      "Epoch 1448, CIFAR-10 Batch 1:  loss and accuracy: 1.8979, 0.4268\n",
      "Epoch 1449, CIFAR-10 Batch 1:  loss and accuracy: 1.9043, 0.4230\n",
      "Epoch 1450, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4352\n",
      "Epoch 1451, CIFAR-10 Batch 1:  loss and accuracy: 1.8803, 0.4374\n",
      "Epoch 1452, CIFAR-10 Batch 1:  loss and accuracy: 1.8784, 0.4338\n",
      "Epoch 1453, CIFAR-10 Batch 1:  loss and accuracy: 1.8928, 0.4238\n",
      "Epoch 1454, CIFAR-10 Batch 1:  loss and accuracy: 1.8827, 0.4370\n",
      "Epoch 1455, CIFAR-10 Batch 1:  loss and accuracy: 1.8829, 0.4326\n",
      "Epoch 1456, CIFAR-10 Batch 1:  loss and accuracy: 1.8813, 0.4244\n",
      "Epoch 1457, CIFAR-10 Batch 1:  loss and accuracy: 1.8716, 0.4368\n",
      "Epoch 1458, CIFAR-10 Batch 1:  loss and accuracy: 1.8901, 0.4286\n",
      "Epoch 1459, CIFAR-10 Batch 1:  loss and accuracy: 1.8796, 0.4316\n",
      "Epoch 1460, CIFAR-10 Batch 1:  loss and accuracy: 1.8839, 0.4346\n",
      "Epoch 1461, CIFAR-10 Batch 1:  loss and accuracy: 1.9117, 0.4142\n",
      "Epoch 1462, CIFAR-10 Batch 1:  loss and accuracy: 1.8912, 0.4272\n",
      "Epoch 1463, CIFAR-10 Batch 1:  loss and accuracy: 1.8936, 0.4364\n",
      "Epoch 1464, CIFAR-10 Batch 1:  loss and accuracy: 1.8940, 0.4272\n",
      "Epoch 1465, CIFAR-10 Batch 1:  loss and accuracy: 1.8904, 0.4248\n",
      "Epoch 1466, CIFAR-10 Batch 1:  loss and accuracy: 1.8885, 0.4376\n",
      "Epoch 1467, CIFAR-10 Batch 1:  loss and accuracy: 1.8891, 0.4316\n",
      "Epoch 1468, CIFAR-10 Batch 1:  loss and accuracy: 1.8915, 0.4344\n",
      "Epoch 1469, CIFAR-10 Batch 1:  loss and accuracy: 1.8971, 0.4362\n",
      "Epoch 1470, CIFAR-10 Batch 1:  loss and accuracy: 1.9043, 0.4298\n",
      "Epoch 1471, CIFAR-10 Batch 1:  loss and accuracy: 1.8976, 0.4264\n",
      "Epoch 1472, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4532\n",
      "Epoch 1473, CIFAR-10 Batch 1:  loss and accuracy: 1.8892, 0.4460\n",
      "Epoch 1474, CIFAR-10 Batch 1:  loss and accuracy: 1.8826, 0.4470\n",
      "Epoch 1475, CIFAR-10 Batch 1:  loss and accuracy: 1.8919, 0.4384\n",
      "Epoch 1476, CIFAR-10 Batch 1:  loss and accuracy: 1.8776, 0.4468\n",
      "Epoch 1477, CIFAR-10 Batch 1:  loss and accuracy: 1.8859, 0.4476\n",
      "Epoch 1478, CIFAR-10 Batch 1:  loss and accuracy: 1.8784, 0.4416\n",
      "Epoch 1479, CIFAR-10 Batch 1:  loss and accuracy: 1.8975, 0.4252\n",
      "Epoch 1480, CIFAR-10 Batch 1:  loss and accuracy: 1.8912, 0.4356\n",
      "Epoch 1481, CIFAR-10 Batch 1:  loss and accuracy: 1.8749, 0.4490\n",
      "Epoch 1482, CIFAR-10 Batch 1:  loss and accuracy: 1.8804, 0.4384\n",
      "Epoch 1483, CIFAR-10 Batch 1:  loss and accuracy: 1.8786, 0.4462\n",
      "Epoch 1484, CIFAR-10 Batch 1:  loss and accuracy: 1.8736, 0.4414\n",
      "Epoch 1485, CIFAR-10 Batch 1:  loss and accuracy: 1.8983, 0.4240\n",
      "Epoch 1486, CIFAR-10 Batch 1:  loss and accuracy: 1.8850, 0.4350\n",
      "Epoch 1487, CIFAR-10 Batch 1:  loss and accuracy: 1.8699, 0.4460\n",
      "Epoch 1488, CIFAR-10 Batch 1:  loss and accuracy: 1.8869, 0.4358\n",
      "Epoch 1489, CIFAR-10 Batch 1:  loss and accuracy: 1.8955, 0.4248\n",
      "Epoch 1490, CIFAR-10 Batch 1:  loss and accuracy: 1.8770, 0.4420\n",
      "Epoch 1491, CIFAR-10 Batch 1:  loss and accuracy: 1.9090, 0.4264\n",
      "Epoch 1492, CIFAR-10 Batch 1:  loss and accuracy: 1.8985, 0.4280\n",
      "Epoch 1493, CIFAR-10 Batch 1:  loss and accuracy: 1.8803, 0.4494\n",
      "Epoch 1494, CIFAR-10 Batch 1:  loss and accuracy: 1.8798, 0.4420\n",
      "Epoch 1495, CIFAR-10 Batch 1:  loss and accuracy: 1.8896, 0.4376\n",
      "Epoch 1496, CIFAR-10 Batch 1:  loss and accuracy: 1.8889, 0.4394\n",
      "Epoch 1497, CIFAR-10 Batch 1:  loss and accuracy: 1.8779, 0.4520\n",
      "Epoch 1498, CIFAR-10 Batch 1:  loss and accuracy: 1.8926, 0.4382\n",
      "Epoch 1499, CIFAR-10 Batch 1:  loss and accuracy: 1.8902, 0.4396\n",
      "Epoch 1500, CIFAR-10 Batch 1:  loss and accuracy: 1.9043, 0.4240\n",
      "Epoch 1501, CIFAR-10 Batch 1:  loss and accuracy: 1.8815, 0.4476\n",
      "Epoch 1502, CIFAR-10 Batch 1:  loss and accuracy: 1.8941, 0.4300\n",
      "Epoch 1503, CIFAR-10 Batch 1:  loss and accuracy: 1.8844, 0.4368\n",
      "Epoch 1504, CIFAR-10 Batch 1:  loss and accuracy: 1.8833, 0.4420\n",
      "Epoch 1505, CIFAR-10 Batch 1:  loss and accuracy: 1.8988, 0.4350\n",
      "Epoch 1506, CIFAR-10 Batch 1:  loss and accuracy: 1.8729, 0.4540\n",
      "Epoch 1507, CIFAR-10 Batch 1:  loss and accuracy: 1.8900, 0.4398\n",
      "Epoch 1508, CIFAR-10 Batch 1:  loss and accuracy: 1.8881, 0.4362\n",
      "Epoch 1509, CIFAR-10 Batch 1:  loss and accuracy: 1.8752, 0.4552\n",
      "Epoch 1510, CIFAR-10 Batch 1:  loss and accuracy: 1.8947, 0.4274\n",
      "Epoch 1511, CIFAR-10 Batch 1:  loss and accuracy: 1.8951, 0.4352\n",
      "Epoch 1512, CIFAR-10 Batch 1:  loss and accuracy: 1.8791, 0.4416\n",
      "Epoch 1513, CIFAR-10 Batch 1:  loss and accuracy: 1.8709, 0.4538\n",
      "Epoch 1514, CIFAR-10 Batch 1:  loss and accuracy: 1.8870, 0.4416\n",
      "Epoch 1515, CIFAR-10 Batch 1:  loss and accuracy: 1.8837, 0.4432\n",
      "Epoch 1516, CIFAR-10 Batch 1:  loss and accuracy: 1.8768, 0.4504\n",
      "Epoch 1517, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4442\n",
      "Epoch 1518, CIFAR-10 Batch 1:  loss and accuracy: 1.8834, 0.4506\n",
      "Epoch 1519, CIFAR-10 Batch 1:  loss and accuracy: 1.8919, 0.4354\n",
      "Epoch 1520, CIFAR-10 Batch 1:  loss and accuracy: 1.8830, 0.4408\n",
      "Epoch 1521, CIFAR-10 Batch 1:  loss and accuracy: 1.8881, 0.4410\n",
      "Epoch 1522, CIFAR-10 Batch 1:  loss and accuracy: 1.8753, 0.4482\n",
      "Epoch 1523, CIFAR-10 Batch 1:  loss and accuracy: 1.8809, 0.4438\n",
      "Epoch 1524, CIFAR-10 Batch 1:  loss and accuracy: 1.9037, 0.4238\n",
      "Epoch 1525, CIFAR-10 Batch 1:  loss and accuracy: 1.8878, 0.4410\n",
      "Epoch 1526, CIFAR-10 Batch 1:  loss and accuracy: 1.8954, 0.4268\n",
      "Epoch 1527, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4382\n",
      "Epoch 1528, CIFAR-10 Batch 1:  loss and accuracy: 1.8997, 0.4284\n",
      "Epoch 1529, CIFAR-10 Batch 1:  loss and accuracy: 1.8859, 0.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1530, CIFAR-10 Batch 1:  loss and accuracy: 1.8883, 0.4388\n",
      "Epoch 1531, CIFAR-10 Batch 1:  loss and accuracy: 1.8928, 0.4330\n",
      "Epoch 1532, CIFAR-10 Batch 1:  loss and accuracy: 1.8883, 0.4364\n",
      "Epoch 1533, CIFAR-10 Batch 1:  loss and accuracy: 1.8814, 0.4402\n",
      "Epoch 1534, CIFAR-10 Batch 1:  loss and accuracy: 1.8944, 0.4354\n",
      "Epoch 1535, CIFAR-10 Batch 1:  loss and accuracy: 1.8969, 0.4298\n",
      "Epoch 1536, CIFAR-10 Batch 1:  loss and accuracy: 1.8914, 0.4330\n",
      "Epoch 1537, CIFAR-10 Batch 1:  loss and accuracy: 1.9010, 0.4276\n",
      "Epoch 1538, CIFAR-10 Batch 1:  loss and accuracy: 1.8982, 0.4282\n",
      "Epoch 1539, CIFAR-10 Batch 1:  loss and accuracy: 1.8866, 0.4430\n",
      "Epoch 1540, CIFAR-10 Batch 1:  loss and accuracy: 1.8796, 0.4464\n",
      "Epoch 1541, CIFAR-10 Batch 1:  loss and accuracy: 1.8739, 0.4526\n",
      "Epoch 1542, CIFAR-10 Batch 1:  loss and accuracy: 1.8920, 0.4316\n",
      "Epoch 1543, CIFAR-10 Batch 1:  loss and accuracy: 1.8749, 0.4434\n",
      "Epoch 1544, CIFAR-10 Batch 1:  loss and accuracy: 1.8836, 0.4352\n",
      "Epoch 1545, CIFAR-10 Batch 1:  loss and accuracy: 1.8890, 0.4390\n",
      "Epoch 1546, CIFAR-10 Batch 1:  loss and accuracy: 1.8700, 0.4530\n",
      "Epoch 1547, CIFAR-10 Batch 1:  loss and accuracy: 1.8768, 0.4442\n",
      "Epoch 1548, CIFAR-10 Batch 1:  loss and accuracy: 1.8708, 0.4468\n",
      "Epoch 1549, CIFAR-10 Batch 1:  loss and accuracy: 1.8880, 0.4296\n",
      "Epoch 1550, CIFAR-10 Batch 1:  loss and accuracy: 1.8900, 0.4326\n",
      "Epoch 1551, CIFAR-10 Batch 1:  loss and accuracy: 1.8738, 0.4442\n",
      "Epoch 1552, CIFAR-10 Batch 1:  loss and accuracy: 1.8771, 0.4502\n",
      "Epoch 1553, CIFAR-10 Batch 1:  loss and accuracy: 1.8869, 0.4402\n",
      "Epoch 1554, CIFAR-10 Batch 1:  loss and accuracy: 1.8744, 0.4564\n",
      "Epoch 1555, CIFAR-10 Batch 1:  loss and accuracy: 1.8764, 0.4490\n",
      "Epoch 1556, CIFAR-10 Batch 1:  loss and accuracy: 1.9009, 0.4208\n",
      "Epoch 1557, CIFAR-10 Batch 1:  loss and accuracy: 1.8814, 0.4422\n",
      "Epoch 1558, CIFAR-10 Batch 1:  loss and accuracy: 1.8730, 0.4400\n",
      "Epoch 1559, CIFAR-10 Batch 1:  loss and accuracy: 1.8732, 0.4442\n",
      "Epoch 1560, CIFAR-10 Batch 1:  loss and accuracy: 1.8765, 0.4334\n",
      "Epoch 1561, CIFAR-10 Batch 1:  loss and accuracy: 1.8859, 0.4308\n",
      "Epoch 1562, CIFAR-10 Batch 1:  loss and accuracy: 1.8708, 0.4426\n",
      "Epoch 1563, CIFAR-10 Batch 1:  loss and accuracy: 1.8981, 0.4288\n",
      "Epoch 1564, CIFAR-10 Batch 1:  loss and accuracy: 1.8766, 0.4394\n",
      "Epoch 1565, CIFAR-10 Batch 1:  loss and accuracy: 1.8793, 0.4402\n",
      "Epoch 1566, CIFAR-10 Batch 1:  loss and accuracy: 1.8710, 0.4432\n",
      "Epoch 1567, CIFAR-10 Batch 1:  loss and accuracy: 1.9057, 0.4218\n",
      "Epoch 1568, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4298\n",
      "Epoch 1569, CIFAR-10 Batch 1:  loss and accuracy: 1.8964, 0.4320\n",
      "Epoch 1570, CIFAR-10 Batch 1:  loss and accuracy: 1.9036, 0.4232\n",
      "Epoch 1571, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4594\n",
      "Epoch 1572, CIFAR-10 Batch 1:  loss and accuracy: 1.8711, 0.4478\n",
      "Epoch 1573, CIFAR-10 Batch 1:  loss and accuracy: 1.8848, 0.4370\n",
      "Epoch 1574, CIFAR-10 Batch 1:  loss and accuracy: 1.8933, 0.4364\n",
      "Epoch 1575, CIFAR-10 Batch 1:  loss and accuracy: 1.8745, 0.4536\n",
      "Epoch 1576, CIFAR-10 Batch 1:  loss and accuracy: 1.8856, 0.4420\n",
      "Epoch 1577, CIFAR-10 Batch 1:  loss and accuracy: 1.8802, 0.4516\n",
      "Epoch 1578, CIFAR-10 Batch 1:  loss and accuracy: 1.8839, 0.4402\n",
      "Epoch 1579, CIFAR-10 Batch 1:  loss and accuracy: 1.8787, 0.4504\n",
      "Epoch 1580, CIFAR-10 Batch 1:  loss and accuracy: 1.8721, 0.4578\n",
      "Epoch 1581, CIFAR-10 Batch 1:  loss and accuracy: 1.8839, 0.4412\n",
      "Epoch 1582, CIFAR-10 Batch 1:  loss and accuracy: 1.8660, 0.4568\n",
      "Epoch 1583, CIFAR-10 Batch 1:  loss and accuracy: 1.8704, 0.4514\n",
      "Epoch 1584, CIFAR-10 Batch 1:  loss and accuracy: 1.8761, 0.4496\n",
      "Epoch 1585, CIFAR-10 Batch 1:  loss and accuracy: 1.8814, 0.4500\n",
      "Epoch 1586, CIFAR-10 Batch 1:  loss and accuracy: 1.8759, 0.4500\n",
      "Epoch 1587, CIFAR-10 Batch 1:  loss and accuracy: 1.8904, 0.4350\n",
      "Epoch 1588, CIFAR-10 Batch 1:  loss and accuracy: 1.8799, 0.4420\n",
      "Epoch 1589, CIFAR-10 Batch 1:  loss and accuracy: 1.8847, 0.4348\n",
      "Epoch 1590, CIFAR-10 Batch 1:  loss and accuracy: 1.8756, 0.4524\n",
      "Epoch 1591, CIFAR-10 Batch 1:  loss and accuracy: 1.8706, 0.4516\n",
      "Epoch 1592, CIFAR-10 Batch 1:  loss and accuracy: 1.8723, 0.4502\n",
      "Epoch 1593, CIFAR-10 Batch 1:  loss and accuracy: 1.8658, 0.4516\n",
      "Epoch 1594, CIFAR-10 Batch 1:  loss and accuracy: 1.8711, 0.4488\n",
      "Epoch 1595, CIFAR-10 Batch 1:  loss and accuracy: 1.8716, 0.4490\n",
      "Epoch 1596, CIFAR-10 Batch 1:  loss and accuracy: 1.8830, 0.4412\n",
      "Epoch 1597, CIFAR-10 Batch 1:  loss and accuracy: 1.8912, 0.4358\n",
      "Epoch 1598, CIFAR-10 Batch 1:  loss and accuracy: 1.8876, 0.4398\n",
      "Epoch 1599, CIFAR-10 Batch 1:  loss and accuracy: 1.8772, 0.4540\n",
      "Epoch 1600, CIFAR-10 Batch 1:  loss and accuracy: 1.8762, 0.4450\n",
      "Epoch 1601, CIFAR-10 Batch 1:  loss and accuracy: 1.8756, 0.4512\n",
      "Epoch 1602, CIFAR-10 Batch 1:  loss and accuracy: 1.8807, 0.4456\n",
      "Epoch 1603, CIFAR-10 Batch 1:  loss and accuracy: 1.8937, 0.4304\n",
      "Epoch 1604, CIFAR-10 Batch 1:  loss and accuracy: 1.8755, 0.4430\n",
      "Epoch 1605, CIFAR-10 Batch 1:  loss and accuracy: 1.8814, 0.4406\n",
      "Epoch 1606, CIFAR-10 Batch 1:  loss and accuracy: 1.8844, 0.4338\n",
      "Epoch 1607, CIFAR-10 Batch 1:  loss and accuracy: 1.8809, 0.4368\n",
      "Epoch 1608, CIFAR-10 Batch 1:  loss and accuracy: 1.8852, 0.4388\n",
      "Epoch 1609, CIFAR-10 Batch 1:  loss and accuracy: 1.8861, 0.4400\n",
      "Epoch 1610, CIFAR-10 Batch 1:  loss and accuracy: 1.8818, 0.4446\n",
      "Epoch 1611, CIFAR-10 Batch 1:  loss and accuracy: 1.8947, 0.4334\n",
      "Epoch 1612, CIFAR-10 Batch 1:  loss and accuracy: 1.8924, 0.4394\n",
      "Epoch 1613, CIFAR-10 Batch 1:  loss and accuracy: 1.8730, 0.4550\n",
      "Epoch 1614, CIFAR-10 Batch 1:  loss and accuracy: 1.8807, 0.4492\n",
      "Epoch 1615, CIFAR-10 Batch 1:  loss and accuracy: 1.8836, 0.4446\n",
      "Epoch 1616, CIFAR-10 Batch 1:  loss and accuracy: 1.8700, 0.4546\n",
      "Epoch 1617, CIFAR-10 Batch 1:  loss and accuracy: 1.8865, 0.4398\n",
      "Epoch 1618, CIFAR-10 Batch 1:  loss and accuracy: 1.8736, 0.4438\n",
      "Epoch 1619, CIFAR-10 Batch 1:  loss and accuracy: 1.8831, 0.4348\n",
      "Epoch 1620, CIFAR-10 Batch 1:  loss and accuracy: 1.8835, 0.4398\n",
      "Epoch 1621, CIFAR-10 Batch 1:  loss and accuracy: 1.8805, 0.4458\n",
      "Epoch 1622, CIFAR-10 Batch 1:  loss and accuracy: 1.8844, 0.4408\n",
      "Epoch 1623, CIFAR-10 Batch 1:  loss and accuracy: 1.8887, 0.4386\n",
      "Epoch 1624, CIFAR-10 Batch 1:  loss and accuracy: 1.8786, 0.4528\n",
      "Epoch 1625, CIFAR-10 Batch 1:  loss and accuracy: 1.9022, 0.4252\n",
      "Epoch 1626, CIFAR-10 Batch 1:  loss and accuracy: 1.8833, 0.4396\n",
      "Epoch 1627, CIFAR-10 Batch 1:  loss and accuracy: 1.8909, 0.4330\n",
      "Epoch 1628, CIFAR-10 Batch 1:  loss and accuracy: 1.8965, 0.4294\n",
      "Epoch 1629, CIFAR-10 Batch 1:  loss and accuracy: 1.8912, 0.4378\n",
      "Epoch 1630, CIFAR-10 Batch 1:  loss and accuracy: 1.8872, 0.4364\n",
      "Epoch 1631, CIFAR-10 Batch 1:  loss and accuracy: 1.8871, 0.4328\n",
      "Epoch 1632, CIFAR-10 Batch 1:  loss and accuracy: 1.8602, 0.4524\n",
      "Epoch 1633, CIFAR-10 Batch 1:  loss and accuracy: 1.8703, 0.4494\n",
      "Epoch 1634, CIFAR-10 Batch 1:  loss and accuracy: 1.9150, 0.4088\n",
      "Epoch 1635, CIFAR-10 Batch 1:  loss and accuracy: 1.8750, 0.4550\n",
      "Epoch 1636, CIFAR-10 Batch 1:  loss and accuracy: 1.9072, 0.4234\n",
      "Epoch 1637, CIFAR-10 Batch 1:  loss and accuracy: 1.8797, 0.4390\n",
      "Epoch 1638, CIFAR-10 Batch 1:  loss and accuracy: 1.8795, 0.4442\n",
      "Epoch 1639, CIFAR-10 Batch 1:  loss and accuracy: 1.8753, 0.4458\n",
      "Epoch 1640, CIFAR-10 Batch 1:  loss and accuracy: 1.8820, 0.4412\n",
      "Epoch 1641, CIFAR-10 Batch 1:  loss and accuracy: 1.8700, 0.4472\n",
      "Epoch 1642, CIFAR-10 Batch 1:  loss and accuracy: 1.8692, 0.4522\n",
      "Epoch 1643, CIFAR-10 Batch 1:  loss and accuracy: 1.8799, 0.4466\n",
      "Epoch 1644, CIFAR-10 Batch 1:  loss and accuracy: 1.8842, 0.4358\n",
      "Epoch 1645, CIFAR-10 Batch 1:  loss and accuracy: 1.8772, 0.4340\n",
      "Epoch 1646, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4364\n",
      "Epoch 1647, CIFAR-10 Batch 1:  loss and accuracy: 1.8736, 0.4438\n",
      "Epoch 1648, CIFAR-10 Batch 1:  loss and accuracy: 1.8857, 0.4342\n",
      "Epoch 1649, CIFAR-10 Batch 1:  loss and accuracy: 1.8707, 0.4456\n",
      "Epoch 1650, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4462\n",
      "Epoch 1651, CIFAR-10 Batch 1:  loss and accuracy: 1.8848, 0.4442\n",
      "Epoch 1652, CIFAR-10 Batch 1:  loss and accuracy: 1.8819, 0.4380\n",
      "Epoch 1653, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4280\n",
      "Epoch 1654, CIFAR-10 Batch 1:  loss and accuracy: 1.8568, 0.4606\n",
      "Epoch 1655, CIFAR-10 Batch 1:  loss and accuracy: 1.8610, 0.4504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1656, CIFAR-10 Batch 1:  loss and accuracy: 1.8698, 0.4440\n",
      "Epoch 1657, CIFAR-10 Batch 1:  loss and accuracy: 1.8851, 0.4296\n",
      "Epoch 1658, CIFAR-10 Batch 1:  loss and accuracy: 1.8679, 0.4536\n",
      "Epoch 1659, CIFAR-10 Batch 1:  loss and accuracy: 1.8720, 0.4410\n",
      "Epoch 1660, CIFAR-10 Batch 1:  loss and accuracy: 1.8728, 0.4402\n",
      "Epoch 1661, CIFAR-10 Batch 1:  loss and accuracy: 1.8700, 0.4542\n",
      "Epoch 1662, CIFAR-10 Batch 1:  loss and accuracy: 1.8828, 0.4386\n",
      "Epoch 1663, CIFAR-10 Batch 1:  loss and accuracy: 1.8847, 0.4384\n",
      "Epoch 1664, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4436\n",
      "Epoch 1665, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4262\n",
      "Epoch 1666, CIFAR-10 Batch 1:  loss and accuracy: 1.8893, 0.4344\n",
      "Epoch 1667, CIFAR-10 Batch 1:  loss and accuracy: 1.8595, 0.4630\n",
      "Epoch 1668, CIFAR-10 Batch 1:  loss and accuracy: 1.8711, 0.4538\n",
      "Epoch 1669, CIFAR-10 Batch 1:  loss and accuracy: 1.8892, 0.4468\n",
      "Epoch 1670, CIFAR-10 Batch 1:  loss and accuracy: 1.8906, 0.4374\n",
      "Epoch 1671, CIFAR-10 Batch 1:  loss and accuracy: 1.8714, 0.4486\n",
      "Epoch 1672, CIFAR-10 Batch 1:  loss and accuracy: 1.8707, 0.4492\n",
      "Epoch 1673, CIFAR-10 Batch 1:  loss and accuracy: 1.8776, 0.4416\n",
      "Epoch 1674, CIFAR-10 Batch 1:  loss and accuracy: 1.8874, 0.4354\n",
      "Epoch 1675, CIFAR-10 Batch 1:  loss and accuracy: 1.8791, 0.4472\n",
      "Epoch 1676, CIFAR-10 Batch 1:  loss and accuracy: 1.8791, 0.4462\n",
      "Epoch 1677, CIFAR-10 Batch 1:  loss and accuracy: 1.8706, 0.4468\n",
      "Epoch 1678, CIFAR-10 Batch 1:  loss and accuracy: 1.8769, 0.4464\n",
      "Epoch 1679, CIFAR-10 Batch 1:  loss and accuracy: 1.8824, 0.4438\n",
      "Epoch 1680, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4448\n",
      "Epoch 1681, CIFAR-10 Batch 1:  loss and accuracy: 1.8667, 0.4540\n",
      "Epoch 1682, CIFAR-10 Batch 1:  loss and accuracy: 1.8823, 0.4418\n",
      "Epoch 1683, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4500\n",
      "Epoch 1684, CIFAR-10 Batch 1:  loss and accuracy: 1.8805, 0.4458\n",
      "Epoch 1685, CIFAR-10 Batch 1:  loss and accuracy: 1.8673, 0.4524\n",
      "Epoch 1686, CIFAR-10 Batch 1:  loss and accuracy: 1.8883, 0.4408\n",
      "Epoch 1687, CIFAR-10 Batch 1:  loss and accuracy: 1.8940, 0.4348\n",
      "Epoch 1688, CIFAR-10 Batch 1:  loss and accuracy: 1.8687, 0.4596\n",
      "Epoch 1689, CIFAR-10 Batch 1:  loss and accuracy: 1.8579, 0.4564\n",
      "Epoch 1690, CIFAR-10 Batch 1:  loss and accuracy: 1.8635, 0.4564\n",
      "Epoch 1691, CIFAR-10 Batch 1:  loss and accuracy: 1.8806, 0.4398\n",
      "Epoch 1692, CIFAR-10 Batch 1:  loss and accuracy: 1.8718, 0.4516\n",
      "Epoch 1693, CIFAR-10 Batch 1:  loss and accuracy: 1.8706, 0.4512\n",
      "Epoch 1694, CIFAR-10 Batch 1:  loss and accuracy: 1.8823, 0.4416\n",
      "Epoch 1695, CIFAR-10 Batch 1:  loss and accuracy: 1.8741, 0.4444\n",
      "Epoch 1696, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4564\n",
      "Epoch 1697, CIFAR-10 Batch 1:  loss and accuracy: 1.8757, 0.4466\n",
      "Epoch 1698, CIFAR-10 Batch 1:  loss and accuracy: 1.8714, 0.4510\n",
      "Epoch 1699, CIFAR-10 Batch 1:  loss and accuracy: 1.8726, 0.4536\n",
      "Epoch 1700, CIFAR-10 Batch 1:  loss and accuracy: 1.8931, 0.4306\n",
      "Epoch 1701, CIFAR-10 Batch 1:  loss and accuracy: 1.8703, 0.4576\n",
      "Epoch 1702, CIFAR-10 Batch 1:  loss and accuracy: 1.8744, 0.4458\n",
      "Epoch 1703, CIFAR-10 Batch 1:  loss and accuracy: 1.8811, 0.4412\n",
      "Epoch 1704, CIFAR-10 Batch 1:  loss and accuracy: 1.8810, 0.4432\n",
      "Epoch 1705, CIFAR-10 Batch 1:  loss and accuracy: 1.8751, 0.4526\n",
      "Epoch 1706, CIFAR-10 Batch 1:  loss and accuracy: 1.8654, 0.4608\n",
      "Epoch 1707, CIFAR-10 Batch 1:  loss and accuracy: 1.8850, 0.4376\n",
      "Epoch 1708, CIFAR-10 Batch 1:  loss and accuracy: 1.8732, 0.4542\n",
      "Epoch 1709, CIFAR-10 Batch 1:  loss and accuracy: 1.8690, 0.4620\n",
      "Epoch 1710, CIFAR-10 Batch 1:  loss and accuracy: 1.8702, 0.4486\n",
      "Epoch 1711, CIFAR-10 Batch 1:  loss and accuracy: 1.8625, 0.4574\n",
      "Epoch 1712, CIFAR-10 Batch 1:  loss and accuracy: 1.8694, 0.4510\n",
      "Epoch 1713, CIFAR-10 Batch 1:  loss and accuracy: 1.8697, 0.4518\n",
      "Epoch 1714, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4430\n",
      "Epoch 1715, CIFAR-10 Batch 1:  loss and accuracy: 1.8790, 0.4460\n",
      "Epoch 1716, CIFAR-10 Batch 1:  loss and accuracy: 1.8698, 0.4532\n",
      "Epoch 1717, CIFAR-10 Batch 1:  loss and accuracy: 1.8751, 0.4450\n",
      "Epoch 1718, CIFAR-10 Batch 1:  loss and accuracy: 1.8794, 0.4416\n",
      "Epoch 1719, CIFAR-10 Batch 1:  loss and accuracy: 1.8612, 0.4626\n",
      "Epoch 1720, CIFAR-10 Batch 1:  loss and accuracy: 1.8546, 0.4614\n",
      "Epoch 1721, CIFAR-10 Batch 1:  loss and accuracy: 1.8758, 0.4470\n",
      "Epoch 1722, CIFAR-10 Batch 1:  loss and accuracy: 1.8730, 0.4396\n",
      "Epoch 1723, CIFAR-10 Batch 1:  loss and accuracy: 1.8728, 0.4482\n",
      "Epoch 1724, CIFAR-10 Batch 1:  loss and accuracy: 1.8691, 0.4460\n",
      "Epoch 1725, CIFAR-10 Batch 1:  loss and accuracy: 1.8584, 0.4694\n",
      "Epoch 1726, CIFAR-10 Batch 1:  loss and accuracy: 1.8816, 0.4444\n",
      "Epoch 1727, CIFAR-10 Batch 1:  loss and accuracy: 1.8620, 0.4622\n",
      "Epoch 1728, CIFAR-10 Batch 1:  loss and accuracy: 1.8703, 0.4508\n",
      "Epoch 1729, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4542\n",
      "Epoch 1730, CIFAR-10 Batch 1:  loss and accuracy: 1.8742, 0.4538\n",
      "Epoch 1731, CIFAR-10 Batch 1:  loss and accuracy: 1.8810, 0.4458\n",
      "Epoch 1732, CIFAR-10 Batch 1:  loss and accuracy: 1.8776, 0.4526\n",
      "Epoch 1733, CIFAR-10 Batch 1:  loss and accuracy: 1.8801, 0.4498\n",
      "Epoch 1734, CIFAR-10 Batch 1:  loss and accuracy: 1.8731, 0.4508\n",
      "Epoch 1735, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4570\n",
      "Epoch 1736, CIFAR-10 Batch 1:  loss and accuracy: 1.8804, 0.4516\n",
      "Epoch 1737, CIFAR-10 Batch 1:  loss and accuracy: 1.8868, 0.4440\n",
      "Epoch 1738, CIFAR-10 Batch 1:  loss and accuracy: 1.8854, 0.4384\n",
      "Epoch 1739, CIFAR-10 Batch 1:  loss and accuracy: 1.8829, 0.4416\n",
      "Epoch 1740, CIFAR-10 Batch 1:  loss and accuracy: 1.8614, 0.4636\n",
      "Epoch 1741, CIFAR-10 Batch 1:  loss and accuracy: 1.8757, 0.4472\n",
      "Epoch 1742, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4582\n",
      "Epoch 1743, CIFAR-10 Batch 1:  loss and accuracy: 1.8678, 0.4502\n",
      "Epoch 1744, CIFAR-10 Batch 1:  loss and accuracy: 1.8681, 0.4546\n",
      "Epoch 1745, CIFAR-10 Batch 1:  loss and accuracy: 1.8715, 0.4416\n",
      "Epoch 1746, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4550\n",
      "Epoch 1747, CIFAR-10 Batch 1:  loss and accuracy: 1.8793, 0.4504\n",
      "Epoch 1748, CIFAR-10 Batch 1:  loss and accuracy: 1.8921, 0.4212\n",
      "Epoch 1749, CIFAR-10 Batch 1:  loss and accuracy: 1.8790, 0.4376\n",
      "Epoch 1750, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4544\n",
      "Epoch 1751, CIFAR-10 Batch 1:  loss and accuracy: 1.8831, 0.4358\n",
      "Epoch 1752, CIFAR-10 Batch 1:  loss and accuracy: 1.8886, 0.4338\n",
      "Epoch 1753, CIFAR-10 Batch 1:  loss and accuracy: 1.8705, 0.4520\n",
      "Epoch 1754, CIFAR-10 Batch 1:  loss and accuracy: 1.8641, 0.4586\n",
      "Epoch 1755, CIFAR-10 Batch 1:  loss and accuracy: 1.8594, 0.4664\n",
      "Epoch 1756, CIFAR-10 Batch 1:  loss and accuracy: 1.8644, 0.4576\n",
      "Epoch 1757, CIFAR-10 Batch 1:  loss and accuracy: 1.8739, 0.4512\n",
      "Epoch 1758, CIFAR-10 Batch 1:  loss and accuracy: 1.8717, 0.4580\n",
      "Epoch 1759, CIFAR-10 Batch 1:  loss and accuracy: 1.8717, 0.4540\n",
      "Epoch 1760, CIFAR-10 Batch 1:  loss and accuracy: 1.8730, 0.4544\n",
      "Epoch 1761, CIFAR-10 Batch 1:  loss and accuracy: 1.8613, 0.4622\n",
      "Epoch 1762, CIFAR-10 Batch 1:  loss and accuracy: 1.8796, 0.4538\n",
      "Epoch 1763, CIFAR-10 Batch 1:  loss and accuracy: 1.8832, 0.4484\n",
      "Epoch 1764, CIFAR-10 Batch 1:  loss and accuracy: 1.8696, 0.4610\n",
      "Epoch 1765, CIFAR-10 Batch 1:  loss and accuracy: 1.8726, 0.4430\n",
      "Epoch 1766, CIFAR-10 Batch 1:  loss and accuracy: 1.8676, 0.4580\n",
      "Epoch 1767, CIFAR-10 Batch 1:  loss and accuracy: 1.8712, 0.4578\n",
      "Epoch 1768, CIFAR-10 Batch 1:  loss and accuracy: 1.8790, 0.4532\n",
      "Epoch 1769, CIFAR-10 Batch 1:  loss and accuracy: 1.8838, 0.4426\n",
      "Epoch 1770, CIFAR-10 Batch 1:  loss and accuracy: 1.8914, 0.4376\n",
      "Epoch 1771, CIFAR-10 Batch 1:  loss and accuracy: 1.8775, 0.4442\n",
      "Epoch 1772, CIFAR-10 Batch 1:  loss and accuracy: 1.8724, 0.4522\n",
      "Epoch 1773, CIFAR-10 Batch 1:  loss and accuracy: 1.8668, 0.4578\n",
      "Epoch 1774, CIFAR-10 Batch 1:  loss and accuracy: 1.8864, 0.4344\n",
      "Epoch 1775, CIFAR-10 Batch 1:  loss and accuracy: 1.8807, 0.4528\n",
      "Epoch 1776, CIFAR-10 Batch 1:  loss and accuracy: 1.8558, 0.4680\n",
      "Epoch 1777, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4598\n",
      "Epoch 1778, CIFAR-10 Batch 1:  loss and accuracy: 1.8628, 0.4630\n",
      "Epoch 1779, CIFAR-10 Batch 1:  loss and accuracy: 1.8503, 0.4668\n",
      "Epoch 1780, CIFAR-10 Batch 1:  loss and accuracy: 1.8674, 0.4590\n",
      "Epoch 1781, CIFAR-10 Batch 1:  loss and accuracy: 1.8901, 0.4394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1782, CIFAR-10 Batch 1:  loss and accuracy: 1.8571, 0.4644\n",
      "Epoch 1783, CIFAR-10 Batch 1:  loss and accuracy: 1.8758, 0.4516\n",
      "Epoch 1784, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4620\n",
      "Epoch 1785, CIFAR-10 Batch 1:  loss and accuracy: 1.8801, 0.4392\n",
      "Epoch 1786, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4588\n",
      "Epoch 1787, CIFAR-10 Batch 1:  loss and accuracy: 1.8622, 0.4618\n",
      "Epoch 1788, CIFAR-10 Batch 1:  loss and accuracy: 1.8774, 0.4410\n",
      "Epoch 1789, CIFAR-10 Batch 1:  loss and accuracy: 1.8608, 0.4684\n",
      "Epoch 1790, CIFAR-10 Batch 1:  loss and accuracy: 1.8844, 0.4456\n",
      "Epoch 1791, CIFAR-10 Batch 1:  loss and accuracy: 1.8559, 0.4656\n",
      "Epoch 1792, CIFAR-10 Batch 1:  loss and accuracy: 1.8698, 0.4630\n",
      "Epoch 1793, CIFAR-10 Batch 1:  loss and accuracy: 1.8575, 0.4630\n",
      "Epoch 1794, CIFAR-10 Batch 1:  loss and accuracy: 1.8603, 0.4588\n",
      "Epoch 1795, CIFAR-10 Batch 1:  loss and accuracy: 1.8604, 0.4658\n",
      "Epoch 1796, CIFAR-10 Batch 1:  loss and accuracy: 1.8748, 0.4554\n",
      "Epoch 1797, CIFAR-10 Batch 1:  loss and accuracy: 1.8735, 0.4520\n",
      "Epoch 1798, CIFAR-10 Batch 1:  loss and accuracy: 1.8713, 0.4564\n",
      "Epoch 1799, CIFAR-10 Batch 1:  loss and accuracy: 1.8706, 0.4550\n",
      "Epoch 1800, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4606\n",
      "Epoch 1801, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4630\n",
      "Epoch 1802, CIFAR-10 Batch 1:  loss and accuracy: 1.8760, 0.4502\n",
      "Epoch 1803, CIFAR-10 Batch 1:  loss and accuracy: 1.8794, 0.4518\n",
      "Epoch 1804, CIFAR-10 Batch 1:  loss and accuracy: 1.8790, 0.4478\n",
      "Epoch 1805, CIFAR-10 Batch 1:  loss and accuracy: 1.8794, 0.4518\n",
      "Epoch 1806, CIFAR-10 Batch 1:  loss and accuracy: 1.8819, 0.4428\n",
      "Epoch 1807, CIFAR-10 Batch 1:  loss and accuracy: 1.8775, 0.4494\n",
      "Epoch 1808, CIFAR-10 Batch 1:  loss and accuracy: 1.8684, 0.4512\n",
      "Epoch 1809, CIFAR-10 Batch 1:  loss and accuracy: 1.8624, 0.4548\n",
      "Epoch 1810, CIFAR-10 Batch 1:  loss and accuracy: 1.8677, 0.4536\n",
      "Epoch 1811, CIFAR-10 Batch 1:  loss and accuracy: 1.8730, 0.4440\n",
      "Epoch 1812, CIFAR-10 Batch 1:  loss and accuracy: 1.8738, 0.4464\n",
      "Epoch 1813, CIFAR-10 Batch 1:  loss and accuracy: 1.8733, 0.4546\n",
      "Epoch 1814, CIFAR-10 Batch 1:  loss and accuracy: 1.8915, 0.4370\n",
      "Epoch 1815, CIFAR-10 Batch 1:  loss and accuracy: 1.8656, 0.4612\n",
      "Epoch 1816, CIFAR-10 Batch 1:  loss and accuracy: 1.8793, 0.4502\n",
      "Epoch 1817, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4578\n",
      "Epoch 1818, CIFAR-10 Batch 1:  loss and accuracy: 1.8667, 0.4592\n",
      "Epoch 1819, CIFAR-10 Batch 1:  loss and accuracy: 1.8727, 0.4552\n",
      "Epoch 1820, CIFAR-10 Batch 1:  loss and accuracy: 1.8718, 0.4540\n",
      "Epoch 1821, CIFAR-10 Batch 1:  loss and accuracy: 1.8748, 0.4522\n",
      "Epoch 1822, CIFAR-10 Batch 1:  loss and accuracy: 1.8726, 0.4476\n",
      "Epoch 1823, CIFAR-10 Batch 1:  loss and accuracy: 1.8657, 0.4592\n",
      "Epoch 1824, CIFAR-10 Batch 1:  loss and accuracy: 1.8594, 0.4632\n",
      "Epoch 1825, CIFAR-10 Batch 1:  loss and accuracy: 1.8680, 0.4606\n",
      "Epoch 1826, CIFAR-10 Batch 1:  loss and accuracy: 1.8628, 0.4624\n",
      "Epoch 1827, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4596\n",
      "Epoch 1828, CIFAR-10 Batch 1:  loss and accuracy: 1.8723, 0.4498\n",
      "Epoch 1829, CIFAR-10 Batch 1:  loss and accuracy: 1.8680, 0.4486\n",
      "Epoch 1830, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4502\n",
      "Epoch 1831, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4528\n",
      "Epoch 1832, CIFAR-10 Batch 1:  loss and accuracy: 1.8655, 0.4480\n",
      "Epoch 1833, CIFAR-10 Batch 1:  loss and accuracy: 1.8735, 0.4386\n",
      "Epoch 1834, CIFAR-10 Batch 1:  loss and accuracy: 1.8755, 0.4402\n",
      "Epoch 1835, CIFAR-10 Batch 1:  loss and accuracy: 1.8629, 0.4540\n",
      "Epoch 1836, CIFAR-10 Batch 1:  loss and accuracy: 1.8692, 0.4520\n",
      "Epoch 1837, CIFAR-10 Batch 1:  loss and accuracy: 1.8655, 0.4492\n",
      "Epoch 1838, CIFAR-10 Batch 1:  loss and accuracy: 1.8679, 0.4410\n",
      "Epoch 1839, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4552\n",
      "Epoch 1840, CIFAR-10 Batch 1:  loss and accuracy: 1.8761, 0.4384\n",
      "Epoch 1841, CIFAR-10 Batch 1:  loss and accuracy: 1.8778, 0.4400\n",
      "Epoch 1842, CIFAR-10 Batch 1:  loss and accuracy: 1.8808, 0.4384\n",
      "Epoch 1843, CIFAR-10 Batch 1:  loss and accuracy: 1.8822, 0.4334\n",
      "Epoch 1844, CIFAR-10 Batch 1:  loss and accuracy: 1.8565, 0.4606\n",
      "Epoch 1845, CIFAR-10 Batch 1:  loss and accuracy: 1.8582, 0.4610\n",
      "Epoch 1846, CIFAR-10 Batch 1:  loss and accuracy: 1.8806, 0.4392\n",
      "Epoch 1847, CIFAR-10 Batch 1:  loss and accuracy: 1.8680, 0.4632\n",
      "Epoch 1848, CIFAR-10 Batch 1:  loss and accuracy: 1.8709, 0.4538\n",
      "Epoch 1849, CIFAR-10 Batch 1:  loss and accuracy: 1.8610, 0.4606\n",
      "Epoch 1850, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4592\n",
      "Epoch 1851, CIFAR-10 Batch 1:  loss and accuracy: 1.8647, 0.4610\n",
      "Epoch 1852, CIFAR-10 Batch 1:  loss and accuracy: 1.8722, 0.4506\n",
      "Epoch 1853, CIFAR-10 Batch 1:  loss and accuracy: 1.8662, 0.4516\n",
      "Epoch 1854, CIFAR-10 Batch 1:  loss and accuracy: 1.8609, 0.4570\n",
      "Epoch 1855, CIFAR-10 Batch 1:  loss and accuracy: 1.8762, 0.4564\n",
      "Epoch 1856, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4502\n",
      "Epoch 1857, CIFAR-10 Batch 1:  loss and accuracy: 1.8758, 0.4560\n",
      "Epoch 1858, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4650\n",
      "Epoch 1859, CIFAR-10 Batch 1:  loss and accuracy: 1.8570, 0.4682\n",
      "Epoch 1860, CIFAR-10 Batch 1:  loss and accuracy: 1.8685, 0.4520\n",
      "Epoch 1861, CIFAR-10 Batch 1:  loss and accuracy: 1.8760, 0.4486\n",
      "Epoch 1862, CIFAR-10 Batch 1:  loss and accuracy: 1.8685, 0.4584\n",
      "Epoch 1863, CIFAR-10 Batch 1:  loss and accuracy: 1.8661, 0.4588\n",
      "Epoch 1864, CIFAR-10 Batch 1:  loss and accuracy: 1.8690, 0.4498\n",
      "Epoch 1865, CIFAR-10 Batch 1:  loss and accuracy: 1.8635, 0.4570\n",
      "Epoch 1866, CIFAR-10 Batch 1:  loss and accuracy: 1.8696, 0.4538\n",
      "Epoch 1867, CIFAR-10 Batch 1:  loss and accuracy: 1.8708, 0.4544\n",
      "Epoch 1868, CIFAR-10 Batch 1:  loss and accuracy: 1.8623, 0.4582\n",
      "Epoch 1869, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4634\n",
      "Epoch 1870, CIFAR-10 Batch 1:  loss and accuracy: 1.8748, 0.4546\n",
      "Epoch 1871, CIFAR-10 Batch 1:  loss and accuracy: 1.8641, 0.4554\n",
      "Epoch 1872, CIFAR-10 Batch 1:  loss and accuracy: 1.8701, 0.4594\n",
      "Epoch 1873, CIFAR-10 Batch 1:  loss and accuracy: 1.8689, 0.4494\n",
      "Epoch 1874, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4562\n",
      "Epoch 1875, CIFAR-10 Batch 1:  loss and accuracy: 1.8633, 0.4610\n",
      "Epoch 1876, CIFAR-10 Batch 1:  loss and accuracy: 1.8629, 0.4660\n",
      "Epoch 1877, CIFAR-10 Batch 1:  loss and accuracy: 1.8636, 0.4652\n",
      "Epoch 1878, CIFAR-10 Batch 1:  loss and accuracy: 1.8792, 0.4588\n",
      "Epoch 1879, CIFAR-10 Batch 1:  loss and accuracy: 1.8709, 0.4538\n",
      "Epoch 1880, CIFAR-10 Batch 1:  loss and accuracy: 1.8638, 0.4628\n",
      "Epoch 1881, CIFAR-10 Batch 1:  loss and accuracy: 1.8672, 0.4632\n",
      "Epoch 1882, CIFAR-10 Batch 1:  loss and accuracy: 1.8713, 0.4532\n",
      "Epoch 1883, CIFAR-10 Batch 1:  loss and accuracy: 1.8771, 0.4540\n",
      "Epoch 1884, CIFAR-10 Batch 1:  loss and accuracy: 1.8684, 0.4592\n",
      "Epoch 1885, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4634\n",
      "Epoch 1886, CIFAR-10 Batch 1:  loss and accuracy: 1.8740, 0.4546\n",
      "Epoch 1887, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4552\n",
      "Epoch 1888, CIFAR-10 Batch 1:  loss and accuracy: 1.8622, 0.4590\n",
      "Epoch 1889, CIFAR-10 Batch 1:  loss and accuracy: 1.8627, 0.4550\n",
      "Epoch 1890, CIFAR-10 Batch 1:  loss and accuracy: 1.8682, 0.4542\n",
      "Epoch 1891, CIFAR-10 Batch 1:  loss and accuracy: 1.8739, 0.4512\n",
      "Epoch 1892, CIFAR-10 Batch 1:  loss and accuracy: 1.8639, 0.4576\n",
      "Epoch 1893, CIFAR-10 Batch 1:  loss and accuracy: 1.8761, 0.4586\n",
      "Epoch 1894, CIFAR-10 Batch 1:  loss and accuracy: 1.8714, 0.4544\n",
      "Epoch 1895, CIFAR-10 Batch 1:  loss and accuracy: 1.8773, 0.4506\n",
      "Epoch 1896, CIFAR-10 Batch 1:  loss and accuracy: 1.8702, 0.4664\n",
      "Epoch 1897, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4612\n",
      "Epoch 1898, CIFAR-10 Batch 1:  loss and accuracy: 1.8701, 0.4598\n",
      "Epoch 1899, CIFAR-10 Batch 1:  loss and accuracy: 1.8630, 0.4634\n",
      "Epoch 1900, CIFAR-10 Batch 1:  loss and accuracy: 1.8690, 0.4622\n",
      "Epoch 1901, CIFAR-10 Batch 1:  loss and accuracy: 1.8821, 0.4416\n",
      "Epoch 1902, CIFAR-10 Batch 1:  loss and accuracy: 1.8622, 0.4644\n",
      "Epoch 1903, CIFAR-10 Batch 1:  loss and accuracy: 1.8874, 0.4382\n",
      "Epoch 1904, CIFAR-10 Batch 1:  loss and accuracy: 1.8780, 0.4460\n",
      "Epoch 1905, CIFAR-10 Batch 1:  loss and accuracy: 1.8699, 0.4626\n",
      "Epoch 1906, CIFAR-10 Batch 1:  loss and accuracy: 1.8748, 0.4492\n",
      "Epoch 1907, CIFAR-10 Batch 1:  loss and accuracy: 1.8652, 0.4590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1908, CIFAR-10 Batch 1:  loss and accuracy: 1.8786, 0.4492\n",
      "Epoch 1909, CIFAR-10 Batch 1:  loss and accuracy: 1.8805, 0.4450\n",
      "Epoch 1910, CIFAR-10 Batch 1:  loss and accuracy: 1.8753, 0.4480\n",
      "Epoch 1911, CIFAR-10 Batch 1:  loss and accuracy: 1.8704, 0.4528\n",
      "Epoch 1912, CIFAR-10 Batch 1:  loss and accuracy: 1.8794, 0.4454\n",
      "Epoch 1913, CIFAR-10 Batch 1:  loss and accuracy: 1.8595, 0.4562\n",
      "Epoch 1914, CIFAR-10 Batch 1:  loss and accuracy: 1.8782, 0.4452\n",
      "Epoch 1915, CIFAR-10 Batch 1:  loss and accuracy: 1.8631, 0.4658\n",
      "Epoch 1916, CIFAR-10 Batch 1:  loss and accuracy: 1.8606, 0.4638\n",
      "Epoch 1917, CIFAR-10 Batch 1:  loss and accuracy: 1.8722, 0.4580\n",
      "Epoch 1918, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4620\n",
      "Epoch 1919, CIFAR-10 Batch 1:  loss and accuracy: 1.8779, 0.4550\n",
      "Epoch 1920, CIFAR-10 Batch 1:  loss and accuracy: 1.8717, 0.4572\n",
      "Epoch 1921, CIFAR-10 Batch 1:  loss and accuracy: 1.8620, 0.4658\n",
      "Epoch 1922, CIFAR-10 Batch 1:  loss and accuracy: 1.8554, 0.4662\n",
      "Epoch 1923, CIFAR-10 Batch 1:  loss and accuracy: 1.8670, 0.4624\n",
      "Epoch 1924, CIFAR-10 Batch 1:  loss and accuracy: 1.8615, 0.4644\n",
      "Epoch 1925, CIFAR-10 Batch 1:  loss and accuracy: 1.8636, 0.4608\n",
      "Epoch 1926, CIFAR-10 Batch 1:  loss and accuracy: 1.8662, 0.4572\n",
      "Epoch 1927, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4600\n",
      "Epoch 1928, CIFAR-10 Batch 1:  loss and accuracy: 1.8557, 0.4696\n",
      "Epoch 1929, CIFAR-10 Batch 1:  loss and accuracy: 1.8813, 0.4446\n",
      "Epoch 1930, CIFAR-10 Batch 1:  loss and accuracy: 1.8873, 0.4490\n",
      "Epoch 1931, CIFAR-10 Batch 1:  loss and accuracy: 1.8751, 0.4494\n",
      "Epoch 1932, CIFAR-10 Batch 1:  loss and accuracy: 1.8617, 0.4644\n",
      "Epoch 1933, CIFAR-10 Batch 1:  loss and accuracy: 1.8779, 0.4464\n",
      "Epoch 1934, CIFAR-10 Batch 1:  loss and accuracy: 1.8737, 0.4582\n",
      "Epoch 1935, CIFAR-10 Batch 1:  loss and accuracy: 1.8823, 0.4478\n",
      "Epoch 1936, CIFAR-10 Batch 1:  loss and accuracy: 1.8629, 0.4604\n",
      "Epoch 1937, CIFAR-10 Batch 1:  loss and accuracy: 1.8721, 0.4518\n",
      "Epoch 1938, CIFAR-10 Batch 1:  loss and accuracy: 1.8657, 0.4624\n",
      "Epoch 1939, CIFAR-10 Batch 1:  loss and accuracy: 1.8747, 0.4526\n",
      "Epoch 1940, CIFAR-10 Batch 1:  loss and accuracy: 1.8594, 0.4674\n",
      "Epoch 1941, CIFAR-10 Batch 1:  loss and accuracy: 1.8664, 0.4628\n",
      "Epoch 1942, CIFAR-10 Batch 1:  loss and accuracy: 1.8666, 0.4578\n",
      "Epoch 1943, CIFAR-10 Batch 1:  loss and accuracy: 1.8784, 0.4470\n",
      "Epoch 1944, CIFAR-10 Batch 1:  loss and accuracy: 1.8677, 0.4518\n",
      "Epoch 1945, CIFAR-10 Batch 1:  loss and accuracy: 1.8499, 0.4632\n",
      "Epoch 1946, CIFAR-10 Batch 1:  loss and accuracy: 1.8705, 0.4544\n",
      "Epoch 1947, CIFAR-10 Batch 1:  loss and accuracy: 1.8654, 0.4578\n",
      "Epoch 1948, CIFAR-10 Batch 1:  loss and accuracy: 1.8670, 0.4536\n",
      "Epoch 1949, CIFAR-10 Batch 1:  loss and accuracy: 1.8570, 0.4644\n",
      "Epoch 1950, CIFAR-10 Batch 1:  loss and accuracy: 1.8716, 0.4456\n",
      "Epoch 1951, CIFAR-10 Batch 1:  loss and accuracy: 1.8695, 0.4556\n",
      "Epoch 1952, CIFAR-10 Batch 1:  loss and accuracy: 1.8647, 0.4610\n",
      "Epoch 1953, CIFAR-10 Batch 1:  loss and accuracy: 1.8621, 0.4698\n",
      "Epoch 1954, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4642\n",
      "Epoch 1955, CIFAR-10 Batch 1:  loss and accuracy: 1.8810, 0.4460\n",
      "Epoch 1956, CIFAR-10 Batch 1:  loss and accuracy: 1.8853, 0.4336\n",
      "Epoch 1957, CIFAR-10 Batch 1:  loss and accuracy: 1.8774, 0.4600\n",
      "Epoch 1958, CIFAR-10 Batch 1:  loss and accuracy: 1.8812, 0.4552\n",
      "Epoch 1959, CIFAR-10 Batch 1:  loss and accuracy: 1.8787, 0.4572\n",
      "Epoch 1960, CIFAR-10 Batch 1:  loss and accuracy: 1.8812, 0.4536\n",
      "Epoch 1961, CIFAR-10 Batch 1:  loss and accuracy: 1.8771, 0.4516\n",
      "Epoch 1962, CIFAR-10 Batch 1:  loss and accuracy: 1.8574, 0.4654\n",
      "Epoch 1963, CIFAR-10 Batch 1:  loss and accuracy: 1.8558, 0.4750\n",
      "Epoch 1964, CIFAR-10 Batch 1:  loss and accuracy: 1.8716, 0.4534\n",
      "Epoch 1965, CIFAR-10 Batch 1:  loss and accuracy: 1.8658, 0.4626\n",
      "Epoch 1966, CIFAR-10 Batch 1:  loss and accuracy: 1.8720, 0.4620\n",
      "Epoch 1967, CIFAR-10 Batch 1:  loss and accuracy: 1.8616, 0.4658\n",
      "Epoch 1968, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4600\n",
      "Epoch 1969, CIFAR-10 Batch 1:  loss and accuracy: 1.8680, 0.4658\n",
      "Epoch 1970, CIFAR-10 Batch 1:  loss and accuracy: 1.8816, 0.4518\n",
      "Epoch 1971, CIFAR-10 Batch 1:  loss and accuracy: 1.8734, 0.4562\n",
      "Epoch 1972, CIFAR-10 Batch 1:  loss and accuracy: 1.8678, 0.4612\n",
      "Epoch 1973, CIFAR-10 Batch 1:  loss and accuracy: 1.8696, 0.4592\n",
      "Epoch 1974, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4628\n",
      "Epoch 1975, CIFAR-10 Batch 1:  loss and accuracy: 1.8611, 0.4738\n",
      "Epoch 1976, CIFAR-10 Batch 1:  loss and accuracy: 1.8665, 0.4568\n",
      "Epoch 1977, CIFAR-10 Batch 1:  loss and accuracy: 1.8604, 0.4554\n",
      "Epoch 1978, CIFAR-10 Batch 1:  loss and accuracy: 1.8927, 0.4402\n",
      "Epoch 1979, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4656\n",
      "Epoch 1980, CIFAR-10 Batch 1:  loss and accuracy: 1.8768, 0.4612\n",
      "Epoch 1981, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4690\n",
      "Epoch 1982, CIFAR-10 Batch 1:  loss and accuracy: 1.8714, 0.4592\n",
      "Epoch 1983, CIFAR-10 Batch 1:  loss and accuracy: 1.8642, 0.4624\n",
      "Epoch 1984, CIFAR-10 Batch 1:  loss and accuracy: 1.8789, 0.4466\n",
      "Epoch 1985, CIFAR-10 Batch 1:  loss and accuracy: 1.8785, 0.4472\n",
      "Epoch 1986, CIFAR-10 Batch 1:  loss and accuracy: 1.8719, 0.4526\n",
      "Epoch 1987, CIFAR-10 Batch 1:  loss and accuracy: 1.8643, 0.4654\n",
      "Epoch 1988, CIFAR-10 Batch 1:  loss and accuracy: 1.8735, 0.4532\n",
      "Epoch 1989, CIFAR-10 Batch 1:  loss and accuracy: 1.8761, 0.4568\n",
      "Epoch 1990, CIFAR-10 Batch 1:  loss and accuracy: 1.8632, 0.4646\n",
      "Epoch 1991, CIFAR-10 Batch 1:  loss and accuracy: 1.8687, 0.4532\n",
      "Epoch 1992, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4672\n",
      "Epoch 1993, CIFAR-10 Batch 1:  loss and accuracy: 1.8747, 0.4504\n",
      "Epoch 1994, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4546\n",
      "Epoch 1995, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4626\n",
      "Epoch 1996, CIFAR-10 Batch 1:  loss and accuracy: 1.8606, 0.4676\n",
      "Epoch 1997, CIFAR-10 Batch 1:  loss and accuracy: 1.8578, 0.4670\n",
      "Epoch 1998, CIFAR-10 Batch 1:  loss and accuracy: 1.8625, 0.4518\n",
      "Epoch 1999, CIFAR-10 Batch 1:  loss and accuracy: 1.8553, 0.4676\n",
      "Epoch 2000, CIFAR-10 Batch 1:  loss and accuracy: 1.8750, 0.4532\n",
      "Epoch 2001, CIFAR-10 Batch 1:  loss and accuracy: 1.8748, 0.4596\n",
      "Epoch 2002, CIFAR-10 Batch 1:  loss and accuracy: 1.8659, 0.4598\n",
      "Epoch 2003, CIFAR-10 Batch 1:  loss and accuracy: 1.8815, 0.4434\n",
      "Epoch 2004, CIFAR-10 Batch 1:  loss and accuracy: 1.8783, 0.4568\n",
      "Epoch 2005, CIFAR-10 Batch 1:  loss and accuracy: 1.8643, 0.4674\n",
      "Epoch 2006, CIFAR-10 Batch 1:  loss and accuracy: 1.8654, 0.4626\n",
      "Epoch 2007, CIFAR-10 Batch 1:  loss and accuracy: 1.8659, 0.4662\n",
      "Epoch 2008, CIFAR-10 Batch 1:  loss and accuracy: 1.8688, 0.4550\n",
      "Epoch 2009, CIFAR-10 Batch 1:  loss and accuracy: 1.8676, 0.4572\n",
      "Epoch 2010, CIFAR-10 Batch 1:  loss and accuracy: 1.8628, 0.4600\n",
      "Epoch 2011, CIFAR-10 Batch 1:  loss and accuracy: 1.8556, 0.4672\n",
      "Epoch 2012, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4646\n",
      "Epoch 2013, CIFAR-10 Batch 1:  loss and accuracy: 1.8677, 0.4562\n",
      "Epoch 2014, CIFAR-10 Batch 1:  loss and accuracy: 1.8664, 0.4570\n",
      "Epoch 2015, CIFAR-10 Batch 1:  loss and accuracy: 1.8574, 0.4752\n",
      "Epoch 2016, CIFAR-10 Batch 1:  loss and accuracy: 1.8668, 0.4630\n",
      "Epoch 2017, CIFAR-10 Batch 1:  loss and accuracy: 1.8598, 0.4598\n",
      "Epoch 2018, CIFAR-10 Batch 1:  loss and accuracy: 1.8747, 0.4466\n",
      "Epoch 2019, CIFAR-10 Batch 1:  loss and accuracy: 1.8683, 0.4580\n",
      "Epoch 2020, CIFAR-10 Batch 1:  loss and accuracy: 1.8665, 0.4570\n",
      "Epoch 2021, CIFAR-10 Batch 1:  loss and accuracy: 1.8629, 0.4616\n",
      "Epoch 2022, CIFAR-10 Batch 1:  loss and accuracy: 1.8600, 0.4634\n",
      "Epoch 2023, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4656\n",
      "Epoch 2024, CIFAR-10 Batch 1:  loss and accuracy: 1.8538, 0.4728\n",
      "Epoch 2025, CIFAR-10 Batch 1:  loss and accuracy: 1.8697, 0.4654\n",
      "Epoch 2026, CIFAR-10 Batch 1:  loss and accuracy: 1.8696, 0.4612\n",
      "Epoch 2027, CIFAR-10 Batch 1:  loss and accuracy: 1.8538, 0.4706\n",
      "Epoch 2028, CIFAR-10 Batch 1:  loss and accuracy: 1.8623, 0.4620\n",
      "Epoch 2029, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4624\n",
      "Epoch 2030, CIFAR-10 Batch 1:  loss and accuracy: 1.8683, 0.4610\n",
      "Epoch 2031, CIFAR-10 Batch 1:  loss and accuracy: 1.8684, 0.4592\n",
      "Epoch 2032, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4654\n",
      "Epoch 2033, CIFAR-10 Batch 1:  loss and accuracy: 1.8552, 0.4770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2034, CIFAR-10 Batch 1:  loss and accuracy: 1.8664, 0.4628\n",
      "Epoch 2035, CIFAR-10 Batch 1:  loss and accuracy: 1.8657, 0.4570\n",
      "Epoch 2036, CIFAR-10 Batch 1:  loss and accuracy: 1.8582, 0.4702\n",
      "Epoch 2037, CIFAR-10 Batch 1:  loss and accuracy: 1.8710, 0.4578\n",
      "Epoch 2038, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4628\n",
      "Epoch 2039, CIFAR-10 Batch 1:  loss and accuracy: 1.8513, 0.4720\n",
      "Epoch 2040, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4720\n",
      "Epoch 2041, CIFAR-10 Batch 1:  loss and accuracy: 1.8648, 0.4656\n",
      "Epoch 2042, CIFAR-10 Batch 1:  loss and accuracy: 1.8598, 0.4638\n",
      "Epoch 2043, CIFAR-10 Batch 1:  loss and accuracy: 1.8643, 0.4622\n",
      "Epoch 2044, CIFAR-10 Batch 1:  loss and accuracy: 1.8577, 0.4700\n",
      "Epoch 2045, CIFAR-10 Batch 1:  loss and accuracy: 1.8706, 0.4582\n",
      "Epoch 2046, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4724\n",
      "Epoch 2047, CIFAR-10 Batch 1:  loss and accuracy: 1.8660, 0.4636\n",
      "Epoch 2048, CIFAR-10 Batch 1:  loss and accuracy: 1.8688, 0.4554\n",
      "Epoch 2049, CIFAR-10 Batch 1:  loss and accuracy: 1.8572, 0.4700\n",
      "Epoch 2050, CIFAR-10 Batch 1:  loss and accuracy: 1.8576, 0.4646\n",
      "Epoch 2051, CIFAR-10 Batch 1:  loss and accuracy: 1.8582, 0.4670\n",
      "Epoch 2052, CIFAR-10 Batch 1:  loss and accuracy: 1.8649, 0.4670\n",
      "Epoch 2053, CIFAR-10 Batch 1:  loss and accuracy: 1.8625, 0.4760\n",
      "Epoch 2054, CIFAR-10 Batch 1:  loss and accuracy: 1.8676, 0.4644\n",
      "Epoch 2055, CIFAR-10 Batch 1:  loss and accuracy: 1.8668, 0.4626\n",
      "Epoch 2056, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4604\n",
      "Epoch 2057, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4616\n",
      "Epoch 2058, CIFAR-10 Batch 1:  loss and accuracy: 1.8695, 0.4666\n",
      "Epoch 2059, CIFAR-10 Batch 1:  loss and accuracy: 1.8543, 0.4750\n",
      "Epoch 2060, CIFAR-10 Batch 1:  loss and accuracy: 1.8638, 0.4630\n",
      "Epoch 2061, CIFAR-10 Batch 1:  loss and accuracy: 1.8684, 0.4648\n",
      "Epoch 2062, CIFAR-10 Batch 1:  loss and accuracy: 1.8554, 0.4676\n",
      "Epoch 2063, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4624\n",
      "Epoch 2064, CIFAR-10 Batch 1:  loss and accuracy: 1.8697, 0.4528\n",
      "Epoch 2065, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4664\n",
      "Epoch 2066, CIFAR-10 Batch 1:  loss and accuracy: 1.8701, 0.4528\n",
      "Epoch 2067, CIFAR-10 Batch 1:  loss and accuracy: 1.8635, 0.4650\n",
      "Epoch 2068, CIFAR-10 Batch 1:  loss and accuracy: 1.8767, 0.4452\n",
      "Epoch 2069, CIFAR-10 Batch 1:  loss and accuracy: 1.8620, 0.4648\n",
      "Epoch 2070, CIFAR-10 Batch 1:  loss and accuracy: 1.8558, 0.4706\n",
      "Epoch 2071, CIFAR-10 Batch 1:  loss and accuracy: 1.8783, 0.4466\n",
      "Epoch 2072, CIFAR-10 Batch 1:  loss and accuracy: 1.8612, 0.4644\n",
      "Epoch 2073, CIFAR-10 Batch 1:  loss and accuracy: 1.8785, 0.4542\n",
      "Epoch 2074, CIFAR-10 Batch 1:  loss and accuracy: 1.8724, 0.4506\n",
      "Epoch 2075, CIFAR-10 Batch 1:  loss and accuracy: 1.8614, 0.4634\n",
      "Epoch 2076, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4640\n",
      "Epoch 2077, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4600\n",
      "Epoch 2078, CIFAR-10 Batch 1:  loss and accuracy: 1.8559, 0.4762\n",
      "Epoch 2079, CIFAR-10 Batch 1:  loss and accuracy: 1.8561, 0.4738\n",
      "Epoch 2080, CIFAR-10 Batch 1:  loss and accuracy: 1.8550, 0.4772\n",
      "Epoch 2081, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4646\n",
      "Epoch 2082, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4584\n",
      "Epoch 2083, CIFAR-10 Batch 1:  loss and accuracy: 1.8552, 0.4732\n",
      "Epoch 2084, CIFAR-10 Batch 1:  loss and accuracy: 1.8778, 0.4494\n",
      "Epoch 2085, CIFAR-10 Batch 1:  loss and accuracy: 1.8611, 0.4700\n",
      "Epoch 2086, CIFAR-10 Batch 1:  loss and accuracy: 1.8537, 0.4724\n",
      "Epoch 2087, CIFAR-10 Batch 1:  loss and accuracy: 1.8671, 0.4572\n",
      "Epoch 2088, CIFAR-10 Batch 1:  loss and accuracy: 1.8564, 0.4642\n",
      "Epoch 2089, CIFAR-10 Batch 1:  loss and accuracy: 1.8586, 0.4686\n",
      "Epoch 2090, CIFAR-10 Batch 1:  loss and accuracy: 1.8529, 0.4728\n",
      "Epoch 2091, CIFAR-10 Batch 1:  loss and accuracy: 1.8653, 0.4626\n",
      "Epoch 2092, CIFAR-10 Batch 1:  loss and accuracy: 1.8628, 0.4632\n",
      "Epoch 2093, CIFAR-10 Batch 1:  loss and accuracy: 1.8599, 0.4698\n",
      "Epoch 2094, CIFAR-10 Batch 1:  loss and accuracy: 1.8704, 0.4548\n",
      "Epoch 2095, CIFAR-10 Batch 1:  loss and accuracy: 1.8677, 0.4552\n",
      "Epoch 2096, CIFAR-10 Batch 1:  loss and accuracy: 1.8499, 0.4700\n",
      "Epoch 2097, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4698\n",
      "Epoch 2098, CIFAR-10 Batch 1:  loss and accuracy: 1.8616, 0.4574\n",
      "Epoch 2099, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4572\n",
      "Epoch 2100, CIFAR-10 Batch 1:  loss and accuracy: 1.8574, 0.4658\n",
      "Epoch 2101, CIFAR-10 Batch 1:  loss and accuracy: 1.8632, 0.4644\n",
      "Epoch 2102, CIFAR-10 Batch 1:  loss and accuracy: 1.8573, 0.4702\n",
      "Epoch 2103, CIFAR-10 Batch 1:  loss and accuracy: 1.8691, 0.4644\n",
      "Epoch 2104, CIFAR-10 Batch 1:  loss and accuracy: 1.8553, 0.4700\n",
      "Epoch 2105, CIFAR-10 Batch 1:  loss and accuracy: 1.8768, 0.4534\n",
      "Epoch 2106, CIFAR-10 Batch 1:  loss and accuracy: 1.8727, 0.4630\n",
      "Epoch 2107, CIFAR-10 Batch 1:  loss and accuracy: 1.8487, 0.4748\n",
      "Epoch 2108, CIFAR-10 Batch 1:  loss and accuracy: 1.8613, 0.4726\n",
      "Epoch 2109, CIFAR-10 Batch 1:  loss and accuracy: 1.8595, 0.4700\n",
      "Epoch 2110, CIFAR-10 Batch 1:  loss and accuracy: 1.8485, 0.4752\n",
      "Epoch 2111, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4540\n",
      "Epoch 2112, CIFAR-10 Batch 1:  loss and accuracy: 1.8648, 0.4568\n",
      "Epoch 2113, CIFAR-10 Batch 1:  loss and accuracy: 1.8569, 0.4652\n",
      "Epoch 2114, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4642\n",
      "Epoch 2115, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4668\n",
      "Epoch 2116, CIFAR-10 Batch 1:  loss and accuracy: 1.8577, 0.4680\n",
      "Epoch 2117, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4592\n",
      "Epoch 2118, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4610\n",
      "Epoch 2119, CIFAR-10 Batch 1:  loss and accuracy: 1.8643, 0.4670\n",
      "Epoch 2120, CIFAR-10 Batch 1:  loss and accuracy: 1.8566, 0.4682\n",
      "Epoch 2121, CIFAR-10 Batch 1:  loss and accuracy: 1.8555, 0.4714\n",
      "Epoch 2122, CIFAR-10 Batch 1:  loss and accuracy: 1.8619, 0.4626\n",
      "Epoch 2123, CIFAR-10 Batch 1:  loss and accuracy: 1.8619, 0.4634\n",
      "Epoch 2124, CIFAR-10 Batch 1:  loss and accuracy: 1.8439, 0.4744\n",
      "Epoch 2125, CIFAR-10 Batch 1:  loss and accuracy: 1.8489, 0.4728\n",
      "Epoch 2126, CIFAR-10 Batch 1:  loss and accuracy: 1.8527, 0.4746\n",
      "Epoch 2127, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4734\n",
      "Epoch 2128, CIFAR-10 Batch 1:  loss and accuracy: 1.8683, 0.4654\n",
      "Epoch 2129, CIFAR-10 Batch 1:  loss and accuracy: 1.8654, 0.4644\n",
      "Epoch 2130, CIFAR-10 Batch 1:  loss and accuracy: 1.8614, 0.4620\n",
      "Epoch 2131, CIFAR-10 Batch 1:  loss and accuracy: 1.8598, 0.4680\n",
      "Epoch 2132, CIFAR-10 Batch 1:  loss and accuracy: 1.8593, 0.4638\n",
      "Epoch 2133, CIFAR-10 Batch 1:  loss and accuracy: 1.8478, 0.4718\n",
      "Epoch 2134, CIFAR-10 Batch 1:  loss and accuracy: 1.8606, 0.4590\n",
      "Epoch 2135, CIFAR-10 Batch 1:  loss and accuracy: 1.8795, 0.4584\n",
      "Epoch 2136, CIFAR-10 Batch 1:  loss and accuracy: 1.8400, 0.4856\n",
      "Epoch 2137, CIFAR-10 Batch 1:  loss and accuracy: 1.8624, 0.4660\n",
      "Epoch 2138, CIFAR-10 Batch 1:  loss and accuracy: 1.8751, 0.4490\n",
      "Epoch 2139, CIFAR-10 Batch 1:  loss and accuracy: 1.8631, 0.4628\n",
      "Epoch 2140, CIFAR-10 Batch 1:  loss and accuracy: 1.8660, 0.4654\n",
      "Epoch 2141, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4732\n",
      "Epoch 2142, CIFAR-10 Batch 1:  loss and accuracy: 1.8623, 0.4676\n",
      "Epoch 2143, CIFAR-10 Batch 1:  loss and accuracy: 1.8519, 0.4742\n",
      "Epoch 2144, CIFAR-10 Batch 1:  loss and accuracy: 1.8557, 0.4702\n",
      "Epoch 2145, CIFAR-10 Batch 1:  loss and accuracy: 1.8552, 0.4680\n",
      "Epoch 2146, CIFAR-10 Batch 1:  loss and accuracy: 1.8649, 0.4674\n",
      "Epoch 2147, CIFAR-10 Batch 1:  loss and accuracy: 1.8548, 0.4618\n",
      "Epoch 2148, CIFAR-10 Batch 1:  loss and accuracy: 1.8524, 0.4684\n",
      "Epoch 2149, CIFAR-10 Batch 1:  loss and accuracy: 1.8671, 0.4600\n",
      "Epoch 2150, CIFAR-10 Batch 1:  loss and accuracy: 1.8694, 0.4614\n",
      "Epoch 2151, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4588\n",
      "Epoch 2152, CIFAR-10 Batch 1:  loss and accuracy: 1.8609, 0.4696\n",
      "Epoch 2153, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4756\n",
      "Epoch 2154, CIFAR-10 Batch 1:  loss and accuracy: 1.8603, 0.4686\n",
      "Epoch 2155, CIFAR-10 Batch 1:  loss and accuracy: 1.8572, 0.4724\n",
      "Epoch 2156, CIFAR-10 Batch 1:  loss and accuracy: 1.8462, 0.4838\n",
      "Epoch 2157, CIFAR-10 Batch 1:  loss and accuracy: 1.8619, 0.4730\n",
      "Epoch 2158, CIFAR-10 Batch 1:  loss and accuracy: 1.8576, 0.4736\n",
      "Epoch 2159, CIFAR-10 Batch 1:  loss and accuracy: 1.8539, 0.4796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2160, CIFAR-10 Batch 1:  loss and accuracy: 1.8621, 0.4612\n",
      "Epoch 2161, CIFAR-10 Batch 1:  loss and accuracy: 1.8622, 0.4652\n",
      "Epoch 2162, CIFAR-10 Batch 1:  loss and accuracy: 1.8566, 0.4668\n",
      "Epoch 2163, CIFAR-10 Batch 1:  loss and accuracy: 1.8531, 0.4756\n",
      "Epoch 2164, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4726\n",
      "Epoch 2165, CIFAR-10 Batch 1:  loss and accuracy: 1.8627, 0.4626\n",
      "Epoch 2166, CIFAR-10 Batch 1:  loss and accuracy: 1.8569, 0.4610\n",
      "Epoch 2167, CIFAR-10 Batch 1:  loss and accuracy: 1.8544, 0.4736\n",
      "Epoch 2168, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4610\n",
      "Epoch 2169, CIFAR-10 Batch 1:  loss and accuracy: 1.8689, 0.4632\n",
      "Epoch 2170, CIFAR-10 Batch 1:  loss and accuracy: 1.8580, 0.4792\n",
      "Epoch 2171, CIFAR-10 Batch 1:  loss and accuracy: 1.8631, 0.4600\n",
      "Epoch 2172, CIFAR-10 Batch 1:  loss and accuracy: 1.8635, 0.4588\n",
      "Epoch 2173, CIFAR-10 Batch 1:  loss and accuracy: 1.8697, 0.4652\n",
      "Epoch 2174, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4756\n",
      "Epoch 2175, CIFAR-10 Batch 1:  loss and accuracy: 1.8518, 0.4808\n",
      "Epoch 2176, CIFAR-10 Batch 1:  loss and accuracy: 1.8514, 0.4758\n",
      "Epoch 2177, CIFAR-10 Batch 1:  loss and accuracy: 1.8573, 0.4750\n",
      "Epoch 2178, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4738\n",
      "Epoch 2179, CIFAR-10 Batch 1:  loss and accuracy: 1.8513, 0.4750\n",
      "Epoch 2180, CIFAR-10 Batch 1:  loss and accuracy: 1.8550, 0.4742\n",
      "Epoch 2181, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4642\n",
      "Epoch 2182, CIFAR-10 Batch 1:  loss and accuracy: 1.8540, 0.4774\n",
      "Epoch 2183, CIFAR-10 Batch 1:  loss and accuracy: 1.8562, 0.4798\n",
      "Epoch 2184, CIFAR-10 Batch 1:  loss and accuracy: 1.8582, 0.4754\n",
      "Epoch 2185, CIFAR-10 Batch 1:  loss and accuracy: 1.8679, 0.4662\n",
      "Epoch 2186, CIFAR-10 Batch 1:  loss and accuracy: 1.8613, 0.4634\n",
      "Epoch 2187, CIFAR-10 Batch 1:  loss and accuracy: 1.8593, 0.4714\n",
      "Epoch 2188, CIFAR-10 Batch 1:  loss and accuracy: 1.8701, 0.4616\n",
      "Epoch 2189, CIFAR-10 Batch 1:  loss and accuracy: 1.8654, 0.4596\n",
      "Epoch 2190, CIFAR-10 Batch 1:  loss and accuracy: 1.8595, 0.4716\n",
      "Epoch 2191, CIFAR-10 Batch 1:  loss and accuracy: 1.8497, 0.4798\n",
      "Epoch 2192, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4732\n",
      "Epoch 2193, CIFAR-10 Batch 1:  loss and accuracy: 1.8657, 0.4626\n",
      "Epoch 2194, CIFAR-10 Batch 1:  loss and accuracy: 1.8620, 0.4672\n",
      "Epoch 2195, CIFAR-10 Batch 1:  loss and accuracy: 1.8534, 0.4724\n",
      "Epoch 2196, CIFAR-10 Batch 1:  loss and accuracy: 1.8664, 0.4672\n",
      "Epoch 2197, CIFAR-10 Batch 1:  loss and accuracy: 1.8667, 0.4680\n",
      "Epoch 2198, CIFAR-10 Batch 1:  loss and accuracy: 1.8579, 0.4696\n",
      "Epoch 2199, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4698\n",
      "Epoch 2200, CIFAR-10 Batch 1:  loss and accuracy: 1.8660, 0.4536\n",
      "Epoch 2201, CIFAR-10 Batch 1:  loss and accuracy: 1.8711, 0.4572\n",
      "Epoch 2202, CIFAR-10 Batch 1:  loss and accuracy: 1.8566, 0.4694\n",
      "Epoch 2203, CIFAR-10 Batch 1:  loss and accuracy: 1.8638, 0.4600\n",
      "Epoch 2204, CIFAR-10 Batch 1:  loss and accuracy: 1.8574, 0.4682\n",
      "Epoch 2205, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4664\n",
      "Epoch 2206, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4770\n",
      "Epoch 2207, CIFAR-10 Batch 1:  loss and accuracy: 1.8665, 0.4602\n",
      "Epoch 2208, CIFAR-10 Batch 1:  loss and accuracy: 1.8598, 0.4622\n",
      "Epoch 2209, CIFAR-10 Batch 1:  loss and accuracy: 1.8596, 0.4760\n",
      "Epoch 2210, CIFAR-10 Batch 1:  loss and accuracy: 1.8572, 0.4732\n",
      "Epoch 2211, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4782\n",
      "Epoch 2212, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4776\n",
      "Epoch 2213, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4696\n",
      "Epoch 2214, CIFAR-10 Batch 1:  loss and accuracy: 1.8545, 0.4792\n",
      "Epoch 2215, CIFAR-10 Batch 1:  loss and accuracy: 1.8683, 0.4628\n",
      "Epoch 2216, CIFAR-10 Batch 1:  loss and accuracy: 1.8456, 0.4712\n",
      "Epoch 2217, CIFAR-10 Batch 1:  loss and accuracy: 1.8499, 0.4718\n",
      "Epoch 2218, CIFAR-10 Batch 1:  loss and accuracy: 1.8493, 0.4706\n",
      "Epoch 2219, CIFAR-10 Batch 1:  loss and accuracy: 1.8500, 0.4736\n",
      "Epoch 2220, CIFAR-10 Batch 1:  loss and accuracy: 1.8567, 0.4686\n",
      "Epoch 2221, CIFAR-10 Batch 1:  loss and accuracy: 1.8528, 0.4698\n",
      "Epoch 2222, CIFAR-10 Batch 1:  loss and accuracy: 1.8589, 0.4674\n",
      "Epoch 2223, CIFAR-10 Batch 1:  loss and accuracy: 1.8513, 0.4714\n",
      "Epoch 2224, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4754\n",
      "Epoch 2225, CIFAR-10 Batch 1:  loss and accuracy: 1.8658, 0.4622\n",
      "Epoch 2226, CIFAR-10 Batch 1:  loss and accuracy: 1.8555, 0.4708\n",
      "Epoch 2227, CIFAR-10 Batch 1:  loss and accuracy: 1.8685, 0.4510\n",
      "Epoch 2228, CIFAR-10 Batch 1:  loss and accuracy: 1.8565, 0.4612\n",
      "Epoch 2229, CIFAR-10 Batch 1:  loss and accuracy: 1.8550, 0.4760\n",
      "Epoch 2230, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4662\n",
      "Epoch 2231, CIFAR-10 Batch 1:  loss and accuracy: 1.8582, 0.4634\n",
      "Epoch 2232, CIFAR-10 Batch 1:  loss and accuracy: 1.8564, 0.4660\n",
      "Epoch 2233, CIFAR-10 Batch 1:  loss and accuracy: 1.8667, 0.4544\n",
      "Epoch 2234, CIFAR-10 Batch 1:  loss and accuracy: 1.8522, 0.4778\n",
      "Epoch 2235, CIFAR-10 Batch 1:  loss and accuracy: 1.8668, 0.4542\n",
      "Epoch 2236, CIFAR-10 Batch 1:  loss and accuracy: 1.8573, 0.4674\n",
      "Epoch 2237, CIFAR-10 Batch 1:  loss and accuracy: 1.8658, 0.4546\n",
      "Epoch 2238, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4630\n",
      "Epoch 2239, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4594\n",
      "Epoch 2240, CIFAR-10 Batch 1:  loss and accuracy: 1.8423, 0.4762\n",
      "Epoch 2241, CIFAR-10 Batch 1:  loss and accuracy: 1.8615, 0.4612\n",
      "Epoch 2242, CIFAR-10 Batch 1:  loss and accuracy: 1.8634, 0.4622\n",
      "Epoch 2243, CIFAR-10 Batch 1:  loss and accuracy: 1.8568, 0.4680\n",
      "Epoch 2244, CIFAR-10 Batch 1:  loss and accuracy: 1.8531, 0.4716\n",
      "Epoch 2245, CIFAR-10 Batch 1:  loss and accuracy: 1.8580, 0.4608\n",
      "Epoch 2246, CIFAR-10 Batch 1:  loss and accuracy: 1.8543, 0.4658\n",
      "Epoch 2247, CIFAR-10 Batch 1:  loss and accuracy: 1.8471, 0.4796\n",
      "Epoch 2248, CIFAR-10 Batch 1:  loss and accuracy: 1.8599, 0.4688\n",
      "Epoch 2249, CIFAR-10 Batch 1:  loss and accuracy: 1.8498, 0.4684\n",
      "Epoch 2250, CIFAR-10 Batch 1:  loss and accuracy: 1.8550, 0.4610\n",
      "Epoch 2251, CIFAR-10 Batch 1:  loss and accuracy: 1.8544, 0.4662\n",
      "Epoch 2252, CIFAR-10 Batch 1:  loss and accuracy: 1.8502, 0.4704\n",
      "Epoch 2253, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4708\n",
      "Epoch 2254, CIFAR-10 Batch 1:  loss and accuracy: 1.8529, 0.4746\n",
      "Epoch 2255, CIFAR-10 Batch 1:  loss and accuracy: 1.8638, 0.4520\n",
      "Epoch 2256, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4662\n",
      "Epoch 2257, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4676\n",
      "Epoch 2258, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4724\n",
      "Epoch 2259, CIFAR-10 Batch 1:  loss and accuracy: 1.8588, 0.4646\n",
      "Epoch 2260, CIFAR-10 Batch 1:  loss and accuracy: 1.8458, 0.4746\n",
      "Epoch 2261, CIFAR-10 Batch 1:  loss and accuracy: 1.8513, 0.4644\n",
      "Epoch 2262, CIFAR-10 Batch 1:  loss and accuracy: 1.8532, 0.4678\n",
      "Epoch 2263, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4598\n",
      "Epoch 2264, CIFAR-10 Batch 1:  loss and accuracy: 1.8559, 0.4662\n",
      "Epoch 2265, CIFAR-10 Batch 1:  loss and accuracy: 1.8540, 0.4688\n",
      "Epoch 2266, CIFAR-10 Batch 1:  loss and accuracy: 1.8580, 0.4708\n",
      "Epoch 2267, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4714\n",
      "Epoch 2268, CIFAR-10 Batch 1:  loss and accuracy: 1.8597, 0.4704\n",
      "Epoch 2269, CIFAR-10 Batch 1:  loss and accuracy: 1.8631, 0.4668\n",
      "Epoch 2270, CIFAR-10 Batch 1:  loss and accuracy: 1.8580, 0.4720\n",
      "Epoch 2271, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4760\n",
      "Epoch 2272, CIFAR-10 Batch 1:  loss and accuracy: 1.8570, 0.4664\n",
      "Epoch 2273, CIFAR-10 Batch 1:  loss and accuracy: 1.8746, 0.4516\n",
      "Epoch 2274, CIFAR-10 Batch 1:  loss and accuracy: 1.8584, 0.4658\n",
      "Epoch 2275, CIFAR-10 Batch 1:  loss and accuracy: 1.8520, 0.4770\n",
      "Epoch 2276, CIFAR-10 Batch 1:  loss and accuracy: 1.8676, 0.4562\n",
      "Epoch 2277, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4678\n",
      "Epoch 2278, CIFAR-10 Batch 1:  loss and accuracy: 1.8516, 0.4710\n",
      "Epoch 2279, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4566\n",
      "Epoch 2280, CIFAR-10 Batch 1:  loss and accuracy: 1.8668, 0.4596\n",
      "Epoch 2281, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4658\n",
      "Epoch 2282, CIFAR-10 Batch 1:  loss and accuracy: 1.8588, 0.4694\n",
      "Epoch 2283, CIFAR-10 Batch 1:  loss and accuracy: 1.8608, 0.4650\n",
      "Epoch 2284, CIFAR-10 Batch 1:  loss and accuracy: 1.8588, 0.4696\n",
      "Epoch 2285, CIFAR-10 Batch 1:  loss and accuracy: 1.8642, 0.4700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2286, CIFAR-10 Batch 1:  loss and accuracy: 1.8559, 0.4652\n",
      "Epoch 2287, CIFAR-10 Batch 1:  loss and accuracy: 1.8679, 0.4560\n",
      "Epoch 2288, CIFAR-10 Batch 1:  loss and accuracy: 1.8568, 0.4632\n",
      "Epoch 2289, CIFAR-10 Batch 1:  loss and accuracy: 1.8541, 0.4732\n",
      "Epoch 2290, CIFAR-10 Batch 1:  loss and accuracy: 1.8505, 0.4722\n",
      "Epoch 2291, CIFAR-10 Batch 1:  loss and accuracy: 1.8452, 0.4804\n",
      "Epoch 2292, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4716\n",
      "Epoch 2293, CIFAR-10 Batch 1:  loss and accuracy: 1.8481, 0.4816\n",
      "Epoch 2294, CIFAR-10 Batch 1:  loss and accuracy: 1.8640, 0.4572\n",
      "Epoch 2295, CIFAR-10 Batch 1:  loss and accuracy: 1.8577, 0.4672\n",
      "Epoch 2296, CIFAR-10 Batch 1:  loss and accuracy: 1.8599, 0.4698\n",
      "Epoch 2297, CIFAR-10 Batch 1:  loss and accuracy: 1.8461, 0.4874\n",
      "Epoch 2298, CIFAR-10 Batch 1:  loss and accuracy: 1.8672, 0.4580\n",
      "Epoch 2299, CIFAR-10 Batch 1:  loss and accuracy: 1.8543, 0.4718\n",
      "Epoch 2300, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4698\n",
      "Epoch 2301, CIFAR-10 Batch 1:  loss and accuracy: 1.8588, 0.4658\n",
      "Epoch 2302, CIFAR-10 Batch 1:  loss and accuracy: 1.8522, 0.4722\n",
      "Epoch 2303, CIFAR-10 Batch 1:  loss and accuracy: 1.8507, 0.4734\n",
      "Epoch 2304, CIFAR-10 Batch 1:  loss and accuracy: 1.8524, 0.4760\n",
      "Epoch 2305, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4702\n",
      "Epoch 2306, CIFAR-10 Batch 1:  loss and accuracy: 1.8571, 0.4722\n",
      "Epoch 2307, CIFAR-10 Batch 1:  loss and accuracy: 1.8580, 0.4744\n",
      "Epoch 2308, CIFAR-10 Batch 1:  loss and accuracy: 1.8549, 0.4630\n",
      "Epoch 2309, CIFAR-10 Batch 1:  loss and accuracy: 1.8551, 0.4706\n",
      "Epoch 2310, CIFAR-10 Batch 1:  loss and accuracy: 1.8533, 0.4788\n",
      "Epoch 2311, CIFAR-10 Batch 1:  loss and accuracy: 1.8642, 0.4686\n",
      "Epoch 2312, CIFAR-10 Batch 1:  loss and accuracy: 1.8669, 0.4614\n",
      "Epoch 2313, CIFAR-10 Batch 1:  loss and accuracy: 1.8616, 0.4716\n",
      "Epoch 2314, CIFAR-10 Batch 1:  loss and accuracy: 1.8712, 0.4560\n",
      "Epoch 2315, CIFAR-10 Batch 1:  loss and accuracy: 1.8553, 0.4712\n",
      "Epoch 2316, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4728\n",
      "Epoch 2317, CIFAR-10 Batch 1:  loss and accuracy: 1.8604, 0.4702\n",
      "Epoch 2318, CIFAR-10 Batch 1:  loss and accuracy: 1.8574, 0.4698\n",
      "Epoch 2319, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4734\n",
      "Epoch 2320, CIFAR-10 Batch 1:  loss and accuracy: 1.8593, 0.4660\n",
      "Epoch 2321, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4710\n",
      "Epoch 2322, CIFAR-10 Batch 1:  loss and accuracy: 1.8551, 0.4688\n",
      "Epoch 2323, CIFAR-10 Batch 1:  loss and accuracy: 1.8449, 0.4782\n",
      "Epoch 2324, CIFAR-10 Batch 1:  loss and accuracy: 1.8630, 0.4602\n",
      "Epoch 2325, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4704\n",
      "Epoch 2326, CIFAR-10 Batch 1:  loss and accuracy: 1.8516, 0.4742\n",
      "Epoch 2327, CIFAR-10 Batch 1:  loss and accuracy: 1.8517, 0.4730\n",
      "Epoch 2328, CIFAR-10 Batch 1:  loss and accuracy: 1.8477, 0.4792\n",
      "Epoch 2329, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4712\n",
      "Epoch 2330, CIFAR-10 Batch 1:  loss and accuracy: 1.8560, 0.4656\n",
      "Epoch 2331, CIFAR-10 Batch 1:  loss and accuracy: 1.8540, 0.4686\n",
      "Epoch 2332, CIFAR-10 Batch 1:  loss and accuracy: 1.8458, 0.4798\n",
      "Epoch 2333, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4668\n",
      "Epoch 2334, CIFAR-10 Batch 1:  loss and accuracy: 1.8660, 0.4536\n",
      "Epoch 2335, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4570\n",
      "Epoch 2336, CIFAR-10 Batch 1:  loss and accuracy: 1.8600, 0.4602\n",
      "Epoch 2337, CIFAR-10 Batch 1:  loss and accuracy: 1.8588, 0.4626\n",
      "Epoch 2338, CIFAR-10 Batch 1:  loss and accuracy: 1.8616, 0.4660\n",
      "Epoch 2339, CIFAR-10 Batch 1:  loss and accuracy: 1.8524, 0.4752\n",
      "Epoch 2340, CIFAR-10 Batch 1:  loss and accuracy: 1.8517, 0.4726\n",
      "Epoch 2341, CIFAR-10 Batch 1:  loss and accuracy: 1.8554, 0.4668\n",
      "Epoch 2342, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4622\n",
      "Epoch 2343, CIFAR-10 Batch 1:  loss and accuracy: 1.8586, 0.4674\n",
      "Epoch 2344, CIFAR-10 Batch 1:  loss and accuracy: 1.8621, 0.4624\n",
      "Epoch 2345, CIFAR-10 Batch 1:  loss and accuracy: 1.8578, 0.4642\n",
      "Epoch 2346, CIFAR-10 Batch 1:  loss and accuracy: 1.8493, 0.4766\n",
      "Epoch 2347, CIFAR-10 Batch 1:  loss and accuracy: 1.8554, 0.4692\n",
      "Epoch 2348, CIFAR-10 Batch 1:  loss and accuracy: 1.8646, 0.4706\n",
      "Epoch 2349, CIFAR-10 Batch 1:  loss and accuracy: 1.8484, 0.4792\n",
      "Epoch 2350, CIFAR-10 Batch 1:  loss and accuracy: 1.8616, 0.4620\n",
      "Epoch 2351, CIFAR-10 Batch 1:  loss and accuracy: 1.8532, 0.4758\n",
      "Epoch 2352, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4748\n",
      "Epoch 2353, CIFAR-10 Batch 1:  loss and accuracy: 1.8614, 0.4708\n",
      "Epoch 2354, CIFAR-10 Batch 1:  loss and accuracy: 1.8531, 0.4750\n",
      "Epoch 2355, CIFAR-10 Batch 1:  loss and accuracy: 1.8659, 0.4636\n",
      "Epoch 2356, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4690\n",
      "Epoch 2357, CIFAR-10 Batch 1:  loss and accuracy: 1.8510, 0.4736\n",
      "Epoch 2358, CIFAR-10 Batch 1:  loss and accuracy: 1.8508, 0.4716\n",
      "Epoch 2359, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4750\n",
      "Epoch 2360, CIFAR-10 Batch 1:  loss and accuracy: 1.8597, 0.4638\n",
      "Epoch 2361, CIFAR-10 Batch 1:  loss and accuracy: 1.8620, 0.4638\n",
      "Epoch 2362, CIFAR-10 Batch 1:  loss and accuracy: 1.8512, 0.4714\n",
      "Epoch 2363, CIFAR-10 Batch 1:  loss and accuracy: 1.8498, 0.4712\n",
      "Epoch 2364, CIFAR-10 Batch 1:  loss and accuracy: 1.8495, 0.4732\n",
      "Epoch 2365, CIFAR-10 Batch 1:  loss and accuracy: 1.8551, 0.4704\n",
      "Epoch 2366, CIFAR-10 Batch 1:  loss and accuracy: 1.8528, 0.4726\n",
      "Epoch 2367, CIFAR-10 Batch 1:  loss and accuracy: 1.8440, 0.4746\n",
      "Epoch 2368, CIFAR-10 Batch 1:  loss and accuracy: 1.8459, 0.4716\n",
      "Epoch 2369, CIFAR-10 Batch 1:  loss and accuracy: 1.8626, 0.4570\n",
      "Epoch 2370, CIFAR-10 Batch 1:  loss and accuracy: 1.8555, 0.4700\n",
      "Epoch 2371, CIFAR-10 Batch 1:  loss and accuracy: 1.8456, 0.4802\n",
      "Epoch 2372, CIFAR-10 Batch 1:  loss and accuracy: 1.8481, 0.4788\n",
      "Epoch 2373, CIFAR-10 Batch 1:  loss and accuracy: 1.8555, 0.4734\n",
      "Epoch 2374, CIFAR-10 Batch 1:  loss and accuracy: 1.8651, 0.4642\n",
      "Epoch 2375, CIFAR-10 Batch 1:  loss and accuracy: 1.8594, 0.4708\n",
      "Epoch 2376, CIFAR-10 Batch 1:  loss and accuracy: 1.8437, 0.4808\n",
      "Epoch 2377, CIFAR-10 Batch 1:  loss and accuracy: 1.8450, 0.4796\n",
      "Epoch 2378, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4796\n",
      "Epoch 2379, CIFAR-10 Batch 1:  loss and accuracy: 1.8526, 0.4736\n",
      "Epoch 2380, CIFAR-10 Batch 1:  loss and accuracy: 1.8427, 0.4778\n",
      "Epoch 2381, CIFAR-10 Batch 1:  loss and accuracy: 1.8402, 0.4878\n",
      "Epoch 2382, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4700\n",
      "Epoch 2383, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4762\n",
      "Epoch 2384, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4674\n",
      "Epoch 2385, CIFAR-10 Batch 1:  loss and accuracy: 1.8538, 0.4694\n",
      "Epoch 2386, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4708\n",
      "Epoch 2387, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4710\n",
      "Epoch 2388, CIFAR-10 Batch 1:  loss and accuracy: 1.8435, 0.4834\n",
      "Epoch 2389, CIFAR-10 Batch 1:  loss and accuracy: 1.8473, 0.4750\n",
      "Epoch 2390, CIFAR-10 Batch 1:  loss and accuracy: 1.8535, 0.4742\n",
      "Epoch 2391, CIFAR-10 Batch 1:  loss and accuracy: 1.8487, 0.4768\n",
      "Epoch 2392, CIFAR-10 Batch 1:  loss and accuracy: 1.8568, 0.4750\n",
      "Epoch 2393, CIFAR-10 Batch 1:  loss and accuracy: 1.8687, 0.4616\n",
      "Epoch 2394, CIFAR-10 Batch 1:  loss and accuracy: 1.8495, 0.4772\n",
      "Epoch 2395, CIFAR-10 Batch 1:  loss and accuracy: 1.8529, 0.4728\n",
      "Epoch 2396, CIFAR-10 Batch 1:  loss and accuracy: 1.8604, 0.4688\n",
      "Epoch 2397, CIFAR-10 Batch 1:  loss and accuracy: 1.8467, 0.4798\n",
      "Epoch 2398, CIFAR-10 Batch 1:  loss and accuracy: 1.8708, 0.4538\n",
      "Epoch 2399, CIFAR-10 Batch 1:  loss and accuracy: 1.8587, 0.4724\n",
      "Epoch 2400, CIFAR-10 Batch 1:  loss and accuracy: 1.8532, 0.4802\n",
      "Epoch 2401, CIFAR-10 Batch 1:  loss and accuracy: 1.8545, 0.4776\n",
      "Epoch 2402, CIFAR-10 Batch 1:  loss and accuracy: 1.8544, 0.4716\n",
      "Epoch 2403, CIFAR-10 Batch 1:  loss and accuracy: 1.8501, 0.4740\n",
      "Epoch 2404, CIFAR-10 Batch 1:  loss and accuracy: 1.8464, 0.4802\n",
      "Epoch 2405, CIFAR-10 Batch 1:  loss and accuracy: 1.8533, 0.4690\n",
      "Epoch 2406, CIFAR-10 Batch 1:  loss and accuracy: 1.8531, 0.4716\n",
      "Epoch 2407, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4804\n",
      "Epoch 2408, CIFAR-10 Batch 1:  loss and accuracy: 1.8465, 0.4740\n",
      "Epoch 2409, CIFAR-10 Batch 1:  loss and accuracy: 1.8609, 0.4638\n",
      "Epoch 2410, CIFAR-10 Batch 1:  loss and accuracy: 1.8475, 0.4682\n",
      "Epoch 2411, CIFAR-10 Batch 1:  loss and accuracy: 1.8407, 0.4778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2412, CIFAR-10 Batch 1:  loss and accuracy: 1.8443, 0.4792\n",
      "Epoch 2413, CIFAR-10 Batch 1:  loss and accuracy: 1.8643, 0.4612\n",
      "Epoch 2414, CIFAR-10 Batch 1:  loss and accuracy: 1.8405, 0.4800\n",
      "Epoch 2415, CIFAR-10 Batch 1:  loss and accuracy: 1.8513, 0.4708\n",
      "Epoch 2416, CIFAR-10 Batch 1:  loss and accuracy: 1.8538, 0.4676\n",
      "Epoch 2417, CIFAR-10 Batch 1:  loss and accuracy: 1.8445, 0.4850\n",
      "Epoch 2418, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4794\n",
      "Epoch 2419, CIFAR-10 Batch 1:  loss and accuracy: 1.8586, 0.4664\n",
      "Epoch 2420, CIFAR-10 Batch 1:  loss and accuracy: 1.8478, 0.4768\n",
      "Epoch 2421, CIFAR-10 Batch 1:  loss and accuracy: 1.8533, 0.4740\n",
      "Epoch 2422, CIFAR-10 Batch 1:  loss and accuracy: 1.8467, 0.4754\n",
      "Epoch 2423, CIFAR-10 Batch 1:  loss and accuracy: 1.8571, 0.4728\n",
      "Epoch 2424, CIFAR-10 Batch 1:  loss and accuracy: 1.8524, 0.4736\n",
      "Epoch 2425, CIFAR-10 Batch 1:  loss and accuracy: 1.8506, 0.4714\n",
      "Epoch 2426, CIFAR-10 Batch 1:  loss and accuracy: 1.8664, 0.4616\n",
      "Epoch 2427, CIFAR-10 Batch 1:  loss and accuracy: 1.8526, 0.4730\n",
      "Epoch 2428, CIFAR-10 Batch 1:  loss and accuracy: 1.8487, 0.4800\n",
      "Epoch 2429, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4684\n",
      "Epoch 2430, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4796\n",
      "Epoch 2431, CIFAR-10 Batch 1:  loss and accuracy: 1.8545, 0.4720\n",
      "Epoch 2432, CIFAR-10 Batch 1:  loss and accuracy: 1.8479, 0.4760\n",
      "Epoch 2433, CIFAR-10 Batch 1:  loss and accuracy: 1.8636, 0.4656\n",
      "Epoch 2434, CIFAR-10 Batch 1:  loss and accuracy: 1.8570, 0.4706\n",
      "Epoch 2435, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4704\n",
      "Epoch 2436, CIFAR-10 Batch 1:  loss and accuracy: 1.8474, 0.4778\n",
      "Epoch 2437, CIFAR-10 Batch 1:  loss and accuracy: 1.8477, 0.4812\n",
      "Epoch 2438, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4728\n",
      "Epoch 2439, CIFAR-10 Batch 1:  loss and accuracy: 1.8439, 0.4834\n",
      "Epoch 2440, CIFAR-10 Batch 1:  loss and accuracy: 1.8493, 0.4760\n",
      "Epoch 2441, CIFAR-10 Batch 1:  loss and accuracy: 1.8667, 0.4650\n",
      "Epoch 2442, CIFAR-10 Batch 1:  loss and accuracy: 1.8457, 0.4874\n",
      "Epoch 2443, CIFAR-10 Batch 1:  loss and accuracy: 1.8499, 0.4782\n",
      "Epoch 2444, CIFAR-10 Batch 1:  loss and accuracy: 1.8599, 0.4614\n",
      "Epoch 2445, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4720\n",
      "Epoch 2446, CIFAR-10 Batch 1:  loss and accuracy: 1.8693, 0.4664\n",
      "Epoch 2447, CIFAR-10 Batch 1:  loss and accuracy: 1.8428, 0.4834\n",
      "Epoch 2448, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4764\n",
      "Epoch 2449, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4794\n",
      "Epoch 2450, CIFAR-10 Batch 1:  loss and accuracy: 1.8472, 0.4764\n",
      "Epoch 2451, CIFAR-10 Batch 1:  loss and accuracy: 1.8491, 0.4762\n",
      "Epoch 2452, CIFAR-10 Batch 1:  loss and accuracy: 1.8500, 0.4824\n",
      "Epoch 2453, CIFAR-10 Batch 1:  loss and accuracy: 1.8617, 0.4646\n",
      "Epoch 2454, CIFAR-10 Batch 1:  loss and accuracy: 1.8537, 0.4726\n",
      "Epoch 2455, CIFAR-10 Batch 1:  loss and accuracy: 1.8495, 0.4848\n",
      "Epoch 2456, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4738\n",
      "Epoch 2457, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4670\n",
      "Epoch 2458, CIFAR-10 Batch 1:  loss and accuracy: 1.8459, 0.4786\n",
      "Epoch 2459, CIFAR-10 Batch 1:  loss and accuracy: 1.8491, 0.4728\n",
      "Epoch 2460, CIFAR-10 Batch 1:  loss and accuracy: 1.8483, 0.4786\n",
      "Epoch 2461, CIFAR-10 Batch 1:  loss and accuracy: 1.8483, 0.4730\n",
      "Epoch 2462, CIFAR-10 Batch 1:  loss and accuracy: 1.8500, 0.4780\n",
      "Epoch 2463, CIFAR-10 Batch 1:  loss and accuracy: 1.8458, 0.4858\n",
      "Epoch 2464, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4810\n",
      "Epoch 2465, CIFAR-10 Batch 1:  loss and accuracy: 1.8633, 0.4704\n",
      "Epoch 2466, CIFAR-10 Batch 1:  loss and accuracy: 1.8494, 0.4818\n",
      "Epoch 2467, CIFAR-10 Batch 1:  loss and accuracy: 1.8488, 0.4770\n",
      "Epoch 2468, CIFAR-10 Batch 1:  loss and accuracy: 1.8489, 0.4784\n",
      "Epoch 2469, CIFAR-10 Batch 1:  loss and accuracy: 1.8552, 0.4766\n",
      "Epoch 2470, CIFAR-10 Batch 1:  loss and accuracy: 1.8480, 0.4858\n",
      "Epoch 2471, CIFAR-10 Batch 1:  loss and accuracy: 1.8494, 0.4766\n",
      "Epoch 2472, CIFAR-10 Batch 1:  loss and accuracy: 1.8605, 0.4720\n",
      "Epoch 2473, CIFAR-10 Batch 1:  loss and accuracy: 1.8464, 0.4848\n",
      "Epoch 2474, CIFAR-10 Batch 1:  loss and accuracy: 1.8475, 0.4798\n",
      "Epoch 2475, CIFAR-10 Batch 1:  loss and accuracy: 1.8623, 0.4608\n",
      "Epoch 2476, CIFAR-10 Batch 1:  loss and accuracy: 1.8471, 0.4812\n",
      "Epoch 2477, CIFAR-10 Batch 1:  loss and accuracy: 1.8548, 0.4732\n",
      "Epoch 2478, CIFAR-10 Batch 1:  loss and accuracy: 1.8537, 0.4784\n",
      "Epoch 2479, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4700\n",
      "Epoch 2480, CIFAR-10 Batch 1:  loss and accuracy: 1.8572, 0.4658\n",
      "Epoch 2481, CIFAR-10 Batch 1:  loss and accuracy: 1.8510, 0.4766\n",
      "Epoch 2482, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4708\n",
      "Epoch 2483, CIFAR-10 Batch 1:  loss and accuracy: 1.8549, 0.4718\n",
      "Epoch 2484, CIFAR-10 Batch 1:  loss and accuracy: 1.8598, 0.4678\n",
      "Epoch 2485, CIFAR-10 Batch 1:  loss and accuracy: 1.8609, 0.4660\n",
      "Epoch 2486, CIFAR-10 Batch 1:  loss and accuracy: 1.8476, 0.4836\n",
      "Epoch 2487, CIFAR-10 Batch 1:  loss and accuracy: 1.8478, 0.4846\n",
      "Epoch 2488, CIFAR-10 Batch 1:  loss and accuracy: 1.8474, 0.4824\n",
      "Epoch 2489, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4622\n",
      "Epoch 2490, CIFAR-10 Batch 1:  loss and accuracy: 1.8414, 0.4908\n",
      "Epoch 2491, CIFAR-10 Batch 1:  loss and accuracy: 1.8647, 0.4568\n",
      "Epoch 2492, CIFAR-10 Batch 1:  loss and accuracy: 1.8462, 0.4798\n",
      "Epoch 2493, CIFAR-10 Batch 1:  loss and accuracy: 1.8545, 0.4810\n",
      "Epoch 2494, CIFAR-10 Batch 1:  loss and accuracy: 1.8471, 0.4774\n",
      "Epoch 2495, CIFAR-10 Batch 1:  loss and accuracy: 1.8589, 0.4840\n",
      "Epoch 2496, CIFAR-10 Batch 1:  loss and accuracy: 1.8488, 0.4784\n",
      "Epoch 2497, CIFAR-10 Batch 1:  loss and accuracy: 1.8495, 0.4758\n",
      "Epoch 2498, CIFAR-10 Batch 1:  loss and accuracy: 1.8543, 0.4654\n",
      "Epoch 2499, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4614\n",
      "Epoch 2500, CIFAR-10 Batch 1:  loss and accuracy: 1.8480, 0.4712\n",
      "Epoch 2501, CIFAR-10 Batch 1:  loss and accuracy: 1.8380, 0.4868\n",
      "Epoch 2502, CIFAR-10 Batch 1:  loss and accuracy: 1.8468, 0.4826\n",
      "Epoch 2503, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4684\n",
      "Epoch 2504, CIFAR-10 Batch 1:  loss and accuracy: 1.8544, 0.4622\n",
      "Epoch 2505, CIFAR-10 Batch 1:  loss and accuracy: 1.8440, 0.4770\n",
      "Epoch 2506, CIFAR-10 Batch 1:  loss and accuracy: 1.8402, 0.4828\n",
      "Epoch 2507, CIFAR-10 Batch 1:  loss and accuracy: 1.8576, 0.4728\n",
      "Epoch 2508, CIFAR-10 Batch 1:  loss and accuracy: 1.8409, 0.4844\n",
      "Epoch 2509, CIFAR-10 Batch 1:  loss and accuracy: 1.8472, 0.4752\n",
      "Epoch 2510, CIFAR-10 Batch 1:  loss and accuracy: 1.8600, 0.4694\n",
      "Epoch 2511, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4778\n",
      "Epoch 2512, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4688\n",
      "Epoch 2513, CIFAR-10 Batch 1:  loss and accuracy: 1.8507, 0.4726\n",
      "Epoch 2514, CIFAR-10 Batch 1:  loss and accuracy: 1.8541, 0.4702\n",
      "Epoch 2515, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4740\n",
      "Epoch 2516, CIFAR-10 Batch 1:  loss and accuracy: 1.8448, 0.4830\n",
      "Epoch 2517, CIFAR-10 Batch 1:  loss and accuracy: 1.8426, 0.4880\n",
      "Epoch 2518, CIFAR-10 Batch 1:  loss and accuracy: 1.8490, 0.4804\n",
      "Epoch 2519, CIFAR-10 Batch 1:  loss and accuracy: 1.8515, 0.4750\n",
      "Epoch 2520, CIFAR-10 Batch 1:  loss and accuracy: 1.8518, 0.4790\n",
      "Epoch 2521, CIFAR-10 Batch 1:  loss and accuracy: 1.8563, 0.4716\n",
      "Epoch 2522, CIFAR-10 Batch 1:  loss and accuracy: 1.8596, 0.4682\n",
      "Epoch 2523, CIFAR-10 Batch 1:  loss and accuracy: 1.8510, 0.4814\n",
      "Epoch 2524, CIFAR-10 Batch 1:  loss and accuracy: 1.8597, 0.4692\n",
      "Epoch 2525, CIFAR-10 Batch 1:  loss and accuracy: 1.8556, 0.4774\n",
      "Epoch 2526, CIFAR-10 Batch 1:  loss and accuracy: 1.8474, 0.4758\n",
      "Epoch 2527, CIFAR-10 Batch 1:  loss and accuracy: 1.8465, 0.4836\n",
      "Epoch 2528, CIFAR-10 Batch 1:  loss and accuracy: 1.8523, 0.4750\n",
      "Epoch 2529, CIFAR-10 Batch 1:  loss and accuracy: 1.8543, 0.4734\n",
      "Epoch 2530, CIFAR-10 Batch 1:  loss and accuracy: 1.8607, 0.4702\n",
      "Epoch 2531, CIFAR-10 Batch 1:  loss and accuracy: 1.8463, 0.4792\n",
      "Epoch 2532, CIFAR-10 Batch 1:  loss and accuracy: 1.8490, 0.4830\n",
      "Epoch 2533, CIFAR-10 Batch 1:  loss and accuracy: 1.8465, 0.4812\n",
      "Epoch 2534, CIFAR-10 Batch 1:  loss and accuracy: 1.8426, 0.4864\n",
      "Epoch 2535, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4778\n",
      "Epoch 2536, CIFAR-10 Batch 1:  loss and accuracy: 1.8497, 0.4834\n",
      "Epoch 2537, CIFAR-10 Batch 1:  loss and accuracy: 1.8482, 0.4842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2538, CIFAR-10 Batch 1:  loss and accuracy: 1.8556, 0.4726\n",
      "Epoch 2539, CIFAR-10 Batch 1:  loss and accuracy: 1.8435, 0.4786\n",
      "Epoch 2540, CIFAR-10 Batch 1:  loss and accuracy: 1.8518, 0.4772\n",
      "Epoch 2541, CIFAR-10 Batch 1:  loss and accuracy: 1.8584, 0.4750\n",
      "Epoch 2542, CIFAR-10 Batch 1:  loss and accuracy: 1.8411, 0.4856\n",
      "Epoch 2543, CIFAR-10 Batch 1:  loss and accuracy: 1.8584, 0.4678\n",
      "Epoch 2544, CIFAR-10 Batch 1:  loss and accuracy: 1.8656, 0.4588\n",
      "Epoch 2545, CIFAR-10 Batch 1:  loss and accuracy: 1.8557, 0.4730\n",
      "Epoch 2546, CIFAR-10 Batch 1:  loss and accuracy: 1.8585, 0.4720\n",
      "Epoch 2547, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4796\n",
      "Epoch 2548, CIFAR-10 Batch 1:  loss and accuracy: 1.8507, 0.4802\n",
      "Epoch 2549, CIFAR-10 Batch 1:  loss and accuracy: 1.8547, 0.4756\n",
      "Epoch 2550, CIFAR-10 Batch 1:  loss and accuracy: 1.8491, 0.4772\n",
      "Epoch 2551, CIFAR-10 Batch 1:  loss and accuracy: 1.8540, 0.4740\n",
      "Epoch 2552, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4822\n",
      "Epoch 2553, CIFAR-10 Batch 1:  loss and accuracy: 1.8490, 0.4736\n",
      "Epoch 2554, CIFAR-10 Batch 1:  loss and accuracy: 1.8634, 0.4720\n",
      "Epoch 2555, CIFAR-10 Batch 1:  loss and accuracy: 1.8674, 0.4654\n",
      "Epoch 2556, CIFAR-10 Batch 1:  loss and accuracy: 1.8541, 0.4812\n",
      "Epoch 2557, CIFAR-10 Batch 1:  loss and accuracy: 1.8418, 0.4880\n",
      "Epoch 2558, CIFAR-10 Batch 1:  loss and accuracy: 1.8562, 0.4688\n",
      "Epoch 2559, CIFAR-10 Batch 1:  loss and accuracy: 1.8514, 0.4748\n",
      "Epoch 2560, CIFAR-10 Batch 1:  loss and accuracy: 1.8494, 0.4666\n",
      "Epoch 2561, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4672\n",
      "Epoch 2562, CIFAR-10 Batch 1:  loss and accuracy: 1.8526, 0.4734\n",
      "Epoch 2563, CIFAR-10 Batch 1:  loss and accuracy: 1.8590, 0.4738\n",
      "Epoch 2564, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4674\n",
      "Epoch 2565, CIFAR-10 Batch 1:  loss and accuracy: 1.8617, 0.4706\n",
      "Epoch 2566, CIFAR-10 Batch 1:  loss and accuracy: 1.8447, 0.4860\n",
      "Epoch 2567, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4708\n",
      "Epoch 2568, CIFAR-10 Batch 1:  loss and accuracy: 1.8501, 0.4802\n",
      "Epoch 2569, CIFAR-10 Batch 1:  loss and accuracy: 1.8520, 0.4840\n",
      "Epoch 2570, CIFAR-10 Batch 1:  loss and accuracy: 1.8568, 0.4820\n",
      "Epoch 2571, CIFAR-10 Batch 1:  loss and accuracy: 1.8478, 0.4858\n",
      "Epoch 2572, CIFAR-10 Batch 1:  loss and accuracy: 1.8498, 0.4812\n",
      "Epoch 2573, CIFAR-10 Batch 1:  loss and accuracy: 1.8527, 0.4784\n",
      "Epoch 2574, CIFAR-10 Batch 1:  loss and accuracy: 1.8463, 0.4874\n",
      "Epoch 2575, CIFAR-10 Batch 1:  loss and accuracy: 1.8425, 0.4874\n",
      "Epoch 2576, CIFAR-10 Batch 1:  loss and accuracy: 1.8519, 0.4812\n",
      "Epoch 2577, CIFAR-10 Batch 1:  loss and accuracy: 1.8519, 0.4796\n",
      "Epoch 2578, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4868\n",
      "Epoch 2579, CIFAR-10 Batch 1:  loss and accuracy: 1.8501, 0.4874\n",
      "Epoch 2580, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4858\n",
      "Epoch 2581, CIFAR-10 Batch 1:  loss and accuracy: 1.8484, 0.4840\n",
      "Epoch 2582, CIFAR-10 Batch 1:  loss and accuracy: 1.8595, 0.4718\n",
      "Epoch 2583, CIFAR-10 Batch 1:  loss and accuracy: 1.8423, 0.4846\n",
      "Epoch 2584, CIFAR-10 Batch 1:  loss and accuracy: 1.8535, 0.4744\n",
      "Epoch 2585, CIFAR-10 Batch 1:  loss and accuracy: 1.8500, 0.4814\n",
      "Epoch 2586, CIFAR-10 Batch 1:  loss and accuracy: 1.8536, 0.4712\n",
      "Epoch 2587, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4666\n",
      "Epoch 2588, CIFAR-10 Batch 1:  loss and accuracy: 1.8594, 0.4692\n",
      "Epoch 2589, CIFAR-10 Batch 1:  loss and accuracy: 1.8501, 0.4756\n",
      "Epoch 2590, CIFAR-10 Batch 1:  loss and accuracy: 1.8436, 0.4814\n",
      "Epoch 2591, CIFAR-10 Batch 1:  loss and accuracy: 1.8529, 0.4786\n",
      "Epoch 2592, CIFAR-10 Batch 1:  loss and accuracy: 1.8540, 0.4780\n",
      "Epoch 2593, CIFAR-10 Batch 1:  loss and accuracy: 1.8475, 0.4832\n",
      "Epoch 2594, CIFAR-10 Batch 1:  loss and accuracy: 1.8583, 0.4648\n",
      "Epoch 2595, CIFAR-10 Batch 1:  loss and accuracy: 1.8489, 0.4790\n",
      "Epoch 2596, CIFAR-10 Batch 1:  loss and accuracy: 1.8483, 0.4822\n",
      "Epoch 2597, CIFAR-10 Batch 1:  loss and accuracy: 1.8474, 0.4810\n",
      "Epoch 2598, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4738\n",
      "Epoch 2599, CIFAR-10 Batch 1:  loss and accuracy: 1.8363, 0.4930\n",
      "Epoch 2600, CIFAR-10 Batch 1:  loss and accuracy: 1.8570, 0.4678\n",
      "Epoch 2601, CIFAR-10 Batch 1:  loss and accuracy: 1.8379, 0.4858\n",
      "Epoch 2602, CIFAR-10 Batch 1:  loss and accuracy: 1.8650, 0.4612\n",
      "Epoch 2603, CIFAR-10 Batch 1:  loss and accuracy: 1.8498, 0.4734\n",
      "Epoch 2604, CIFAR-10 Batch 1:  loss and accuracy: 1.8470, 0.4786\n",
      "Epoch 2605, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4822\n",
      "Epoch 2606, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4880\n",
      "Epoch 2607, CIFAR-10 Batch 1:  loss and accuracy: 1.8466, 0.4784\n",
      "Epoch 2608, CIFAR-10 Batch 1:  loss and accuracy: 1.8495, 0.4838\n",
      "Epoch 2609, CIFAR-10 Batch 1:  loss and accuracy: 1.8433, 0.4832\n",
      "Epoch 2610, CIFAR-10 Batch 1:  loss and accuracy: 1.8499, 0.4806\n",
      "Epoch 2611, CIFAR-10 Batch 1:  loss and accuracy: 1.8483, 0.4736\n",
      "Epoch 2612, CIFAR-10 Batch 1:  loss and accuracy: 1.8437, 0.4834\n",
      "Epoch 2613, CIFAR-10 Batch 1:  loss and accuracy: 1.8498, 0.4728\n",
      "Epoch 2614, CIFAR-10 Batch 1:  loss and accuracy: 1.8447, 0.4856\n",
      "Epoch 2615, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4814\n",
      "Epoch 2616, CIFAR-10 Batch 1:  loss and accuracy: 1.8468, 0.4824\n",
      "Epoch 2617, CIFAR-10 Batch 1:  loss and accuracy: 1.8523, 0.4770\n",
      "Epoch 2618, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4936\n",
      "Epoch 2619, CIFAR-10 Batch 1:  loss and accuracy: 1.8589, 0.4744\n",
      "Epoch 2620, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4918\n",
      "Epoch 2621, CIFAR-10 Batch 1:  loss and accuracy: 1.8426, 0.4804\n",
      "Epoch 2622, CIFAR-10 Batch 1:  loss and accuracy: 1.8527, 0.4760\n",
      "Epoch 2623, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4832\n",
      "Epoch 2624, CIFAR-10 Batch 1:  loss and accuracy: 1.8428, 0.4758\n",
      "Epoch 2625, CIFAR-10 Batch 1:  loss and accuracy: 1.8505, 0.4766\n",
      "Epoch 2626, CIFAR-10 Batch 1:  loss and accuracy: 1.8455, 0.4790\n",
      "Epoch 2627, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4754\n",
      "Epoch 2628, CIFAR-10 Batch 1:  loss and accuracy: 1.8377, 0.4912\n",
      "Epoch 2629, CIFAR-10 Batch 1:  loss and accuracy: 1.8353, 0.4920\n",
      "Epoch 2630, CIFAR-10 Batch 1:  loss and accuracy: 1.8357, 0.4852\n",
      "Epoch 2631, CIFAR-10 Batch 1:  loss and accuracy: 1.8469, 0.4730\n",
      "Epoch 2632, CIFAR-10 Batch 1:  loss and accuracy: 1.8433, 0.4848\n",
      "Epoch 2633, CIFAR-10 Batch 1:  loss and accuracy: 1.8367, 0.4842\n",
      "Epoch 2634, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4858\n",
      "Epoch 2635, CIFAR-10 Batch 1:  loss and accuracy: 1.8537, 0.4738\n",
      "Epoch 2636, CIFAR-10 Batch 1:  loss and accuracy: 1.8655, 0.4624\n",
      "Epoch 2637, CIFAR-10 Batch 1:  loss and accuracy: 1.8488, 0.4792\n",
      "Epoch 2638, CIFAR-10 Batch 1:  loss and accuracy: 1.8553, 0.4736\n",
      "Epoch 2639, CIFAR-10 Batch 1:  loss and accuracy: 1.8371, 0.4944\n",
      "Epoch 2640, CIFAR-10 Batch 1:  loss and accuracy: 1.8577, 0.4704\n",
      "Epoch 2641, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4780\n",
      "Epoch 2642, CIFAR-10 Batch 1:  loss and accuracy: 1.8567, 0.4742\n",
      "Epoch 2643, CIFAR-10 Batch 1:  loss and accuracy: 1.8426, 0.4864\n",
      "Epoch 2644, CIFAR-10 Batch 1:  loss and accuracy: 1.8452, 0.4842\n",
      "Epoch 2645, CIFAR-10 Batch 1:  loss and accuracy: 1.8504, 0.4772\n",
      "Epoch 2646, CIFAR-10 Batch 1:  loss and accuracy: 1.8592, 0.4732\n",
      "Epoch 2647, CIFAR-10 Batch 1:  loss and accuracy: 1.8407, 0.4946\n",
      "Epoch 2648, CIFAR-10 Batch 1:  loss and accuracy: 1.8398, 0.4882\n",
      "Epoch 2649, CIFAR-10 Batch 1:  loss and accuracy: 1.8468, 0.4808\n",
      "Epoch 2650, CIFAR-10 Batch 1:  loss and accuracy: 1.8468, 0.4800\n",
      "Epoch 2651, CIFAR-10 Batch 1:  loss and accuracy: 1.8470, 0.4756\n",
      "Epoch 2652, CIFAR-10 Batch 1:  loss and accuracy: 1.8408, 0.4828\n",
      "Epoch 2653, CIFAR-10 Batch 1:  loss and accuracy: 1.8448, 0.4808\n",
      "Epoch 2654, CIFAR-10 Batch 1:  loss and accuracy: 1.8478, 0.4842\n",
      "Epoch 2655, CIFAR-10 Batch 1:  loss and accuracy: 1.8437, 0.4908\n",
      "Epoch 2656, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4858\n",
      "Epoch 2657, CIFAR-10 Batch 1:  loss and accuracy: 1.8428, 0.4908\n",
      "Epoch 2658, CIFAR-10 Batch 1:  loss and accuracy: 1.8589, 0.4700\n",
      "Epoch 2659, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4858\n",
      "Epoch 2660, CIFAR-10 Batch 1:  loss and accuracy: 1.8367, 0.4886\n",
      "Epoch 2661, CIFAR-10 Batch 1:  loss and accuracy: 1.8375, 0.4906\n",
      "Epoch 2662, CIFAR-10 Batch 1:  loss and accuracy: 1.8395, 0.4866\n",
      "Epoch 2663, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2664, CIFAR-10 Batch 1:  loss and accuracy: 1.8475, 0.4838\n",
      "Epoch 2665, CIFAR-10 Batch 1:  loss and accuracy: 1.8581, 0.4746\n",
      "Epoch 2666, CIFAR-10 Batch 1:  loss and accuracy: 1.8375, 0.4936\n",
      "Epoch 2667, CIFAR-10 Batch 1:  loss and accuracy: 1.8488, 0.4780\n",
      "Epoch 2668, CIFAR-10 Batch 1:  loss and accuracy: 1.8387, 0.4950\n",
      "Epoch 2669, CIFAR-10 Batch 1:  loss and accuracy: 1.8488, 0.4756\n",
      "Epoch 2670, CIFAR-10 Batch 1:  loss and accuracy: 1.8442, 0.4792\n",
      "Epoch 2671, CIFAR-10 Batch 1:  loss and accuracy: 1.8502, 0.4704\n",
      "Epoch 2672, CIFAR-10 Batch 1:  loss and accuracy: 1.8412, 0.4838\n",
      "Epoch 2673, CIFAR-10 Batch 1:  loss and accuracy: 1.8388, 0.4932\n",
      "Epoch 2674, CIFAR-10 Batch 1:  loss and accuracy: 1.8437, 0.4836\n",
      "Epoch 2675, CIFAR-10 Batch 1:  loss and accuracy: 1.8492, 0.4768\n",
      "Epoch 2676, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4888\n",
      "Epoch 2677, CIFAR-10 Batch 1:  loss and accuracy: 1.8405, 0.4858\n",
      "Epoch 2678, CIFAR-10 Batch 1:  loss and accuracy: 1.8500, 0.4784\n",
      "Epoch 2679, CIFAR-10 Batch 1:  loss and accuracy: 1.8504, 0.4790\n",
      "Epoch 2680, CIFAR-10 Batch 1:  loss and accuracy: 1.8441, 0.4876\n",
      "Epoch 2681, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4876\n",
      "Epoch 2682, CIFAR-10 Batch 1:  loss and accuracy: 1.8371, 0.4890\n",
      "Epoch 2683, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4794\n",
      "Epoch 2684, CIFAR-10 Batch 1:  loss and accuracy: 1.8386, 0.4830\n",
      "Epoch 2685, CIFAR-10 Batch 1:  loss and accuracy: 1.8410, 0.4916\n",
      "Epoch 2686, CIFAR-10 Batch 1:  loss and accuracy: 1.8398, 0.4828\n",
      "Epoch 2687, CIFAR-10 Batch 1:  loss and accuracy: 1.8392, 0.4874\n",
      "Epoch 2688, CIFAR-10 Batch 1:  loss and accuracy: 1.8412, 0.4894\n",
      "Epoch 2689, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4872\n",
      "Epoch 2690, CIFAR-10 Batch 1:  loss and accuracy: 1.8484, 0.4768\n",
      "Epoch 2691, CIFAR-10 Batch 1:  loss and accuracy: 1.8368, 0.4952\n",
      "Epoch 2692, CIFAR-10 Batch 1:  loss and accuracy: 1.8366, 0.4852\n",
      "Epoch 2693, CIFAR-10 Batch 1:  loss and accuracy: 1.8458, 0.4782\n",
      "Epoch 2694, CIFAR-10 Batch 1:  loss and accuracy: 1.8410, 0.4776\n",
      "Epoch 2695, CIFAR-10 Batch 1:  loss and accuracy: 1.8468, 0.4816\n",
      "Epoch 2696, CIFAR-10 Batch 1:  loss and accuracy: 1.8503, 0.4834\n",
      "Epoch 2697, CIFAR-10 Batch 1:  loss and accuracy: 1.8521, 0.4746\n",
      "Epoch 2698, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4904\n",
      "Epoch 2699, CIFAR-10 Batch 1:  loss and accuracy: 1.8517, 0.4778\n",
      "Epoch 2700, CIFAR-10 Batch 1:  loss and accuracy: 1.8533, 0.4800\n",
      "Epoch 2701, CIFAR-10 Batch 1:  loss and accuracy: 1.8318, 0.4878\n",
      "Epoch 2702, CIFAR-10 Batch 1:  loss and accuracy: 1.8439, 0.4842\n",
      "Epoch 2703, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4918\n",
      "Epoch 2704, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4758\n",
      "Epoch 2705, CIFAR-10 Batch 1:  loss and accuracy: 1.8475, 0.4828\n",
      "Epoch 2706, CIFAR-10 Batch 1:  loss and accuracy: 1.8396, 0.4908\n",
      "Epoch 2707, CIFAR-10 Batch 1:  loss and accuracy: 1.8392, 0.4942\n",
      "Epoch 2708, CIFAR-10 Batch 1:  loss and accuracy: 1.8494, 0.4752\n",
      "Epoch 2709, CIFAR-10 Batch 1:  loss and accuracy: 1.8387, 0.4856\n",
      "Epoch 2710, CIFAR-10 Batch 1:  loss and accuracy: 1.8407, 0.4874\n",
      "Epoch 2711, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4834\n",
      "Epoch 2712, CIFAR-10 Batch 1:  loss and accuracy: 1.8311, 0.4972\n",
      "Epoch 2713, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4754\n",
      "Epoch 2714, CIFAR-10 Batch 1:  loss and accuracy: 1.8466, 0.4794\n",
      "Epoch 2715, CIFAR-10 Batch 1:  loss and accuracy: 1.8403, 0.4900\n",
      "Epoch 2716, CIFAR-10 Batch 1:  loss and accuracy: 1.8416, 0.4894\n",
      "Epoch 2717, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4906\n",
      "Epoch 2718, CIFAR-10 Batch 1:  loss and accuracy: 1.8382, 0.4954\n",
      "Epoch 2719, CIFAR-10 Batch 1:  loss and accuracy: 1.8345, 0.4896\n",
      "Epoch 2720, CIFAR-10 Batch 1:  loss and accuracy: 1.8426, 0.4896\n",
      "Epoch 2721, CIFAR-10 Batch 1:  loss and accuracy: 1.8420, 0.4870\n",
      "Epoch 2722, CIFAR-10 Batch 1:  loss and accuracy: 1.8318, 0.5012\n",
      "Epoch 2723, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4852\n",
      "Epoch 2724, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4834\n",
      "Epoch 2725, CIFAR-10 Batch 1:  loss and accuracy: 1.8456, 0.4924\n",
      "Epoch 2726, CIFAR-10 Batch 1:  loss and accuracy: 1.8430, 0.4948\n",
      "Epoch 2727, CIFAR-10 Batch 1:  loss and accuracy: 1.8459, 0.4866\n",
      "Epoch 2728, CIFAR-10 Batch 1:  loss and accuracy: 1.8422, 0.4908\n",
      "Epoch 2729, CIFAR-10 Batch 1:  loss and accuracy: 1.8350, 0.4944\n",
      "Epoch 2730, CIFAR-10 Batch 1:  loss and accuracy: 1.8443, 0.4810\n",
      "Epoch 2731, CIFAR-10 Batch 1:  loss and accuracy: 1.8378, 0.4942\n",
      "Epoch 2732, CIFAR-10 Batch 1:  loss and accuracy: 1.8410, 0.4890\n",
      "Epoch 2733, CIFAR-10 Batch 1:  loss and accuracy: 1.8444, 0.4870\n",
      "Epoch 2734, CIFAR-10 Batch 1:  loss and accuracy: 1.8397, 0.4872\n",
      "Epoch 2735, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4824\n",
      "Epoch 2736, CIFAR-10 Batch 1:  loss and accuracy: 1.8464, 0.4760\n",
      "Epoch 2737, CIFAR-10 Batch 1:  loss and accuracy: 1.8338, 0.4960\n",
      "Epoch 2738, CIFAR-10 Batch 1:  loss and accuracy: 1.8465, 0.4828\n",
      "Epoch 2739, CIFAR-10 Batch 1:  loss and accuracy: 1.8441, 0.4908\n",
      "Epoch 2740, CIFAR-10 Batch 1:  loss and accuracy: 1.8463, 0.4830\n",
      "Epoch 2741, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4868\n",
      "Epoch 2742, CIFAR-10 Batch 1:  loss and accuracy: 1.8381, 0.4922\n",
      "Epoch 2743, CIFAR-10 Batch 1:  loss and accuracy: 1.8427, 0.4866\n",
      "Epoch 2744, CIFAR-10 Batch 1:  loss and accuracy: 1.8368, 0.4912\n",
      "Epoch 2745, CIFAR-10 Batch 1:  loss and accuracy: 1.8404, 0.4864\n",
      "Epoch 2746, CIFAR-10 Batch 1:  loss and accuracy: 1.8400, 0.4844\n",
      "Epoch 2747, CIFAR-10 Batch 1:  loss and accuracy: 1.8466, 0.4904\n",
      "Epoch 2748, CIFAR-10 Batch 1:  loss and accuracy: 1.8509, 0.4804\n",
      "Epoch 2749, CIFAR-10 Batch 1:  loss and accuracy: 1.8440, 0.4918\n",
      "Epoch 2750, CIFAR-10 Batch 1:  loss and accuracy: 1.8366, 0.5020\n",
      "Epoch 2751, CIFAR-10 Batch 1:  loss and accuracy: 1.8405, 0.4894\n",
      "Epoch 2752, CIFAR-10 Batch 1:  loss and accuracy: 1.8554, 0.4760\n",
      "Epoch 2753, CIFAR-10 Batch 1:  loss and accuracy: 1.8455, 0.4772\n",
      "Epoch 2754, CIFAR-10 Batch 1:  loss and accuracy: 1.8339, 0.4980\n",
      "Epoch 2755, CIFAR-10 Batch 1:  loss and accuracy: 1.8349, 0.5014\n",
      "Epoch 2756, CIFAR-10 Batch 1:  loss and accuracy: 1.8348, 0.4982\n",
      "Epoch 2757, CIFAR-10 Batch 1:  loss and accuracy: 1.8364, 0.5054\n",
      "Epoch 2758, CIFAR-10 Batch 1:  loss and accuracy: 1.8445, 0.4904\n",
      "Epoch 2759, CIFAR-10 Batch 1:  loss and accuracy: 1.8394, 0.5020\n",
      "Epoch 2760, CIFAR-10 Batch 1:  loss and accuracy: 1.8382, 0.4978\n",
      "Epoch 2761, CIFAR-10 Batch 1:  loss and accuracy: 1.8472, 0.4858\n",
      "Epoch 2762, CIFAR-10 Batch 1:  loss and accuracy: 1.8520, 0.4846\n",
      "Epoch 2763, CIFAR-10 Batch 1:  loss and accuracy: 1.8420, 0.4894\n",
      "Epoch 2764, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4930\n",
      "Epoch 2765, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4852\n",
      "Epoch 2766, CIFAR-10 Batch 1:  loss and accuracy: 1.8427, 0.4802\n",
      "Epoch 2767, CIFAR-10 Batch 1:  loss and accuracy: 1.8386, 0.4904\n",
      "Epoch 2768, CIFAR-10 Batch 1:  loss and accuracy: 1.8400, 0.4832\n",
      "Epoch 2769, CIFAR-10 Batch 1:  loss and accuracy: 1.8326, 0.5000\n",
      "Epoch 2770, CIFAR-10 Batch 1:  loss and accuracy: 1.8397, 0.4856\n",
      "Epoch 2771, CIFAR-10 Batch 1:  loss and accuracy: 1.8483, 0.4812\n",
      "Epoch 2772, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4868\n",
      "Epoch 2773, CIFAR-10 Batch 1:  loss and accuracy: 1.8354, 0.4942\n",
      "Epoch 2774, CIFAR-10 Batch 1:  loss and accuracy: 1.8443, 0.4850\n",
      "Epoch 2775, CIFAR-10 Batch 1:  loss and accuracy: 1.8356, 0.4934\n",
      "Epoch 2776, CIFAR-10 Batch 1:  loss and accuracy: 1.8359, 0.4824\n",
      "Epoch 2777, CIFAR-10 Batch 1:  loss and accuracy: 1.8466, 0.4706\n",
      "Epoch 2778, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4884\n",
      "Epoch 2779, CIFAR-10 Batch 1:  loss and accuracy: 1.8505, 0.4768\n",
      "Epoch 2780, CIFAR-10 Batch 1:  loss and accuracy: 1.8504, 0.4706\n",
      "Epoch 2781, CIFAR-10 Batch 1:  loss and accuracy: 1.8372, 0.4960\n",
      "Epoch 2782, CIFAR-10 Batch 1:  loss and accuracy: 1.8412, 0.4878\n",
      "Epoch 2783, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4834\n",
      "Epoch 2784, CIFAR-10 Batch 1:  loss and accuracy: 1.8485, 0.4750\n",
      "Epoch 2785, CIFAR-10 Batch 1:  loss and accuracy: 1.8321, 0.4966\n",
      "Epoch 2786, CIFAR-10 Batch 1:  loss and accuracy: 1.8382, 0.4940\n",
      "Epoch 2787, CIFAR-10 Batch 1:  loss and accuracy: 1.8377, 0.4954\n",
      "Epoch 2788, CIFAR-10 Batch 1:  loss and accuracy: 1.8413, 0.4912\n",
      "Epoch 2789, CIFAR-10 Batch 1:  loss and accuracy: 1.8379, 0.4944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2790, CIFAR-10 Batch 1:  loss and accuracy: 1.8471, 0.4834\n",
      "Epoch 2791, CIFAR-10 Batch 1:  loss and accuracy: 1.8399, 0.4878\n",
      "Epoch 2792, CIFAR-10 Batch 1:  loss and accuracy: 1.8405, 0.4944\n",
      "Epoch 2793, CIFAR-10 Batch 1:  loss and accuracy: 1.8384, 0.4980\n",
      "Epoch 2794, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4834\n",
      "Epoch 2795, CIFAR-10 Batch 1:  loss and accuracy: 1.8378, 0.4850\n",
      "Epoch 2796, CIFAR-10 Batch 1:  loss and accuracy: 1.8404, 0.4888\n",
      "Epoch 2797, CIFAR-10 Batch 1:  loss and accuracy: 1.8428, 0.4910\n",
      "Epoch 2798, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4862\n",
      "Epoch 2799, CIFAR-10 Batch 1:  loss and accuracy: 1.8519, 0.4854\n",
      "Epoch 2800, CIFAR-10 Batch 1:  loss and accuracy: 1.8351, 0.4928\n",
      "Epoch 2801, CIFAR-10 Batch 1:  loss and accuracy: 1.8405, 0.4832\n",
      "Epoch 2802, CIFAR-10 Batch 1:  loss and accuracy: 1.8449, 0.4816\n",
      "Epoch 2803, CIFAR-10 Batch 1:  loss and accuracy: 1.8447, 0.4820\n",
      "Epoch 2804, CIFAR-10 Batch 1:  loss and accuracy: 1.8366, 0.4896\n",
      "Epoch 2805, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4840\n",
      "Epoch 2806, CIFAR-10 Batch 1:  loss and accuracy: 1.8364, 0.4936\n",
      "Epoch 2807, CIFAR-10 Batch 1:  loss and accuracy: 1.8386, 0.4948\n",
      "Epoch 2808, CIFAR-10 Batch 1:  loss and accuracy: 1.8421, 0.4830\n",
      "Epoch 2809, CIFAR-10 Batch 1:  loss and accuracy: 1.8388, 0.4804\n",
      "Epoch 2810, CIFAR-10 Batch 1:  loss and accuracy: 1.8369, 0.4802\n",
      "Epoch 2811, CIFAR-10 Batch 1:  loss and accuracy: 1.8407, 0.4878\n",
      "Epoch 2812, CIFAR-10 Batch 1:  loss and accuracy: 1.8330, 0.4958\n",
      "Epoch 2813, CIFAR-10 Batch 1:  loss and accuracy: 1.8395, 0.4896\n",
      "Epoch 2814, CIFAR-10 Batch 1:  loss and accuracy: 1.8457, 0.4870\n",
      "Epoch 2815, CIFAR-10 Batch 1:  loss and accuracy: 1.8453, 0.4894\n",
      "Epoch 2816, CIFAR-10 Batch 1:  loss and accuracy: 1.8455, 0.4886\n",
      "Epoch 2817, CIFAR-10 Batch 1:  loss and accuracy: 1.8372, 0.4860\n",
      "Epoch 2818, CIFAR-10 Batch 1:  loss and accuracy: 1.8374, 0.4958\n",
      "Epoch 2819, CIFAR-10 Batch 1:  loss and accuracy: 1.8416, 0.4860\n",
      "Epoch 2820, CIFAR-10 Batch 1:  loss and accuracy: 1.8385, 0.4856\n",
      "Epoch 2821, CIFAR-10 Batch 1:  loss and accuracy: 1.8360, 0.4814\n",
      "Epoch 2822, CIFAR-10 Batch 1:  loss and accuracy: 1.8410, 0.4812\n",
      "Epoch 2823, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4838\n",
      "Epoch 2824, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4892\n",
      "Epoch 2825, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4872\n",
      "Epoch 2826, CIFAR-10 Batch 1:  loss and accuracy: 1.8448, 0.4916\n",
      "Epoch 2827, CIFAR-10 Batch 1:  loss and accuracy: 1.8397, 0.4900\n",
      "Epoch 2828, CIFAR-10 Batch 1:  loss and accuracy: 1.8370, 0.4906\n",
      "Epoch 2829, CIFAR-10 Batch 1:  loss and accuracy: 1.8552, 0.4794\n",
      "Epoch 2830, CIFAR-10 Batch 1:  loss and accuracy: 1.8374, 0.4930\n",
      "Epoch 2831, CIFAR-10 Batch 1:  loss and accuracy: 1.8373, 0.4922\n",
      "Epoch 2832, CIFAR-10 Batch 1:  loss and accuracy: 1.8510, 0.4856\n",
      "Epoch 2833, CIFAR-10 Batch 1:  loss and accuracy: 1.8460, 0.4890\n",
      "Epoch 2834, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4834\n",
      "Epoch 2835, CIFAR-10 Batch 1:  loss and accuracy: 1.8422, 0.4866\n",
      "Epoch 2836, CIFAR-10 Batch 1:  loss and accuracy: 1.8464, 0.4872\n",
      "Epoch 2837, CIFAR-10 Batch 1:  loss and accuracy: 1.8436, 0.4886\n",
      "Epoch 2838, CIFAR-10 Batch 1:  loss and accuracy: 1.8453, 0.4894\n",
      "Epoch 2839, CIFAR-10 Batch 1:  loss and accuracy: 1.8360, 0.4940\n",
      "Epoch 2840, CIFAR-10 Batch 1:  loss and accuracy: 1.8347, 0.4982\n",
      "Epoch 2841, CIFAR-10 Batch 1:  loss and accuracy: 1.8396, 0.4836\n",
      "Epoch 2842, CIFAR-10 Batch 1:  loss and accuracy: 1.8397, 0.4920\n",
      "Epoch 2843, CIFAR-10 Batch 1:  loss and accuracy: 1.8393, 0.4906\n",
      "Epoch 2844, CIFAR-10 Batch 1:  loss and accuracy: 1.8377, 0.4978\n",
      "Epoch 2845, CIFAR-10 Batch 1:  loss and accuracy: 1.8378, 0.4882\n",
      "Epoch 2846, CIFAR-10 Batch 1:  loss and accuracy: 1.8423, 0.4872\n",
      "Epoch 2847, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4798\n",
      "Epoch 2848, CIFAR-10 Batch 1:  loss and accuracy: 1.8400, 0.4852\n",
      "Epoch 2849, CIFAR-10 Batch 1:  loss and accuracy: 1.8491, 0.4728\n",
      "Epoch 2850, CIFAR-10 Batch 1:  loss and accuracy: 1.8320, 0.4938\n",
      "Epoch 2851, CIFAR-10 Batch 1:  loss and accuracy: 1.8350, 0.4934\n",
      "Epoch 2852, CIFAR-10 Batch 1:  loss and accuracy: 1.8412, 0.4906\n",
      "Epoch 2853, CIFAR-10 Batch 1:  loss and accuracy: 1.8442, 0.4850\n",
      "Epoch 2854, CIFAR-10 Batch 1:  loss and accuracy: 1.8428, 0.4816\n",
      "Epoch 2855, CIFAR-10 Batch 1:  loss and accuracy: 1.8425, 0.4838\n",
      "Epoch 2856, CIFAR-10 Batch 1:  loss and accuracy: 1.8449, 0.4780\n",
      "Epoch 2857, CIFAR-10 Batch 1:  loss and accuracy: 1.8370, 0.5014\n",
      "Epoch 2858, CIFAR-10 Batch 1:  loss and accuracy: 1.8484, 0.4774\n",
      "Epoch 2859, CIFAR-10 Batch 1:  loss and accuracy: 1.8457, 0.4878\n",
      "Epoch 2860, CIFAR-10 Batch 1:  loss and accuracy: 1.8375, 0.4928\n",
      "Epoch 2861, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4862\n",
      "Epoch 2862, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4896\n",
      "Epoch 2863, CIFAR-10 Batch 1:  loss and accuracy: 1.8446, 0.4864\n",
      "Epoch 2864, CIFAR-10 Batch 1:  loss and accuracy: 1.8511, 0.4802\n",
      "Epoch 2865, CIFAR-10 Batch 1:  loss and accuracy: 1.8537, 0.4806\n",
      "Epoch 2866, CIFAR-10 Batch 1:  loss and accuracy: 1.8334, 0.5010\n",
      "Epoch 2867, CIFAR-10 Batch 1:  loss and accuracy: 1.8370, 0.4988\n",
      "Epoch 2868, CIFAR-10 Batch 1:  loss and accuracy: 1.8392, 0.4920\n",
      "Epoch 2869, CIFAR-10 Batch 1:  loss and accuracy: 1.8462, 0.4812\n",
      "Epoch 2870, CIFAR-10 Batch 1:  loss and accuracy: 1.8375, 0.4920\n",
      "Epoch 2871, CIFAR-10 Batch 1:  loss and accuracy: 1.8450, 0.4888\n",
      "Epoch 2872, CIFAR-10 Batch 1:  loss and accuracy: 1.8429, 0.4948\n",
      "Epoch 2873, CIFAR-10 Batch 1:  loss and accuracy: 1.8417, 0.4870\n",
      "Epoch 2874, CIFAR-10 Batch 1:  loss and accuracy: 1.8381, 0.4920\n",
      "Epoch 2875, CIFAR-10 Batch 1:  loss and accuracy: 1.8399, 0.4942\n",
      "Epoch 2876, CIFAR-10 Batch 1:  loss and accuracy: 1.8380, 0.4958\n",
      "Epoch 2877, CIFAR-10 Batch 1:  loss and accuracy: 1.8542, 0.4818\n",
      "Epoch 2878, CIFAR-10 Batch 1:  loss and accuracy: 1.8435, 0.4894\n",
      "Epoch 2879, CIFAR-10 Batch 1:  loss and accuracy: 1.8442, 0.4954\n",
      "Epoch 2880, CIFAR-10 Batch 1:  loss and accuracy: 1.8421, 0.4948\n",
      "Epoch 2881, CIFAR-10 Batch 1:  loss and accuracy: 1.8383, 0.4876\n",
      "Epoch 2882, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4834\n",
      "Epoch 2883, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4814\n",
      "Epoch 2884, CIFAR-10 Batch 1:  loss and accuracy: 1.8530, 0.4764\n",
      "Epoch 2885, CIFAR-10 Batch 1:  loss and accuracy: 1.8366, 0.4900\n",
      "Epoch 2886, CIFAR-10 Batch 1:  loss and accuracy: 1.8363, 0.4964\n",
      "Epoch 2887, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4898\n",
      "Epoch 2888, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4840\n",
      "Epoch 2889, CIFAR-10 Batch 1:  loss and accuracy: 1.8347, 0.4988\n",
      "Epoch 2890, CIFAR-10 Batch 1:  loss and accuracy: 1.8451, 0.4874\n",
      "Epoch 2891, CIFAR-10 Batch 1:  loss and accuracy: 1.8477, 0.4906\n",
      "Epoch 2892, CIFAR-10 Batch 1:  loss and accuracy: 1.8408, 0.4950\n",
      "Epoch 2893, CIFAR-10 Batch 1:  loss and accuracy: 1.8445, 0.4902\n",
      "Epoch 2894, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4912\n",
      "Epoch 2895, CIFAR-10 Batch 1:  loss and accuracy: 1.8433, 0.4928\n",
      "Epoch 2896, CIFAR-10 Batch 1:  loss and accuracy: 1.8355, 0.4934\n",
      "Epoch 2897, CIFAR-10 Batch 1:  loss and accuracy: 1.8373, 0.4868\n",
      "Epoch 2898, CIFAR-10 Batch 1:  loss and accuracy: 1.8434, 0.4832\n",
      "Epoch 2899, CIFAR-10 Batch 1:  loss and accuracy: 1.8415, 0.4888\n",
      "Epoch 2900, CIFAR-10 Batch 1:  loss and accuracy: 1.8436, 0.4904\n",
      "Epoch 2901, CIFAR-10 Batch 1:  loss and accuracy: 1.8369, 0.4914\n",
      "Epoch 2902, CIFAR-10 Batch 1:  loss and accuracy: 1.8341, 0.4958\n",
      "Epoch 2903, CIFAR-10 Batch 1:  loss and accuracy: 1.8423, 0.4880\n",
      "Epoch 2904, CIFAR-10 Batch 1:  loss and accuracy: 1.8457, 0.4798\n",
      "Epoch 2905, CIFAR-10 Batch 1:  loss and accuracy: 1.8310, 0.5050\n",
      "Epoch 2906, CIFAR-10 Batch 1:  loss and accuracy: 1.8306, 0.5020\n",
      "Epoch 2907, CIFAR-10 Batch 1:  loss and accuracy: 1.8411, 0.4896\n",
      "Epoch 2908, CIFAR-10 Batch 1:  loss and accuracy: 1.8404, 0.4948\n",
      "Epoch 2909, CIFAR-10 Batch 1:  loss and accuracy: 1.8384, 0.4956\n",
      "Epoch 2910, CIFAR-10 Batch 1:  loss and accuracy: 1.8366, 0.4958\n",
      "Epoch 2911, CIFAR-10 Batch 1:  loss and accuracy: 1.8367, 0.4942\n",
      "Epoch 2912, CIFAR-10 Batch 1:  loss and accuracy: 1.8412, 0.4944\n",
      "Epoch 2913, CIFAR-10 Batch 1:  loss and accuracy: 1.8409, 0.4972\n",
      "Epoch 2914, CIFAR-10 Batch 1:  loss and accuracy: 1.8416, 0.4898\n",
      "Epoch 2915, CIFAR-10 Batch 1:  loss and accuracy: 1.8353, 0.4918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2916, CIFAR-10 Batch 1:  loss and accuracy: 1.8368, 0.4944\n",
      "Epoch 2917, CIFAR-10 Batch 1:  loss and accuracy: 1.8550, 0.4796\n",
      "Epoch 2918, CIFAR-10 Batch 1:  loss and accuracy: 1.8407, 0.4846\n",
      "Epoch 2919, CIFAR-10 Batch 1:  loss and accuracy: 1.8378, 0.4872\n",
      "Epoch 2920, CIFAR-10 Batch 1:  loss and accuracy: 1.8381, 0.4906\n",
      "Epoch 2921, CIFAR-10 Batch 1:  loss and accuracy: 1.8436, 0.4826\n",
      "Epoch 2922, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4906\n",
      "Epoch 2923, CIFAR-10 Batch 1:  loss and accuracy: 1.8379, 0.4928\n",
      "Epoch 2924, CIFAR-10 Batch 1:  loss and accuracy: 1.8371, 0.4898\n",
      "Epoch 2925, CIFAR-10 Batch 1:  loss and accuracy: 1.8382, 0.4940\n",
      "Epoch 2926, CIFAR-10 Batch 1:  loss and accuracy: 1.8361, 0.4928\n",
      "Epoch 2927, CIFAR-10 Batch 1:  loss and accuracy: 1.8353, 0.4956\n",
      "Epoch 2928, CIFAR-10 Batch 1:  loss and accuracy: 1.8432, 0.4822\n",
      "Epoch 2929, CIFAR-10 Batch 1:  loss and accuracy: 1.8350, 0.4964\n",
      "Epoch 2930, CIFAR-10 Batch 1:  loss and accuracy: 1.8352, 0.4932\n",
      "Epoch 2931, CIFAR-10 Batch 1:  loss and accuracy: 1.8333, 0.5008\n",
      "Epoch 2932, CIFAR-10 Batch 1:  loss and accuracy: 1.8474, 0.4882\n",
      "Epoch 2933, CIFAR-10 Batch 1:  loss and accuracy: 1.8496, 0.4828\n",
      "Epoch 2934, CIFAR-10 Batch 1:  loss and accuracy: 1.8321, 0.5014\n",
      "Epoch 2935, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4848\n",
      "Epoch 2936, CIFAR-10 Batch 1:  loss and accuracy: 1.8491, 0.4836\n",
      "Epoch 2937, CIFAR-10 Batch 1:  loss and accuracy: 1.8459, 0.4872\n",
      "Epoch 2938, CIFAR-10 Batch 1:  loss and accuracy: 1.8455, 0.4842\n",
      "Epoch 2939, CIFAR-10 Batch 1:  loss and accuracy: 1.8380, 0.4908\n",
      "Epoch 2940, CIFAR-10 Batch 1:  loss and accuracy: 1.8340, 0.4930\n",
      "Epoch 2941, CIFAR-10 Batch 1:  loss and accuracy: 1.8330, 0.4964\n",
      "Epoch 2942, CIFAR-10 Batch 1:  loss and accuracy: 1.8359, 0.4962\n",
      "Epoch 2943, CIFAR-10 Batch 1:  loss and accuracy: 1.8380, 0.4942\n",
      "Epoch 2944, CIFAR-10 Batch 1:  loss and accuracy: 1.8431, 0.4916\n",
      "Epoch 2945, CIFAR-10 Batch 1:  loss and accuracy: 1.8362, 0.4878\n",
      "Epoch 2946, CIFAR-10 Batch 1:  loss and accuracy: 1.8348, 0.4950\n",
      "Epoch 2947, CIFAR-10 Batch 1:  loss and accuracy: 1.8375, 0.4944\n",
      "Epoch 2948, CIFAR-10 Batch 1:  loss and accuracy: 1.8411, 0.4878\n",
      "Epoch 2949, CIFAR-10 Batch 1:  loss and accuracy: 1.8349, 0.4984\n",
      "Epoch 2950, CIFAR-10 Batch 1:  loss and accuracy: 1.8396, 0.4906\n",
      "Epoch 2951, CIFAR-10 Batch 1:  loss and accuracy: 1.8406, 0.4926\n",
      "Epoch 2952, CIFAR-10 Batch 1:  loss and accuracy: 1.8419, 0.4932\n",
      "Epoch 2953, CIFAR-10 Batch 1:  loss and accuracy: 1.8390, 0.4986\n",
      "Epoch 2954, CIFAR-10 Batch 1:  loss and accuracy: 1.8534, 0.4796\n",
      "Epoch 2955, CIFAR-10 Batch 1:  loss and accuracy: 1.8454, 0.4886\n",
      "Epoch 2956, CIFAR-10 Batch 1:  loss and accuracy: 1.8434, 0.4886\n",
      "Epoch 2957, CIFAR-10 Batch 1:  loss and accuracy: 1.8331, 0.5016\n",
      "Epoch 2958, CIFAR-10 Batch 1:  loss and accuracy: 1.8436, 0.4902\n",
      "Epoch 2959, CIFAR-10 Batch 1:  loss and accuracy: 1.8360, 0.4924\n",
      "Epoch 2960, CIFAR-10 Batch 1:  loss and accuracy: 1.8438, 0.4840\n",
      "Epoch 2961, CIFAR-10 Batch 1:  loss and accuracy: 1.8385, 0.4916\n",
      "Epoch 2962, CIFAR-10 Batch 1:  loss and accuracy: 1.8363, 0.4938\n",
      "Epoch 2963, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4890\n",
      "Epoch 2964, CIFAR-10 Batch 1:  loss and accuracy: 1.8485, 0.4792\n",
      "Epoch 2965, CIFAR-10 Batch 1:  loss and accuracy: 1.8350, 0.4912\n",
      "Epoch 2966, CIFAR-10 Batch 1:  loss and accuracy: 1.8367, 0.4996\n",
      "Epoch 2967, CIFAR-10 Batch 1:  loss and accuracy: 1.8352, 0.4958\n",
      "Epoch 2968, CIFAR-10 Batch 1:  loss and accuracy: 1.8342, 0.4990\n",
      "Epoch 2969, CIFAR-10 Batch 1:  loss and accuracy: 1.8548, 0.4718\n",
      "Epoch 2970, CIFAR-10 Batch 1:  loss and accuracy: 1.8449, 0.4948\n",
      "Epoch 2971, CIFAR-10 Batch 1:  loss and accuracy: 1.8462, 0.4874\n",
      "Epoch 2972, CIFAR-10 Batch 1:  loss and accuracy: 1.8442, 0.4864\n",
      "Epoch 2973, CIFAR-10 Batch 1:  loss and accuracy: 1.8414, 0.4934\n",
      "Epoch 2974, CIFAR-10 Batch 1:  loss and accuracy: 1.8409, 0.5008\n",
      "Epoch 2975, CIFAR-10 Batch 1:  loss and accuracy: 1.8562, 0.4808\n",
      "Epoch 2976, CIFAR-10 Batch 1:  loss and accuracy: 1.8410, 0.4922\n",
      "Epoch 2977, CIFAR-10 Batch 1:  loss and accuracy: 1.8397, 0.4972\n",
      "Epoch 2978, CIFAR-10 Batch 1:  loss and accuracy: 1.8477, 0.4808\n",
      "Epoch 2979, CIFAR-10 Batch 1:  loss and accuracy: 1.8601, 0.4754\n",
      "Epoch 2980, CIFAR-10 Batch 1:  loss and accuracy: 1.8357, 0.5040\n",
      "Epoch 2981, CIFAR-10 Batch 1:  loss and accuracy: 1.8338, 0.4910\n",
      "Epoch 2982, CIFAR-10 Batch 1:  loss and accuracy: 1.8376, 0.4898\n",
      "Epoch 2983, CIFAR-10 Batch 1:  loss and accuracy: 1.8424, 0.4888\n",
      "Epoch 2984, CIFAR-10 Batch 1:  loss and accuracy: 1.8450, 0.4822\n",
      "Epoch 2985, CIFAR-10 Batch 1:  loss and accuracy: 1.8548, 0.4806\n",
      "Epoch 2986, CIFAR-10 Batch 1:  loss and accuracy: 1.8465, 0.4788\n",
      "Epoch 2987, CIFAR-10 Batch 1:  loss and accuracy: 1.8391, 0.4910\n",
      "Epoch 2988, CIFAR-10 Batch 1:  loss and accuracy: 1.8389, 0.4950\n",
      "Epoch 2989, CIFAR-10 Batch 1:  loss and accuracy: 1.8441, 0.4842\n",
      "Epoch 2990, CIFAR-10 Batch 1:  loss and accuracy: 1.8429, 0.4948\n",
      "Epoch 2991, CIFAR-10 Batch 1:  loss and accuracy: 1.8541, 0.4762\n",
      "Epoch 2992, CIFAR-10 Batch 1:  loss and accuracy: 1.8401, 0.4820\n",
      "Epoch 2993, CIFAR-10 Batch 1:  loss and accuracy: 1.8378, 0.4888\n",
      "Epoch 2994, CIFAR-10 Batch 1:  loss and accuracy: 1.8387, 0.4922\n",
      "Epoch 2995, CIFAR-10 Batch 1:  loss and accuracy: 1.8342, 0.4928\n",
      "Epoch 2996, CIFAR-10 Batch 1:  loss and accuracy: 1.8333, 0.4982\n",
      "Epoch 2997, CIFAR-10 Batch 1:  loss and accuracy: 1.8357, 0.4908\n",
      "Epoch 2998, CIFAR-10 Batch 1:  loss and accuracy: 1.8309, 0.4996\n",
      "Epoch 2999, CIFAR-10 Batch 1:  loss and accuracy: 1.8280, 0.5024\n",
      "Epoch 3000, CIFAR-10 Batch 1:  loss and accuracy: 1.8311, 0.4972\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        \n",
    "        # if(epoch + 1 == 1 or (epoch + 1) % 50 == 0):\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, valid_features, valid_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss and accuracy: 2.3862, 0.1014\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss and accuracy: 2.3642, 0.1016\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss and accuracy: 2.3409, 0.1020\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss and accuracy: 2.3368, 0.1022\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss and accuracy: 2.3371, 0.1020\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss and accuracy: 1.9889, 0.3398\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss and accuracy: 1.9871, 0.3388\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss and accuracy: 1.9851, 0.3374\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss and accuracy: 1.9963, 0.3346\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss and accuracy: 1.9818, 0.3364\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss and accuracy: 1.9232, 0.3882\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss and accuracy: 1.9177, 0.3992\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss and accuracy: 1.9373, 0.3802\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss and accuracy: 1.9227, 0.3844\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss and accuracy: 1.9213, 0.3934\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss and accuracy: 1.8903, 0.4214\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss and accuracy: 1.8918, 0.4238\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss and accuracy: 1.8809, 0.4236\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss and accuracy: 1.8854, 0.4188\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss and accuracy: 1.8821, 0.4188\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss and accuracy: 1.8558, 0.4574\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss and accuracy: 1.8607, 0.4550\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss and accuracy: 1.8603, 0.4548\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss and accuracy: 1.8667, 0.4484\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss and accuracy: 1.8565, 0.4600\n",
      "Epoch 250, CIFAR-10 Batch 1:  loss and accuracy: 1.8442, 0.4732\n",
      "Epoch 250, CIFAR-10 Batch 2:  loss and accuracy: 1.8457, 0.4738\n",
      "Epoch 250, CIFAR-10 Batch 3:  loss and accuracy: 1.8458, 0.4798\n",
      "Epoch 250, CIFAR-10 Batch 4:  loss and accuracy: 1.8414, 0.4924\n",
      "Epoch 250, CIFAR-10 Batch 5:  loss and accuracy: 1.8416, 0.4820\n",
      "Epoch 300, CIFAR-10 Batch 1:  loss and accuracy: 1.8193, 0.4928\n",
      "Epoch 300, CIFAR-10 Batch 2:  loss and accuracy: 1.8583, 0.4696\n",
      "Epoch 300, CIFAR-10 Batch 3:  loss and accuracy: 1.8358, 0.4882\n",
      "Epoch 300, CIFAR-10 Batch 4:  loss and accuracy: 1.8303, 0.4948\n",
      "Epoch 300, CIFAR-10 Batch 5:  loss and accuracy: 1.8280, 0.4916\n",
      "Epoch 350, CIFAR-10 Batch 1:  loss and accuracy: 1.8160, 0.5116\n",
      "Epoch 350, CIFAR-10 Batch 2:  loss and accuracy: 1.8254, 0.4964\n",
      "Epoch 350, CIFAR-10 Batch 3:  loss and accuracy: 1.8089, 0.5092\n",
      "Epoch 350, CIFAR-10 Batch 4:  loss and accuracy: 1.8182, 0.5056\n",
      "Epoch 350, CIFAR-10 Batch 5:  loss and accuracy: 1.8256, 0.4928\n",
      "Epoch 400, CIFAR-10 Batch 1:  loss and accuracy: 1.8068, 0.5228\n",
      "Epoch 400, CIFAR-10 Batch 2:  loss and accuracy: 1.8094, 0.5180\n",
      "Epoch 400, CIFAR-10 Batch 3:  loss and accuracy: 1.8147, 0.5114\n",
      "Epoch 400, CIFAR-10 Batch 4:  loss and accuracy: 1.8122, 0.5174\n",
      "Epoch 400, CIFAR-10 Batch 5:  loss and accuracy: 1.8049, 0.5234\n",
      "Epoch 450, CIFAR-10 Batch 1:  loss and accuracy: 1.8157, 0.5156\n",
      "Epoch 450, CIFAR-10 Batch 2:  loss and accuracy: 1.8104, 0.5176\n",
      "Epoch 450, CIFAR-10 Batch 3:  loss and accuracy: 1.8164, 0.5128\n",
      "Epoch 450, CIFAR-10 Batch 4:  loss and accuracy: 1.7934, 0.5426\n",
      "Epoch 450, CIFAR-10 Batch 5:  loss and accuracy: 1.7991, 0.5338\n",
      "Epoch 500, CIFAR-10 Batch 1:  loss and accuracy: 1.7963, 0.5368\n",
      "Epoch 500, CIFAR-10 Batch 2:  loss and accuracy: 1.7860, 0.5484\n",
      "Epoch 500, CIFAR-10 Batch 3:  loss and accuracy: 1.7901, 0.5468\n",
      "Epoch 500, CIFAR-10 Batch 4:  loss and accuracy: 1.7920, 0.5364\n",
      "Epoch 500, CIFAR-10 Batch 5:  loss and accuracy: 1.7921, 0.5364\n",
      "Epoch 550, CIFAR-10 Batch 1:  loss and accuracy: 1.7717, 0.5600\n",
      "Epoch 550, CIFAR-10 Batch 2:  loss and accuracy: 1.7791, 0.5568\n",
      "Epoch 550, CIFAR-10 Batch 3:  loss and accuracy: 1.7744, 0.5640\n",
      "Epoch 550, CIFAR-10 Batch 4:  loss and accuracy: 1.7829, 0.5520\n",
      "Epoch 550, CIFAR-10 Batch 5:  loss and accuracy: 1.8022, 0.5348\n",
      "Epoch 600, CIFAR-10 Batch 1:  loss and accuracy: 1.7890, 0.5420\n",
      "Epoch 600, CIFAR-10 Batch 2:  loss and accuracy: 1.7872, 0.5492\n",
      "Epoch 600, CIFAR-10 Batch 3:  loss and accuracy: 1.7763, 0.5552\n",
      "Epoch 600, CIFAR-10 Batch 4:  loss and accuracy: 1.7776, 0.5578\n",
      "Epoch 600, CIFAR-10 Batch 5:  loss and accuracy: 1.7769, 0.5530\n",
      "Epoch 650, CIFAR-10 Batch 1:  loss and accuracy: 1.7685, 0.5734\n",
      "Epoch 650, CIFAR-10 Batch 2:  loss and accuracy: 1.7865, 0.5586\n",
      "Epoch 650, CIFAR-10 Batch 3:  loss and accuracy: 1.7739, 0.5612\n",
      "Epoch 650, CIFAR-10 Batch 4:  loss and accuracy: 1.7632, 0.5770\n",
      "Epoch 650, CIFAR-10 Batch 5:  loss and accuracy: 1.7737, 0.5628\n",
      "Epoch 700, CIFAR-10 Batch 1:  loss and accuracy: 1.7636, 0.5898\n",
      "Epoch 700, CIFAR-10 Batch 2:  loss and accuracy: 1.7690, 0.5740\n",
      "Epoch 700, CIFAR-10 Batch 3:  loss and accuracy: 1.7675, 0.5852\n",
      "Epoch 700, CIFAR-10 Batch 4:  loss and accuracy: 1.7701, 0.5842\n",
      "Epoch 700, CIFAR-10 Batch 5:  loss and accuracy: 1.7675, 0.5830\n",
      "Epoch 750, CIFAR-10 Batch 1:  loss and accuracy: 1.7557, 0.6116\n",
      "Epoch 750, CIFAR-10 Batch 2:  loss and accuracy: 1.7569, 0.6044\n",
      "Epoch 750, CIFAR-10 Batch 3:  loss and accuracy: 1.7497, 0.6072\n",
      "Epoch 750, CIFAR-10 Batch 4:  loss and accuracy: 1.7684, 0.5942\n",
      "Epoch 750, CIFAR-10 Batch 5:  loss and accuracy: 1.7587, 0.6072\n",
      "Epoch 800, CIFAR-10 Batch 1:  loss and accuracy: 1.7574, 0.6020\n",
      "Epoch 800, CIFAR-10 Batch 2:  loss and accuracy: 1.7521, 0.6026\n",
      "Epoch 800, CIFAR-10 Batch 3:  loss and accuracy: 1.7452, 0.6076\n",
      "Epoch 800, CIFAR-10 Batch 4:  loss and accuracy: 1.7402, 0.6192\n",
      "Epoch 800, CIFAR-10 Batch 5:  loss and accuracy: 1.7515, 0.6106\n",
      "Epoch 850, CIFAR-10 Batch 1:  loss and accuracy: 1.7444, 0.6236\n",
      "Epoch 850, CIFAR-10 Batch 2:  loss and accuracy: 1.7485, 0.6122\n",
      "Epoch 850, CIFAR-10 Batch 3:  loss and accuracy: 1.7495, 0.6114\n",
      "Epoch 850, CIFAR-10 Batch 4:  loss and accuracy: 1.7421, 0.6188\n",
      "Epoch 850, CIFAR-10 Batch 5:  loss and accuracy: 1.7487, 0.6128\n",
      "Epoch 900, CIFAR-10 Batch 1:  loss and accuracy: 1.7396, 0.6276\n",
      "Epoch 900, CIFAR-10 Batch 2:  loss and accuracy: 1.7262, 0.6364\n",
      "Epoch 900, CIFAR-10 Batch 3:  loss and accuracy: 1.7471, 0.6158\n",
      "Epoch 900, CIFAR-10 Batch 4:  loss and accuracy: 1.7455, 0.6224\n",
      "Epoch 900, CIFAR-10 Batch 5:  loss and accuracy: 1.7369, 0.6332\n",
      "Epoch 950, CIFAR-10 Batch 1:  loss and accuracy: 1.7358, 0.6396\n",
      "Epoch 950, CIFAR-10 Batch 2:  loss and accuracy: 1.7367, 0.6300\n",
      "Epoch 950, CIFAR-10 Batch 3:  loss and accuracy: 1.7352, 0.6380\n",
      "Epoch 950, CIFAR-10 Batch 4:  loss and accuracy: 1.7273, 0.6446\n",
      "Epoch 950, CIFAR-10 Batch 5:  loss and accuracy: 1.7330, 0.6456\n",
      "Epoch 1000, CIFAR-10 Batch 1:  loss and accuracy: 1.7308, 0.6450\n",
      "Epoch 1000, CIFAR-10 Batch 2:  loss and accuracy: 1.7245, 0.6490\n",
      "Epoch 1000, CIFAR-10 Batch 3:  loss and accuracy: 1.7298, 0.6432\n",
      "Epoch 1000, CIFAR-10 Batch 4:  loss and accuracy: 1.7291, 0.6426\n",
      "Epoch 1000, CIFAR-10 Batch 5:  loss and accuracy: 1.7304, 0.6360\n",
      "Epoch 1050, CIFAR-10 Batch 1:  loss and accuracy: 1.7237, 0.6566\n",
      "Epoch 1050, CIFAR-10 Batch 2:  loss and accuracy: 1.7263, 0.6490\n",
      "Epoch 1050, CIFAR-10 Batch 3:  loss and accuracy: 1.7127, 0.6624\n",
      "Epoch 1050, CIFAR-10 Batch 4:  loss and accuracy: 1.7214, 0.6548\n",
      "Epoch 1050, CIFAR-10 Batch 5:  loss and accuracy: 1.7379, 0.6438\n",
      "Epoch 1100, CIFAR-10 Batch 1:  loss and accuracy: 1.7247, 0.6530\n",
      "Epoch 1100, CIFAR-10 Batch 2:  loss and accuracy: 1.7159, 0.6698\n",
      "Epoch 1100, CIFAR-10 Batch 3:  loss and accuracy: 1.7218, 0.6576\n",
      "Epoch 1100, CIFAR-10 Batch 4:  loss and accuracy: 1.7208, 0.6626\n",
      "Epoch 1100, CIFAR-10 Batch 5:  loss and accuracy: 1.7163, 0.6664\n",
      "Epoch 1150, CIFAR-10 Batch 1:  loss and accuracy: 1.7050, 0.6736\n",
      "Epoch 1150, CIFAR-10 Batch 2:  loss and accuracy: 1.7171, 0.6578\n",
      "Epoch 1150, CIFAR-10 Batch 3:  loss and accuracy: 1.7076, 0.6688\n",
      "Epoch 1150, CIFAR-10 Batch 4:  loss and accuracy: 1.7088, 0.6668\n",
      "Epoch 1150, CIFAR-10 Batch 5:  loss and accuracy: 1.7179, 0.6508\n",
      "Epoch 1200, CIFAR-10 Batch 1:  loss and accuracy: 1.7119, 0.6676\n",
      "Epoch 1200, CIFAR-10 Batch 2:  loss and accuracy: 1.7048, 0.6704\n",
      "Epoch 1200, CIFAR-10 Batch 3:  loss and accuracy: 1.7148, 0.6558\n",
      "Epoch 1200, CIFAR-10 Batch 4:  loss and accuracy: 1.6974, 0.6828\n",
      "Epoch 1200, CIFAR-10 Batch 5:  loss and accuracy: 1.7153, 0.6554\n",
      "Epoch 1250, CIFAR-10 Batch 1:  loss and accuracy: 1.7082, 0.6712\n",
      "Epoch 1250, CIFAR-10 Batch 2:  loss and accuracy: 1.7032, 0.6780\n",
      "Epoch 1250, CIFAR-10 Batch 3:  loss and accuracy: 1.7029, 0.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1250, CIFAR-10 Batch 4:  loss and accuracy: 1.7042, 0.6780\n",
      "Epoch 1250, CIFAR-10 Batch 5:  loss and accuracy: 1.7010, 0.6788\n",
      "Epoch 1300, CIFAR-10 Batch 1:  loss and accuracy: 1.7008, 0.6850\n",
      "Epoch 1300, CIFAR-10 Batch 2:  loss and accuracy: 1.7028, 0.6788\n",
      "Epoch 1300, CIFAR-10 Batch 3:  loss and accuracy: 1.7016, 0.6754\n",
      "Epoch 1300, CIFAR-10 Batch 4:  loss and accuracy: 1.7004, 0.6812\n",
      "Epoch 1300, CIFAR-10 Batch 5:  loss and accuracy: 1.7002, 0.6786\n",
      "Epoch 1350, CIFAR-10 Batch 1:  loss and accuracy: 1.6921, 0.6882\n",
      "Epoch 1350, CIFAR-10 Batch 2:  loss and accuracy: 1.6919, 0.6914\n",
      "Epoch 1350, CIFAR-10 Batch 3:  loss and accuracy: 1.6941, 0.6824\n",
      "Epoch 1350, CIFAR-10 Batch 4:  loss and accuracy: 1.6986, 0.6812\n",
      "Epoch 1350, CIFAR-10 Batch 5:  loss and accuracy: 1.6961, 0.6888\n",
      "Epoch 1400, CIFAR-10 Batch 1:  loss and accuracy: 1.6992, 0.6824\n",
      "Epoch 1400, CIFAR-10 Batch 2:  loss and accuracy: 1.6937, 0.6872\n",
      "Epoch 1400, CIFAR-10 Batch 3:  loss and accuracy: 1.6888, 0.6922\n",
      "Epoch 1400, CIFAR-10 Batch 4:  loss and accuracy: 1.6877, 0.6938\n",
      "Epoch 1400, CIFAR-10 Batch 5:  loss and accuracy: 1.6968, 0.6862\n",
      "Epoch 1450, CIFAR-10 Batch 1:  loss and accuracy: 1.6909, 0.6938\n",
      "Epoch 1450, CIFAR-10 Batch 2:  loss and accuracy: 1.6903, 0.6934\n",
      "Epoch 1450, CIFAR-10 Batch 3:  loss and accuracy: 1.6945, 0.6902\n",
      "Epoch 1450, CIFAR-10 Batch 4:  loss and accuracy: 1.6886, 0.6902\n",
      "Epoch 1450, CIFAR-10 Batch 5:  loss and accuracy: 1.6907, 0.6940\n",
      "Epoch 1500, CIFAR-10 Batch 1:  loss and accuracy: 1.6951, 0.6948\n",
      "Epoch 1500, CIFAR-10 Batch 2:  loss and accuracy: 1.6948, 0.6882\n",
      "Epoch 1500, CIFAR-10 Batch 3:  loss and accuracy: 1.6974, 0.6846\n",
      "Epoch 1500, CIFAR-10 Batch 4:  loss and accuracy: 1.6922, 0.6902\n",
      "Epoch 1500, CIFAR-10 Batch 5:  loss and accuracy: 1.6925, 0.6946\n",
      "Epoch 1550, CIFAR-10 Batch 1:  loss and accuracy: 1.6869, 0.7078\n",
      "Epoch 1550, CIFAR-10 Batch 2:  loss and accuracy: 1.6854, 0.6944\n",
      "Epoch 1550, CIFAR-10 Batch 3:  loss and accuracy: 1.6888, 0.7078\n",
      "Epoch 1550, CIFAR-10 Batch 4:  loss and accuracy: 1.6882, 0.7026\n",
      "Epoch 1550, CIFAR-10 Batch 5:  loss and accuracy: 1.6892, 0.7022\n",
      "Epoch 1600, CIFAR-10 Batch 1:  loss and accuracy: 1.6798, 0.7106\n",
      "Epoch 1600, CIFAR-10 Batch 2:  loss and accuracy: 1.6818, 0.7034\n",
      "Epoch 1600, CIFAR-10 Batch 3:  loss and accuracy: 1.6843, 0.7040\n",
      "Epoch 1600, CIFAR-10 Batch 4:  loss and accuracy: 1.6829, 0.7034\n",
      "Epoch 1600, CIFAR-10 Batch 5:  loss and accuracy: 1.6844, 0.7026\n",
      "Epoch 1650, CIFAR-10 Batch 1:  loss and accuracy: 1.6847, 0.7014\n",
      "Epoch 1650, CIFAR-10 Batch 2:  loss and accuracy: 1.6816, 0.7136\n",
      "Epoch 1650, CIFAR-10 Batch 3:  loss and accuracy: 1.6811, 0.7052\n",
      "Epoch 1650, CIFAR-10 Batch 4:  loss and accuracy: 1.6804, 0.7058\n",
      "Epoch 1650, CIFAR-10 Batch 5:  loss and accuracy: 1.6785, 0.7112\n",
      "Epoch 1700, CIFAR-10 Batch 1:  loss and accuracy: 1.6757, 0.7054\n",
      "Epoch 1700, CIFAR-10 Batch 2:  loss and accuracy: 1.6822, 0.7054\n",
      "Epoch 1700, CIFAR-10 Batch 3:  loss and accuracy: 1.6814, 0.7082\n",
      "Epoch 1700, CIFAR-10 Batch 4:  loss and accuracy: 1.6767, 0.7102\n",
      "Epoch 1700, CIFAR-10 Batch 5:  loss and accuracy: 1.6781, 0.7056\n",
      "Epoch 1750, CIFAR-10 Batch 1:  loss and accuracy: 1.6797, 0.7148\n",
      "Epoch 1750, CIFAR-10 Batch 2:  loss and accuracy: 1.6877, 0.7018\n",
      "Epoch 1750, CIFAR-10 Batch 3:  loss and accuracy: 1.6838, 0.7098\n",
      "Epoch 1750, CIFAR-10 Batch 4:  loss and accuracy: 1.6799, 0.7068\n",
      "Epoch 1750, CIFAR-10 Batch 5:  loss and accuracy: 1.6789, 0.7128\n",
      "Epoch 1800, CIFAR-10 Batch 1:  loss and accuracy: 1.6791, 0.7080\n",
      "Epoch 1800, CIFAR-10 Batch 2:  loss and accuracy: 1.6786, 0.7158\n",
      "Epoch 1800, CIFAR-10 Batch 3:  loss and accuracy: 1.6797, 0.7100\n",
      "Epoch 1800, CIFAR-10 Batch 4:  loss and accuracy: 1.6753, 0.7126\n",
      "Epoch 1800, CIFAR-10 Batch 5:  loss and accuracy: 1.6738, 0.7194\n",
      "Epoch 1850, CIFAR-10 Batch 1:  loss and accuracy: 1.6769, 0.7094\n",
      "Epoch 1850, CIFAR-10 Batch 2:  loss and accuracy: 1.6798, 0.7122\n",
      "Epoch 1850, CIFAR-10 Batch 3:  loss and accuracy: 1.6777, 0.7174\n",
      "Epoch 1850, CIFAR-10 Batch 4:  loss and accuracy: 1.6780, 0.7146\n",
      "Epoch 1850, CIFAR-10 Batch 5:  loss and accuracy: 1.6787, 0.7186\n",
      "Epoch 1900, CIFAR-10 Batch 1:  loss and accuracy: 1.6753, 0.7206\n",
      "Epoch 1900, CIFAR-10 Batch 2:  loss and accuracy: 1.6787, 0.7148\n",
      "Epoch 1900, CIFAR-10 Batch 3:  loss and accuracy: 1.6734, 0.7178\n",
      "Epoch 1900, CIFAR-10 Batch 4:  loss and accuracy: 1.6768, 0.7080\n",
      "Epoch 1900, CIFAR-10 Batch 5:  loss and accuracy: 1.6686, 0.7178\n",
      "Epoch 1950, CIFAR-10 Batch 1:  loss and accuracy: 1.6745, 0.7182\n",
      "Epoch 1950, CIFAR-10 Batch 2:  loss and accuracy: 1.6689, 0.7242\n",
      "Epoch 1950, CIFAR-10 Batch 3:  loss and accuracy: 1.6754, 0.7168\n",
      "Epoch 1950, CIFAR-10 Batch 4:  loss and accuracy: 1.6762, 0.7172\n",
      "Epoch 1950, CIFAR-10 Batch 5:  loss and accuracy: 1.6729, 0.7164\n",
      "Epoch 2000, CIFAR-10 Batch 1:  loss and accuracy: 1.6721, 0.7232\n",
      "Epoch 2000, CIFAR-10 Batch 2:  loss and accuracy: 1.6714, 0.7222\n",
      "Epoch 2000, CIFAR-10 Batch 3:  loss and accuracy: 1.6688, 0.7248\n",
      "Epoch 2000, CIFAR-10 Batch 4:  loss and accuracy: 1.6696, 0.7220\n",
      "Epoch 2000, CIFAR-10 Batch 5:  loss and accuracy: 1.6725, 0.7198\n",
      "Epoch 2050, CIFAR-10 Batch 1:  loss and accuracy: 1.6712, 0.7280\n",
      "Epoch 2050, CIFAR-10 Batch 2:  loss and accuracy: 1.6720, 0.7262\n",
      "Epoch 2050, CIFAR-10 Batch 3:  loss and accuracy: 1.6700, 0.7242\n",
      "Epoch 2050, CIFAR-10 Batch 4:  loss and accuracy: 1.6694, 0.7246\n",
      "Epoch 2050, CIFAR-10 Batch 5:  loss and accuracy: 1.6669, 0.7286\n",
      "Epoch 2100, CIFAR-10 Batch 1:  loss and accuracy: 1.6772, 0.7206\n",
      "Epoch 2100, CIFAR-10 Batch 2:  loss and accuracy: 1.6729, 0.7248\n",
      "Epoch 2100, CIFAR-10 Batch 3:  loss and accuracy: 1.6733, 0.7262\n",
      "Epoch 2100, CIFAR-10 Batch 4:  loss and accuracy: 1.6747, 0.7228\n",
      "Epoch 2100, CIFAR-10 Batch 5:  loss and accuracy: 1.6706, 0.7264\n",
      "Epoch 2150, CIFAR-10 Batch 1:  loss and accuracy: 1.6722, 0.7216\n",
      "Epoch 2150, CIFAR-10 Batch 2:  loss and accuracy: 1.6724, 0.7186\n",
      "Epoch 2150, CIFAR-10 Batch 3:  loss and accuracy: 1.6659, 0.7280\n",
      "Epoch 2150, CIFAR-10 Batch 4:  loss and accuracy: 1.6784, 0.7152\n",
      "Epoch 2150, CIFAR-10 Batch 5:  loss and accuracy: 1.6727, 0.7216\n",
      "Epoch 2200, CIFAR-10 Batch 1:  loss and accuracy: 1.6656, 0.7278\n",
      "Epoch 2200, CIFAR-10 Batch 2:  loss and accuracy: 1.6646, 0.7284\n",
      "Epoch 2200, CIFAR-10 Batch 3:  loss and accuracy: 1.6681, 0.7296\n",
      "Epoch 2200, CIFAR-10 Batch 4:  loss and accuracy: 1.6714, 0.7226\n",
      "Epoch 2200, CIFAR-10 Batch 5:  loss and accuracy: 1.6686, 0.7256\n",
      "Epoch 2250, CIFAR-10 Batch 1:  loss and accuracy: 1.6593, 0.7390\n",
      "Epoch 2250, CIFAR-10 Batch 2:  loss and accuracy: 1.6665, 0.7320\n",
      "Epoch 2250, CIFAR-10 Batch 3:  loss and accuracy: 1.6684, 0.7312\n",
      "Epoch 2250, CIFAR-10 Batch 4:  loss and accuracy: 1.6716, 0.7296\n",
      "Epoch 2250, CIFAR-10 Batch 5:  loss and accuracy: 1.6702, 0.7306\n",
      "Epoch 2300, CIFAR-10 Batch 1:  loss and accuracy: 1.6685, 0.7310\n",
      "Epoch 2300, CIFAR-10 Batch 2:  loss and accuracy: 1.6704, 0.7262\n",
      "Epoch 2300, CIFAR-10 Batch 3:  loss and accuracy: 1.6612, 0.7396\n",
      "Epoch 2300, CIFAR-10 Batch 4:  loss and accuracy: 1.6615, 0.7366\n",
      "Epoch 2300, CIFAR-10 Batch 5:  loss and accuracy: 1.6614, 0.7380\n",
      "Epoch 2350, CIFAR-10 Batch 1:  loss and accuracy: 1.6634, 0.7334\n",
      "Epoch 2350, CIFAR-10 Batch 2:  loss and accuracy: 1.6656, 0.7320\n",
      "Epoch 2350, CIFAR-10 Batch 3:  loss and accuracy: 1.6648, 0.7294\n",
      "Epoch 2350, CIFAR-10 Batch 4:  loss and accuracy: 1.6649, 0.7332\n",
      "Epoch 2350, CIFAR-10 Batch 5:  loss and accuracy: 1.6675, 0.7282\n",
      "Epoch 2400, CIFAR-10 Batch 1:  loss and accuracy: 1.6586, 0.7384\n",
      "Epoch 2400, CIFAR-10 Batch 2:  loss and accuracy: 1.6632, 0.7354\n",
      "Epoch 2400, CIFAR-10 Batch 3:  loss and accuracy: 1.6631, 0.7330\n",
      "Epoch 2400, CIFAR-10 Batch 4:  loss and accuracy: 1.6618, 0.7388\n",
      "Epoch 2400, CIFAR-10 Batch 5:  loss and accuracy: 1.6651, 0.7362\n",
      "Epoch 2450, CIFAR-10 Batch 1:  loss and accuracy: 1.6652, 0.7378\n",
      "Epoch 2450, CIFAR-10 Batch 2:  loss and accuracy: 1.6609, 0.7398\n",
      "Epoch 2450, CIFAR-10 Batch 3:  loss and accuracy: 1.6621, 0.7378\n",
      "Epoch 2450, CIFAR-10 Batch 4:  loss and accuracy: 1.6613, 0.7336\n",
      "Epoch 2450, CIFAR-10 Batch 5:  loss and accuracy: 1.6669, 0.7352\n",
      "Epoch 2500, CIFAR-10 Batch 1:  loss and accuracy: 1.6608, 0.7434\n",
      "Epoch 2500, CIFAR-10 Batch 2:  loss and accuracy: 1.6649, 0.7392\n",
      "Epoch 2500, CIFAR-10 Batch 3:  loss and accuracy: 1.6615, 0.7408\n",
      "Epoch 2500, CIFAR-10 Batch 4:  loss and accuracy: 1.6629, 0.7396\n",
      "Epoch 2500, CIFAR-10 Batch 5:  loss and accuracy: 1.6640, 0.7388\n",
      "Epoch 2550, CIFAR-10 Batch 1:  loss and accuracy: 1.6603, 0.7420\n",
      "Epoch 2550, CIFAR-10 Batch 2:  loss and accuracy: 1.6624, 0.7336\n",
      "Epoch 2550, CIFAR-10 Batch 3:  loss and accuracy: 1.6642, 0.7326\n",
      "Epoch 2550, CIFAR-10 Batch 4:  loss and accuracy: 1.6636, 0.7382\n",
      "Epoch 2550, CIFAR-10 Batch 5:  loss and accuracy: 1.6650, 0.7364\n",
      "Epoch 2600, CIFAR-10 Batch 1:  loss and accuracy: 1.6636, 0.7436\n",
      "Epoch 2600, CIFAR-10 Batch 2:  loss and accuracy: 1.6610, 0.7446\n",
      "Epoch 2600, CIFAR-10 Batch 3:  loss and accuracy: 1.6623, 0.7390\n",
      "Epoch 2600, CIFAR-10 Batch 4:  loss and accuracy: 1.6666, 0.7404\n",
      "Epoch 2600, CIFAR-10 Batch 5:  loss and accuracy: 1.6640, 0.7394\n",
      "Epoch 2650, CIFAR-10 Batch 1:  loss and accuracy: 1.6579, 0.7382\n",
      "Epoch 2650, CIFAR-10 Batch 2:  loss and accuracy: 1.6607, 0.7430\n",
      "Epoch 2650, CIFAR-10 Batch 3:  loss and accuracy: 1.6653, 0.7408\n",
      "Epoch 2650, CIFAR-10 Batch 4:  loss and accuracy: 1.6612, 0.7354\n",
      "Epoch 2650, CIFAR-10 Batch 5:  loss and accuracy: 1.6545, 0.7456\n",
      "Epoch 2700, CIFAR-10 Batch 1:  loss and accuracy: 1.6608, 0.7440\n",
      "Epoch 2700, CIFAR-10 Batch 2:  loss and accuracy: 1.6672, 0.7426\n",
      "Epoch 2700, CIFAR-10 Batch 3:  loss and accuracy: 1.6583, 0.7460\n",
      "Epoch 2700, CIFAR-10 Batch 4:  loss and accuracy: 1.6630, 0.7454\n",
      "Epoch 2700, CIFAR-10 Batch 5:  loss and accuracy: 1.6623, 0.7452\n",
      "Epoch 2750, CIFAR-10 Batch 1:  loss and accuracy: 1.6572, 0.7400\n",
      "Epoch 2750, CIFAR-10 Batch 2:  loss and accuracy: 1.6631, 0.7418\n",
      "Epoch 2750, CIFAR-10 Batch 3:  loss and accuracy: 1.6605, 0.7484\n",
      "Epoch 2750, CIFAR-10 Batch 4:  loss and accuracy: 1.6618, 0.7456\n",
      "Epoch 2750, CIFAR-10 Batch 5:  loss and accuracy: 1.6558, 0.7484\n",
      "Epoch 2800, CIFAR-10 Batch 1:  loss and accuracy: 1.6590, 0.7434\n",
      "Epoch 2800, CIFAR-10 Batch 2:  loss and accuracy: 1.6593, 0.7482\n",
      "Epoch 2800, CIFAR-10 Batch 3:  loss and accuracy: 1.6606, 0.7384\n",
      "Epoch 2800, CIFAR-10 Batch 4:  loss and accuracy: 1.6579, 0.7456\n",
      "Epoch 2800, CIFAR-10 Batch 5:  loss and accuracy: 1.6607, 0.7438\n",
      "Epoch 2850, CIFAR-10 Batch 1:  loss and accuracy: 1.6622, 0.7410\n",
      "Epoch 2850, CIFAR-10 Batch 2:  loss and accuracy: 1.6607, 0.7434\n",
      "Epoch 2850, CIFAR-10 Batch 3:  loss and accuracy: 1.6589, 0.7450\n",
      "Epoch 2850, CIFAR-10 Batch 4:  loss and accuracy: 1.6587, 0.7480\n",
      "Epoch 2850, CIFAR-10 Batch 5:  loss and accuracy: 1.6590, 0.7476\n",
      "Epoch 2900, CIFAR-10 Batch 1:  loss and accuracy: 1.6569, 0.7532\n",
      "Epoch 2900, CIFAR-10 Batch 2:  loss and accuracy: 1.6583, 0.7444\n",
      "Epoch 2900, CIFAR-10 Batch 3:  loss and accuracy: 1.6608, 0.7452\n",
      "Epoch 2900, CIFAR-10 Batch 4:  loss and accuracy: 1.6634, 0.7432\n",
      "Epoch 2900, CIFAR-10 Batch 5:  loss and accuracy: 1.6581, 0.7474\n",
      "Epoch 2950, CIFAR-10 Batch 1:  loss and accuracy: 1.6646, 0.7436\n",
      "Epoch 2950, CIFAR-10 Batch 2:  loss and accuracy: 1.6535, 0.7550\n",
      "Epoch 2950, CIFAR-10 Batch 3:  loss and accuracy: 1.6592, 0.7480\n",
      "Epoch 2950, CIFAR-10 Batch 4:  loss and accuracy: 1.6572, 0.7502\n",
      "Epoch 2950, CIFAR-10 Batch 5:  loss and accuracy: 1.6565, 0.7516\n",
      "Epoch 3000, CIFAR-10 Batch 1:  loss and accuracy: 1.6591, 0.7438\n",
      "Epoch 3000, CIFAR-10 Batch 2:  loss and accuracy: 1.6586, 0.7458\n",
      "Epoch 3000, CIFAR-10 Batch 3:  loss and accuracy: 1.6600, 0.7484\n",
      "Epoch 3000, CIFAR-10 Batch 4:  loss and accuracy: 1.6602, 0.7466\n",
      "Epoch 3000, CIFAR-10 Batch 5:  loss and accuracy: 1.6577, 0.7490\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            # print the evaluation result every 50 epoch\n",
    "            if(epoch + 1 == 1 or (epoch + 1) % 50 == 0):\n",
    "                print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "                print_stats(sess, valid_features, valid_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "*Note*: \"\"\"In order to show the training results more clearly, I add a piece of code to show the evaluation \n",
    "accuracy data on validation dataset every 50 epoch, which I believe does not influence the training progress\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.74111328125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c6870ac18>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-3.0",
   "language": "python",
   "name": "tf-gpu-3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
